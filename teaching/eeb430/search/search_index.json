{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EEB30 Modelling in Ecology and Evolution Mathematics is central to science because it provides a rigorous way to go from a set of assumptions to their logical consequences. In ecology & evolution this might be how we think a virus will spread and evolve, how climate change will impact a threatened population, or how much genetic diversity we expect to see in a randomly mating population. In this course you'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, tutorials, assignments, computer labs, and a final project. Our focus is on deterministic dynamical models (recursions and differential equations) but we'll also touch on probability theory and stochastic simulations near the end. Please see the University of Toronto Academic Calendar for more details on the course prerequisites and additional information on the distribution/breadth requirements this course satisfies.","title":"Overview"},{"location":"acknowledgements/","text":"Acknowledgements This course is based on a fantastic textbook by Sally Otto and Troy Day , A biologist's guide to mathematical modeling in ecology and evolution . I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice. I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me. Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher. Thanks are also due to Puneeth Deraje who has gone above and beyond as a TA. And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website. Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"This course is based on a fantastic textbook by Sally Otto and Troy Day , A biologist's guide to mathematical modeling in ecology and evolution . I've been fortunate to have been taught by both of these excellent mentors and I hope I can do them and their book justice. I'm also very grateful to Leithen M'Gonigle who has slickly elaborated on Sally's lecture notes in LaTeX and kindly shared them with me. Thanks also to Ailene McPherson who, along with Sally and Leithen, continues to help me teach this course and inspires me to become a better teacher. Thanks are also due to Puneeth Deraje who has gone above and beyond as a TA. And you wouldn't be reading this without the help of Tom Ouellette who did an amazing job creating this website. Finally, thanks to all the students who have taken the course and given me feedback (explicit or not) -- thanks for your patience!","title":"Acknowledgements"},{"location":"resources/","text":"Resources","title":"Resources"},{"location":"resources/#resources","text":"","title":"Resources"},{"location":"labs/schedule/","text":"Schedule Lab Topic Notebook 1 Introduction to Jupyter, Python, and SymPy 2 Recursion and differential equations - 3 Numerical and graphical techniques - 4 Equilibria and stability (univariate) - 5 General solutions (univariate) - 6 Midterm - 7 Linear algebra in SymPy - 8 Multivariate models - 9 Demography - 10 Evolutionary invasion analysis - 11 Stochastic models of evolution - 12 Stochastic population dynamics - To access a lab, click on the image. The easiest option is then to \"Open with Google Colaboratory\", but you can also download the notebook and open it locally (this requires you install Jupyter, Python, and required Python libraries).","title":"Schedule"},{"location":"labs/schedule/#schedule","text":"Lab Topic Notebook 1 Introduction to Jupyter, Python, and SymPy 2 Recursion and differential equations - 3 Numerical and graphical techniques - 4 Equilibria and stability (univariate) - 5 General solutions (univariate) - 6 Midterm - 7 Linear algebra in SymPy - 8 Multivariate models - 9 Demography - 10 Evolutionary invasion analysis - 11 Stochastic models of evolution - 12 Stochastic population dynamics - To access a lab, click on the image. The easiest option is then to \"Open with Google Colaboratory\", but you can also download the notebook and open it locally (this requires you install Jupyter, Python, and required Python libraries).","title":"Schedule"},{"location":"lectures/lecture-01/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 1: Introduction Run notes interactively? Lecture overview Why use mathematical models in ecology and evolution? Syllabus 1. Why use mathematical models in ecology and evolution? Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions. To see this (while also introducing you to the kind of models I work with), let's look at a few examples. Example 1: HIV See sections 1.2-1.4 of the text for more info. Human immunodeficiency virus (HIV) is, as the name suggests, a virus that infects humans and causes acquired immunodeficiency syndrome (AIDS). This is a terrible diesase that has caused over 20 million deaths worldwide. It is transmitted via bodily fluids. Once inside the body HIV particles (virions) attach to a protein called CD4 on the cell membrane of helper T cells (part of our immune system) and others. Once attached, the virus inserts its RNA into the host cell, which is reverse transcribed into DNA and becomes part of the host genome. The host cell can remain in the 'latently infected' state for some time. When the viral DNA is eventually transcribed by the host cell, starting an 'active infection', hundreds of new virions are produced, often killing the host cell. These new virions then go on to infect other CD4+ cells. A large decline in the number of helper T cells is what causes AIDS. This is because helper T cells bind to viruses and secrete chemical signals to stimulate the immune system. So without helper T cells the immune system is very comprimised. Based on this, once infected with HIV (and without treatment) we expect the number of virions to rapidly increase and the number of helper T cells to decline. This is generally what is observed. However, the number of virions then tends to steeply decline. Why might this be? Here are two hypotheses: the immune system recognizes HIV and suppresses it the decline in helper T cells prevents HIV from replicating To decide whether the second hypothesis is valid Phillips (1996) built a mathematical model describing the rate of change in the number of CD4+ cells and free virions. See the note below for the details of the model if you are interested (but don't worry if you don't understand all this yet!). Details of the Philips (1996) model See Box 2.4 in the text for more details. Philips built a dynamical model of the infection process using 4 differential equations: \\(\\frac{dR}{dt} = \\Gamma \\tau - \\mu R - \\beta V R\\) \\(\\frac{dL}{dt} = p \\beta V R - \\mu L - \\alpha L\\) \\(\\frac{dE}{dt} = (1 - p) \\beta V R + \\alpha L - \\delta E\\) \\(\\frac{dV}{dt} = \\pi E - \\sigma V\\) These equations describe the rate of change in the number of susceptible CD4+ cells (R), latently infected cells (L), actively infected cells (E), and free virions (V). These are the 4 \"variables\" of the model, as their values change over time. The remainder of the symbols represent \"parameters\", whose values do not change (they are constants). The meaning of the parameters and the values used by Phillips (1996) are shown in the table below. Symbol Description Value (units/day) \\(\\Gamma\\) Rate that CD4+ cells are produced 1.36 \\(\\tau\\) Proportion of CD4+ cells that are susceptible to attack 0.2 \\(\\mu\\) HIV-independent death rate of susceptible CD4+ cells 1.36 x 10^-3 \\(\\beta\\) Rate of CD4+ cell infection per HIV virion 0.00027 \\(p\\) Proportion of newly infected cells becoming latently infected 0.1 \\(\\alpha\\) Activation rate of latently infected cells 3.6 x 10^-2 \\(\\delta\\) Death rate of actively infected cells 0.33 \\(\\pi\\) Rate of production of virions by actively infected cells 100 \\(\\sigma\\) Removal rate of cell-free virus 2 Given these parameter descriptions, can you \"read\" the differential equations above? For example, what does \\(\\Gamma \\tau\\) represent, biologically? Using these equations and parameter values Philips (1996) asked how the number of CD4+ cells (R + L + E) and free virions (V) changed over time following infection. To see how Philips' model behaves, activate the kernel at the top of the page and run the code below import numpy as np import matplotlib.pyplot as plt # Build a function to iterate through the model (see note above for description of equations) def philips_model(R=200, L=0, E=0, V=4e-5, days=120, steps=120): #choose parameter values gamma, mu, tau, beta, p, alpha, sigma, delta, pi = [1.36, 1.36e-3, 0.2, 0.00027, 0.1, 3.6e-2, 2, 0.33, 100] record = [] for t in np.linspace(0, days, steps): dRdt = gamma * tau - mu * R - beta * V * R R += dRdt dLdt = p * beta * V * R - mu * L - alpha * L L += dLdt dEdt = (1-p) * beta * V * R + alpha * L - delta * E E += dEdt dVdt = pi * E - sigma * V V += dVdt record += ([[R + L + E, V]]) return np.array(record) # Interpolate through differentials and record population sizes steps = 120 days = 120 uninfected_lymphocytes, free_virions = philips_model(days=days, steps=steps).T number_of_days = np.linspace(1, days, steps) # Initialize plot fig, left_ax = plt.subplots() right_ax = left_ax.twinx() fig.set_size_inches(8,4) # Plot data on left axis left_ax.plot(number_of_days, uninfected_lymphocytes, color='green', label='R'); left_ax.set_ylabel('CD4+ cells (R + E + L)', fontsize=14) left_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.99)) left_ax.set_xlabel('Days from infection', fontsize=14) # Plot data on right axis right_ax.plot(number_of_days, free_virions, color='darkred', label='V'); right_ax.set_ylabel('Free virions V', fontsize=14) right_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.85)) # Format plot and show it plt.yscale('log') fig.tight_layout() plt.show() Compare to Figure 1.3 and Figure 1.4 in the text. We see the initial increase in virions (red) and delcine in CD4+ cells (green), followed by a decline virions (note the log scale on the right axis -- this is a big decline, from over 1000 to about 10). Because this model does not include an immune response against the virions but still exhibits the decline in virions, we conclude that the second hypothesis is valid, that it is theoretically plausible that the decline in virions is due to a lack of CD4+ cells to infect. A few years later this hypothesis was empirically tested and validated -- a nice example of theory guiding science. Feel free to play around with the code above, changing parameter values or even the structure of the model. Do the dynamics change as you expected? Example 2: Extreme Events A second example is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis with Sebastian Schreiber) with in Lyberger et al 2021 . Kelsey Lyberger, doing Daphnia fieldwork I assume. Kelsey was interested in how populations respond to extreme climatic events, like hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include: Ice-storms select on sparrow body size Hurricanes select on lizard limbs and toe pads Droughts select on Darwin finch beaks Droughts select on flowering time in Brassica Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model. To do: give details of Kelsey's model Below is a stochastic simulation much like that used by Kelsey. With an activated kernel, run the code below to create a plot very similar to Figure 1 in Lyberger et al. (this may take a minute). import numpy as np import matplotlib.pyplot as plt def lyberger_model(Vg=0.75, Ve=0, event_duration=1, seed=0, other_parameters=[120, 500, 1, 2, 100, 0, 2.5]): # Unpack parameters generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t = other_parameters # Initialize population members = np.random.normal(initial_theta_t, 0, K) # Run simulations np.random.seed(seed) population_size, mean_breeding_value = [], [] for g in range(generations): if g in np.arange(event_time, event_time + event_duration): theta_t = initial_theta_t + dtheta_t else: theta_t = initial_theta_t # Viability selection prob_survival = np.array([np.exp(-(theta_t - z + np.random.normal(0, Ve))**2 / (2*w**2)) for z in members]) survived = np.array([True if p > np.random.uniform(0,1) else False for p in prob_survival]) if len(survived) == 0: break # Survivors members = members[survived] # Random mating offspring = [] for m in np.random.choice(members, len(members)): if len(offspring) > K: offspring = np.random.choice(offspring, K) break else: n_off = np.random.poisson(lmbda) mate = np.random.choice(members) offspring += [(m + mate)/2 for _ in range(n_off)] # Sample new trait values for offspring offspring = np.array(offspring) offspring = np.random.normal(offspring, Vg) # Record statistics population_size.append(len(offspring)) mean_breeding_value.append(np.mean(offspring)) members = offspring return ( np.arange(0, generations-event_time+1), np.array(population_size[event_time-1:]), np.array(mean_breeding_value[event_time-1:]) ) # Initialize plot fig, ax = plt.subplots(2, sharex=True) fig.set_size_inches(8,6) event_duration = 1 # Run 10 simulations per segregation (V0) and environment variance (VE) parameter combination for Vg, Ve, c, lab in [[1, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]: # Plot simulations simulations = np.array([lyberger_model(Vg=Vg, Ve=Ve, event_duration=event_duration, seed=s) for s in range(10)]) ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3); ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3); # Hack together only one instance of the legend ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1, label = lab, color=c) ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1, label = lab, color=c) # Add environmental event duration ax[0].fill_between([0,event_duration], y1=500, alpha=0.2) ax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2) # Add labels ax[0].set_ylabel('Population size', fontsize=12) ax[1].set_ylabel('Mean trait value', fontsize=12) ax[1].set_xlabel('Generation', fontsize=14) # Add legend plt.legend(frameon=False) plt.show() The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals). Example 3: Sex chromosomes A third example is a model that I worked on during my PhD, led by fellow PhD student Dr. Michael Scott, in Scott et al. 2018 . Michael Scott, with snakes and penguins. The inspiration for this model comes from the fact that new sex determination systems are constantly evolving, as can be deduced from this phylogeny. For example, it is clear from this phylogeny that ZW sex determination (where females are ZW and males are ZZ) has evolved multiple times (eg, in both birds/reptiles and lepidoptera). Figure 3 from Bachtrog et al. 2014 Sex vs gender Here we are referring to biological sex (defined by the type of gametes produced by an individual) not gender (defined by behaviour and morphology). For more information see https://gendersexandnature.wordpress.com and https://genderinclusivebiology.squarespace.com/ . Focusing solely on sex chromosomes (as opposed to hermaphroditism or environmental sex determination (ESD)), there are two main hypotheses for changes in sex determination systems: A \"selfish\" Y chromosome gets in more than 50% of male gametes \\(\\rightarrow\\) more than 50% of the population is male (\"sex-ratio bias\") \\(\\rightarrow\\) a W chromosome, which causes all its carriers to be female, invades because individuals of the rarer sex have more offspring (since every offspring has a female and male parent). Selection favours different copies of a gene in the two sexes (\"sexually-antagonistic selection\") \\(\\rightarrow\\) the chromosome carrying this gene becomes a new sex chromosome, allowing the gene copy (\"allele\") favoured in males to be on the Y chromosome (and therefore only in males) and the allele favoured in females to be on the X (which is in females 2/3 of the time). A key element missing from both of these hypotheses (and models supporting them) is selection that occurs during the haploid phase of the life-cycle (eg, competition between pollen grains for fertilization of an ovule). This \"haploid selection\" is important to consider because it can cause both sex-ratio bias (eg, if pollen grains carrying a Y are more successful than those carrying an X) and sexually-antagonistic selection (eg, if selection at the haploid phase differs between gamete types). It can also cause selection to differ at the haploid and diploid stages of the life-cycle (\"ploidally-antagonistic selection\"). So how is haploid selection expected to affect turnover between sex determination systems? Michael and I built a model to ask this question, with the help of Sally Otto (our supervisor), and used many of the techniques covered in this class to analyze it. To do: add model details (Placeholder) Modelling plan discussed on the way home from a conference This was quite a complicated model and analysis, but the general take-home was that haploid selection greatly increases the scope for sex chromosome turnover. For example, in the plot below we see a case where selection favours one allele in male gametes (eg, during pollen competition) and another allele in diploids (ie, this is ploidally-antagonistic selection). Despite the fact that it causes sex-ratio bias (dashed curve), the Y chromosome spreads through the population (black and gray curves), while at the same time a W chromosome invades (blue curve) and equalizes the sex ratio. To do: add code to create this plot Sex chromosome turnover. The conclusion we reached from this mathematical model -- that haploid selection is expected to lead to more turnovers in sex determination systems -- was simultaneously supported by work done in Stephen Wright's lab here in EEB, by (then undergrad and now PhD student) George Sandler in Sandler et al. 2018 . 2. Syllabus OK, now that we've gone over some motivating examples of modeling in ecology and evolution, let's take a look at how we're going to learn to become modelers in this course. Point your browser over to the syllabus and read each of the pages there.","title":"Lecture 1"},{"location":"lectures/lecture-01/#lecture-1-introduction","text":"Run notes interactively?","title":"Lecture 1: Introduction"},{"location":"lectures/lecture-01/#lecture-overview","text":"Why use mathematical models in ecology and evolution? Syllabus","title":"Lecture overview"},{"location":"lectures/lecture-01/#1-why-use-mathematical-models-in-ecology-and-evolution","text":"Mathematics permeates ecology and evolution, from simple back-of-the-envelope calculations to the development of sophisticated mathematical models. This is because mathematics is a unique tool that can take what we already know (or assume to be true) and rigorously lead us to the logical conclusions. To see this (while also introducing you to the kind of models I work with), let's look at a few examples.","title":"1. Why use mathematical models in ecology and evolution?"},{"location":"lectures/lecture-01/#example-1-hiv","text":"See sections 1.2-1.4 of the text for more info. Human immunodeficiency virus (HIV) is, as the name suggests, a virus that infects humans and causes acquired immunodeficiency syndrome (AIDS). This is a terrible diesase that has caused over 20 million deaths worldwide. It is transmitted via bodily fluids. Once inside the body HIV particles (virions) attach to a protein called CD4 on the cell membrane of helper T cells (part of our immune system) and others. Once attached, the virus inserts its RNA into the host cell, which is reverse transcribed into DNA and becomes part of the host genome. The host cell can remain in the 'latently infected' state for some time. When the viral DNA is eventually transcribed by the host cell, starting an 'active infection', hundreds of new virions are produced, often killing the host cell. These new virions then go on to infect other CD4+ cells. A large decline in the number of helper T cells is what causes AIDS. This is because helper T cells bind to viruses and secrete chemical signals to stimulate the immune system. So without helper T cells the immune system is very comprimised. Based on this, once infected with HIV (and without treatment) we expect the number of virions to rapidly increase and the number of helper T cells to decline. This is generally what is observed. However, the number of virions then tends to steeply decline. Why might this be? Here are two hypotheses: the immune system recognizes HIV and suppresses it the decline in helper T cells prevents HIV from replicating To decide whether the second hypothesis is valid Phillips (1996) built a mathematical model describing the rate of change in the number of CD4+ cells and free virions. See the note below for the details of the model if you are interested (but don't worry if you don't understand all this yet!). Details of the Philips (1996) model See Box 2.4 in the text for more details. Philips built a dynamical model of the infection process using 4 differential equations: \\(\\frac{dR}{dt} = \\Gamma \\tau - \\mu R - \\beta V R\\) \\(\\frac{dL}{dt} = p \\beta V R - \\mu L - \\alpha L\\) \\(\\frac{dE}{dt} = (1 - p) \\beta V R + \\alpha L - \\delta E\\) \\(\\frac{dV}{dt} = \\pi E - \\sigma V\\) These equations describe the rate of change in the number of susceptible CD4+ cells (R), latently infected cells (L), actively infected cells (E), and free virions (V). These are the 4 \"variables\" of the model, as their values change over time. The remainder of the symbols represent \"parameters\", whose values do not change (they are constants). The meaning of the parameters and the values used by Phillips (1996) are shown in the table below. Symbol Description Value (units/day) \\(\\Gamma\\) Rate that CD4+ cells are produced 1.36 \\(\\tau\\) Proportion of CD4+ cells that are susceptible to attack 0.2 \\(\\mu\\) HIV-independent death rate of susceptible CD4+ cells 1.36 x 10^-3 \\(\\beta\\) Rate of CD4+ cell infection per HIV virion 0.00027 \\(p\\) Proportion of newly infected cells becoming latently infected 0.1 \\(\\alpha\\) Activation rate of latently infected cells 3.6 x 10^-2 \\(\\delta\\) Death rate of actively infected cells 0.33 \\(\\pi\\) Rate of production of virions by actively infected cells 100 \\(\\sigma\\) Removal rate of cell-free virus 2 Given these parameter descriptions, can you \"read\" the differential equations above? For example, what does \\(\\Gamma \\tau\\) represent, biologically? Using these equations and parameter values Philips (1996) asked how the number of CD4+ cells (R + L + E) and free virions (V) changed over time following infection. To see how Philips' model behaves, activate the kernel at the top of the page and run the code below import numpy as np import matplotlib.pyplot as plt # Build a function to iterate through the model (see note above for description of equations) def philips_model(R=200, L=0, E=0, V=4e-5, days=120, steps=120): #choose parameter values gamma, mu, tau, beta, p, alpha, sigma, delta, pi = [1.36, 1.36e-3, 0.2, 0.00027, 0.1, 3.6e-2, 2, 0.33, 100] record = [] for t in np.linspace(0, days, steps): dRdt = gamma * tau - mu * R - beta * V * R R += dRdt dLdt = p * beta * V * R - mu * L - alpha * L L += dLdt dEdt = (1-p) * beta * V * R + alpha * L - delta * E E += dEdt dVdt = pi * E - sigma * V V += dVdt record += ([[R + L + E, V]]) return np.array(record) # Interpolate through differentials and record population sizes steps = 120 days = 120 uninfected_lymphocytes, free_virions = philips_model(days=days, steps=steps).T number_of_days = np.linspace(1, days, steps) # Initialize plot fig, left_ax = plt.subplots() right_ax = left_ax.twinx() fig.set_size_inches(8,4) # Plot data on left axis left_ax.plot(number_of_days, uninfected_lymphocytes, color='green', label='R'); left_ax.set_ylabel('CD4+ cells (R + E + L)', fontsize=14) left_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.99)) left_ax.set_xlabel('Days from infection', fontsize=14) # Plot data on right axis right_ax.plot(number_of_days, free_virions, color='darkred', label='V'); right_ax.set_ylabel('Free virions V', fontsize=14) right_ax.legend(frameon=False, bbox_to_anchor=(0.99, 0.85)) # Format plot and show it plt.yscale('log') fig.tight_layout() plt.show() Compare to Figure 1.3 and Figure 1.4 in the text. We see the initial increase in virions (red) and delcine in CD4+ cells (green), followed by a decline virions (note the log scale on the right axis -- this is a big decline, from over 1000 to about 10). Because this model does not include an immune response against the virions but still exhibits the decline in virions, we conclude that the second hypothesis is valid, that it is theoretically plausible that the decline in virions is due to a lack of CD4+ cells to infect. A few years later this hypothesis was empirically tested and validated -- a nice example of theory guiding science. Feel free to play around with the code above, changing parameter values or even the structure of the model. Do the dynamics change as you expected?","title":"Example 1: HIV"},{"location":"lectures/lecture-01/#example-2-extreme-events","text":"A second example is a model that I helped Dr. Kelsey Lyberger (then a PhD student at UC Davis with Sebastian Schreiber) with in Lyberger et al 2021 . Kelsey Lyberger, doing Daphnia fieldwork I assume. Kelsey was interested in how populations respond to extreme climatic events, like hurricanes. It has long been clear that such events can impact the size of a population, e.g., by causing extra mortality, and may in fact put populations at risk of extinction. More recently it has become apparent that extreme events can also impose strong natural selection, and that populations can quickly adapt to the new environment. Some examples include: Ice-storms select on sparrow body size Hurricanes select on lizard limbs and toe pads Droughts select on Darwin finch beaks Droughts select on flowering time in Brassica Now, how should such rapid adaptive evolution impact population size? This is the question Kelsey set out to answer with a mathematical model. To do: give details of Kelsey's model Below is a stochastic simulation much like that used by Kelsey. With an activated kernel, run the code below to create a plot very similar to Figure 1 in Lyberger et al. (this may take a minute). import numpy as np import matplotlib.pyplot as plt def lyberger_model(Vg=0.75, Ve=0, event_duration=1, seed=0, other_parameters=[120, 500, 1, 2, 100, 0, 2.5]): # Unpack parameters generations, K, w, lmbda, event_time, initial_theta_t, dtheta_t = other_parameters # Initialize population members = np.random.normal(initial_theta_t, 0, K) # Run simulations np.random.seed(seed) population_size, mean_breeding_value = [], [] for g in range(generations): if g in np.arange(event_time, event_time + event_duration): theta_t = initial_theta_t + dtheta_t else: theta_t = initial_theta_t # Viability selection prob_survival = np.array([np.exp(-(theta_t - z + np.random.normal(0, Ve))**2 / (2*w**2)) for z in members]) survived = np.array([True if p > np.random.uniform(0,1) else False for p in prob_survival]) if len(survived) == 0: break # Survivors members = members[survived] # Random mating offspring = [] for m in np.random.choice(members, len(members)): if len(offspring) > K: offspring = np.random.choice(offspring, K) break else: n_off = np.random.poisson(lmbda) mate = np.random.choice(members) offspring += [(m + mate)/2 for _ in range(n_off)] # Sample new trait values for offspring offspring = np.array(offspring) offspring = np.random.normal(offspring, Vg) # Record statistics population_size.append(len(offspring)) mean_breeding_value.append(np.mean(offspring)) members = offspring return ( np.arange(0, generations-event_time+1), np.array(population_size[event_time-1:]), np.array(mean_breeding_value[event_time-1:]) ) # Initialize plot fig, ax = plt.subplots(2, sharex=True) fig.set_size_inches(8,6) event_duration = 1 # Run 10 simulations per segregation (V0) and environment variance (VE) parameter combination for Vg, Ve, c, lab in [[1, 0, 'black', 'with evolution'], [0, 1, 'red', 'without evolution']]: # Plot simulations simulations = np.array([lyberger_model(Vg=Vg, Ve=Ve, event_duration=event_duration, seed=s) for s in range(10)]) ax[0].plot(simulations[:,0].T, simulations[:,1].T, color=c, alpha=0.3); ax[1].plot(simulations[:,0].T, simulations[:,2].T, color=c, alpha=0.3); # Hack together only one instance of the legend ax[0].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,1].T)], alpha=1, label = lab, color=c) ax[1].plot([np.min(simulations[:,0].T)], [np.min(simulations[:,2].T)], alpha=1, label = lab, color=c) # Add environmental event duration ax[0].fill_between([0,event_duration], y1=500, alpha=0.2) ax[1].fill_between([0,event_duration], y1=-0.2, y2=2, alpha=0.2) # Add labels ax[0].set_ylabel('Population size', fontsize=12) ax[1].set_ylabel('Mean trait value', fontsize=12) ax[1].set_xlabel('Generation', fontsize=14) # Add legend plt.legend(frameon=False) plt.show() The key result, that you can see in the plot above, is that when extreme events are short, adaptive evolution (black lines) can paradoxically reduce population size (relative to the red lines, where there is no evolution). The reason for this is that, while during the extreme event (shaded section) evolution is adaptive, once the extreme event ends the population finds itself maladapted to the original environment. Adaptive evolution can therefore hamper population persistence, and this is an important thing to keep in mind when documenting rapid adaptive evolution in response to extreme events -- it is not necessarily a good thing for the species (or our conservation goals).","title":"Example 2: Extreme Events"},{"location":"lectures/lecture-01/#example-3-sex-chromosomes","text":"A third example is a model that I worked on during my PhD, led by fellow PhD student Dr. Michael Scott, in Scott et al. 2018 . Michael Scott, with snakes and penguins. The inspiration for this model comes from the fact that new sex determination systems are constantly evolving, as can be deduced from this phylogeny. For example, it is clear from this phylogeny that ZW sex determination (where females are ZW and males are ZZ) has evolved multiple times (eg, in both birds/reptiles and lepidoptera). Figure 3 from Bachtrog et al. 2014 Sex vs gender Here we are referring to biological sex (defined by the type of gametes produced by an individual) not gender (defined by behaviour and morphology). For more information see https://gendersexandnature.wordpress.com and https://genderinclusivebiology.squarespace.com/ . Focusing solely on sex chromosomes (as opposed to hermaphroditism or environmental sex determination (ESD)), there are two main hypotheses for changes in sex determination systems: A \"selfish\" Y chromosome gets in more than 50% of male gametes \\(\\rightarrow\\) more than 50% of the population is male (\"sex-ratio bias\") \\(\\rightarrow\\) a W chromosome, which causes all its carriers to be female, invades because individuals of the rarer sex have more offspring (since every offspring has a female and male parent). Selection favours different copies of a gene in the two sexes (\"sexually-antagonistic selection\") \\(\\rightarrow\\) the chromosome carrying this gene becomes a new sex chromosome, allowing the gene copy (\"allele\") favoured in males to be on the Y chromosome (and therefore only in males) and the allele favoured in females to be on the X (which is in females 2/3 of the time). A key element missing from both of these hypotheses (and models supporting them) is selection that occurs during the haploid phase of the life-cycle (eg, competition between pollen grains for fertilization of an ovule). This \"haploid selection\" is important to consider because it can cause both sex-ratio bias (eg, if pollen grains carrying a Y are more successful than those carrying an X) and sexually-antagonistic selection (eg, if selection at the haploid phase differs between gamete types). It can also cause selection to differ at the haploid and diploid stages of the life-cycle (\"ploidally-antagonistic selection\"). So how is haploid selection expected to affect turnover between sex determination systems? Michael and I built a model to ask this question, with the help of Sally Otto (our supervisor), and used many of the techniques covered in this class to analyze it. To do: add model details (Placeholder) Modelling plan discussed on the way home from a conference This was quite a complicated model and analysis, but the general take-home was that haploid selection greatly increases the scope for sex chromosome turnover. For example, in the plot below we see a case where selection favours one allele in male gametes (eg, during pollen competition) and another allele in diploids (ie, this is ploidally-antagonistic selection). Despite the fact that it causes sex-ratio bias (dashed curve), the Y chromosome spreads through the population (black and gray curves), while at the same time a W chromosome invades (blue curve) and equalizes the sex ratio. To do: add code to create this plot Sex chromosome turnover. The conclusion we reached from this mathematical model -- that haploid selection is expected to lead to more turnovers in sex determination systems -- was simultaneously supported by work done in Stephen Wright's lab here in EEB, by (then undergrad and now PhD student) George Sandler in Sandler et al. 2018 .","title":"Example 3: Sex chromosomes"},{"location":"lectures/lecture-01/#2-syllabus","text":"OK, now that we've gone over some motivating examples of modeling in ecology and evolution, let's take a look at how we're going to learn to become modelers in this course. Point your browser over to the syllabus and read each of the pages there.","title":"2. Syllabus"},{"location":"lectures/lecture-02/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 2: Model construction Run notes interactively? Lecture overview Constructing a model Evolution and the Hardy-Weinberg equilibrium 1. Constructing a model Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model. i. Formulate the question What do you want to know? Describe the model in the form of a question. Simplify, Simplify! Start with the simplest, biologically reasonable description of the problem. ii. Determine the basic ingredients Define the variables in the model. Describe any constraints on the variables. Describe any interactions between variables. Decide whether you will treat time as discrete or continuous. Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.). Define the parameters in the model. Describe any constraints on the parameters. iii. Qualitatively describe the biological system For continuous-time models, draw a flow diagram to describe changes to the variables over time. For example, if we had a population experiencing migration, death, and birth we could draw the following. graph LR; A((n)) --birth--> A; B[ ] --migration--> A; A --death--> C[ ]; style B height:0px; style C height:0px; Figure. Continuous time flow diagram. For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit. For example, if we were modeling a population that experienced migration, then birth, then death each time step, we could draw the following. graph LR; A((n)) --migration--> B((n')); B --birth--> C((n'')); C --death--> A; </pre> </center> <center><sup>Figure. Discrete-time life-cycle diagram. </sup></center> For discrete time models with multiple variables and events, construct a table listing the outcome of every event. We'll see an example of this below. ### iv. Quantitatively describe the biological system Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side. For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. For example, in the model shown above the rate of change in the number of individuals, $\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t}$, is $$ \\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = m + b n(t) - d n(t) $$ where $m$, $b$, and $d$ are the per capita (ie, per individual) rates of migration, birth, and death. In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, $n(t+1)$, based on the life-cycle diagram above, first construct an equation for each event $$n' = n_t + m$$ $$n'' = n' + bn'$$ $$n_{t+1} = n'' - dn''$$ !!! note Note that $m$ and $d$ are now the *fraction* of individuals that migrate and die, and $b$ is the *number* of offspring per parent (above these parameters were all *rates*). Next, substitute $n''$ and then $n'$ into the equation for $n_{t+1}$ to write $n_{t+1}$ in terms of $n_t$ $$ \\begin{aligned} n_{t+1} &= n'' \u2212 dn'' \\\\ &= (n' + bn\u2032) \u2212 d(n' + bn\u2032) \\\\ &= n'(1 + b \u2212 d \u2212 db) \\\\ &= (n_t +m)(1+b\u2212d\u2212db) \\\\ \\end{aligned} $$ We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death). ### v. Analyze the equations - Start by using the equations to simulate and graph the changes to the system over time. - Choose and perform appropriate analyses. - Make sure that the analyses can address the problem. ### vi. Checks and balances - Check the results against data or any known special cases. - Determine how general the results are. - Consider alternatives to the simplest model. - Extend or simplify the model, as appropriate, and repeat steps 2-5. ### vii. Relate the results back to the question - Do the results answer the biological question? - Are the results counter-intuitive? Why? - Interpret the results verbally, and describe conceptually any new insights into the biological process. - Describe potential experiments. <span id='section2'></span> ## 2. Evolution and the Hardy-Weinberg equilibrium <hr> ### Natural selection Natural selection requires three conditions be met ([Lewontin 1970](https://www.annualreviews.org/doi/10.1146/annurev.es.01.110170.000245)): 1. Variation in traits: different individuals have different traits 2. Variation in fitness: different traits have different rates of survival and/or reproduction 3. Fitness is heritable: traits are inherited to some degree Although selection is an important idea, it's impossible to know what kind of observations are consistent with selection if we don't have a baseline to compare against. So let's first construct a model of evolution in the absence of selection using the general steps described in section 1, <a href='#section1'>Constructing a model</a>. ### Evolution in the absence of selection #### i. Formulate the question - **What do you want to know** - How do allele frequencies change over time in the absence of natural selection? - **Boil the question down** - In a diploid population with two variant \"alleles\" of a gene (A and a), how will the frequency of the A allele change over time? - **Simple, biologically reasonable description** - We assume that each diploid genotype (AA, Aa, and aa) has equal fitness and that individuals reproduce and then die (non-overlapping generations). We also assume that individuals produce haploid gametes that form a gamete pool. Gametes within the gamete pool unite at random to produce the next generation of diploid individuals. #### ii. Determine the basic ingredients - **Variables** - $x$ = frequency of AA individuals - $y$ = frequency of Aa individuals - $z$ = frequency of aa individuals - From this we can extract the allele frequencies - the frequency of A is $p = x + y/2$ (ie, all of the alleles in genotype AA ($x$) are A but only 1/2 of the alleles in genotype Aa ($y$) are A) - the frequency of a is $q = 1 - p = y/2 + z$ - **Constraints on these variables** - $x$, $y$, $z$ are $\u22650$ and $\u22641$ - $x+y+z=1$ - **How we'll treat time** - We will follow the genotype frequencies from one generation to the next, using a discrete-time model - **Parameters** - there are no parameters in this model (which is a little bit unusual) #### iii. Qualitatively describe the biological system Gametes unite at random in the gamete pool to produce diploid offspring. These offspring grow into adults that undergo meiosis, creating gametes, and die. Below we draw the life-cycle diagram. <center> ```mermaid graph LR; A((random<br> union)) --diploid--> B((meiosis)); B --haploid--> A; iv. Quantiatively describe the biological system To calculate the frequency of each genoytpe in the next generation we use a table of events. The \"Union\" column indicates the pair of gametes that are meeting each other, the \"Frequency\" column indicates the proportion of gamete pairs in the population with this particular union, and the remaining columns indicate which diploid genotype is created by the union. Union Frequency AA Aa aa A x A \\(p^2\\) 1 0 0 A x a \\(pq\\) 0 1 0 a x A \\(qp\\) 0 1 0 a x a \\(q^2\\) 0 0 1 \\(p^2\\) \\(2pq\\) \\(q^2\\) The genotype frequencies denoted in red are known as the Hardy-Weinberg frequencies . Note that this table shows that populations not at \"Hardy-Weinberg equilibrium\" reach Hardy-Weinberg equilibrium after only one generation of random mating. We now have the frequency of genotypes in the next generation in terms of the allele frequencies in the previous generation, \\[x' = p^2\\] \\[y' = 2pq\\] \\[z' = q^2\\] To understand how the genotype frequencies at Hardy-Weinberg vary with allele frequency, we can plot them. from sympy import * var('p') x = p**2 #frequency of AA at Hardy-Weinberg as a function of the frequency of allele A y = 2*p*(1-p) #freq of Aa z = (1-p)**2 #freq of aa p = plot(x, y, z, #functions that we are plotting (p,0,1), #plot as a function of allele frequency from 0 to 1 xlabel=\"allele frequency, $p$\", ylabel=\"genotype frequency\", legend=True, show=False ) p[0].label='AA' #give legend genotype labels p[1].label='Aa' p[2].label='aa' p.show() From this plot we see, for example, that the frequency of heterozygotes, Aa, is maximized at intermediate allele frequencies. Subbing in \\(p=x+y/2\\) and \\(q=y/2+z\\) we get the recursion equations describing the frequency of diploid genotypes in the next generation as a function of the diploid genotypes in the current generation, \\[x' = (x + y/2)^2\\] \\[y' = 2(x + y/2)(y/2 + z)\\] \\[z' = (y/2 + z)^2\\] v. Analyze the equations Now back to our question. What is the frequency of allele A in the next generation? \\[p' = x' + y'/2\\] \\[p' = p^2 + 2pq/2\\] \\[p' = p(p + q)\\] \\[p' = p\\] vi. Checks and balances Does \\(x' + y' + z' = 1\\) ? \\[ \\begin{aligned} x' + y' + z' &= p^2 + 2pq + q^2\\\\ &= (p+q)^2\\\\ &= (1)^2\\\\ &= 1 \\end{aligned} \\] vii. Relate the results back to the question How do allele frequencies change over time in the absence of natural selection? They don't Data example: blood types Below is a table describing the frequency of three different blood types (and their associated genotype) in a sample of humans from the USA. Blood type M MN N Genotype MM MN NN Observed frequency in USA 0.292 0.496 0.212 We ask, are these genotypes near Hardy-Weinberg equilibrium? First, write the frequency of each genotype as \\(x\\) , \\(y\\) , and \\(z\\) \\[x = 0.292\\] \\[y = 0.496\\] \\[z = 0.212\\] Next, convert the genotype frequencies into allele frequencies \\[p = x + y/2 = 0.540\\] \\[q = y/2 + z = 0.460\\] Now predict what the genotype frequencies would be at Hardy-Weinberg equilibrium \\[x = p^2 = 0.2916\\] \\[y = 2pq = 0.4985\\] \\[z = q^2 = 0.2116\\] These predicted genotype frequencies are exceptionally close to those actually observed, indicating Hardy-Weinberg equilibrium and suggesting an absence of selection on this phenotype.","title":"Lecture 2"},{"location":"lectures/lecture-02/#lecture-2-model-construction","text":"Run notes interactively?","title":"Lecture 2: Model construction"},{"location":"lectures/lecture-02/#lecture-overview","text":"Constructing a model Evolution and the Hardy-Weinberg equilibrium","title":"Lecture overview"},{"location":"lectures/lecture-02/#1-constructing-a-model","text":"Although many problems require specific formulations and assumptions, there are a few general principles to constructing a model.","title":"1. Constructing a model"},{"location":"lectures/lecture-02/#i-formulate-the-question","text":"What do you want to know? Describe the model in the form of a question. Simplify, Simplify! Start with the simplest, biologically reasonable description of the problem.","title":"i. Formulate the question"},{"location":"lectures/lecture-02/#ii-determine-the-basic-ingredients","text":"Define the variables in the model. Describe any constraints on the variables. Describe any interactions between variables. Decide whether you will treat time as discrete or continuous. Choose a time scale (i.e., decide what a time step equals in discrete time and specify whether rates will be measured per second, minute, day, year, generation, etc.). Define the parameters in the model. Describe any constraints on the parameters.","title":"ii. Determine the basic ingredients"},{"location":"lectures/lecture-02/#iii-qualitatively-describe-the-biological-system","text":"For continuous-time models, draw a flow diagram to describe changes to the variables over time. For example, if we had a population experiencing migration, death, and birth we could draw the following. graph LR; A((n)) --birth--> A; B[ ] --migration--> A; A --death--> C[ ]; style B height:0px; style C height:0px; Figure. Continuous time flow diagram. For discrete-time models, draw a life-cycle diagram with all the events that occur each time unit. For example, if we were modeling a population that experienced migration, then birth, then death each time step, we could draw the following. graph LR; A((n)) --migration--> B((n')); B --birth--> C((n'')); C --death--> A; </pre> </center> <center><sup>Figure. Discrete-time life-cycle diagram. </sup></center> For discrete time models with multiple variables and events, construct a table listing the outcome of every event. We'll see an example of this below. ### iv. Quantitatively describe the biological system Using the diagrams and tables as a guide, write down the equations. Perform checks. Are the constraints on the variables still met as time passes? Make sure that the units of the right-hand side equal those on the left-hand side. For continuous-time models, add rates for arrows coming in to the flow diagram and subtract rates for arrows coming out of the flow diagram. For example, in the model shown above the rate of change in the number of individuals, $\\frac{\\mathrm{d} n(t)}{\\mathrm{d} t}$, is $$ \\frac{\\mathrm{d} n(t)}{\\mathrm{d} t} = m + b n(t) - d n(t) $$ where $m$, $b$, and $d$ are the per capita (ie, per individual) rates of migration, birth, and death. In discrete time, you must take into account the order of events when constructing equations. To build an equation for the population size in the next generation, $n(t+1)$, based on the life-cycle diagram above, first construct an equation for each event $$n' = n_t + m$$ $$n'' = n' + bn'$$ $$n_{t+1} = n'' - dn''$$ !!! note Note that $m$ and $d$ are now the *fraction* of individuals that migrate and die, and $b$ is the *number* of offspring per parent (above these parameters were all *rates*). Next, substitute $n''$ and then $n'$ into the equation for $n_{t+1}$ to write $n_{t+1}$ in terms of $n_t$ $$ \\begin{aligned} n_{t+1} &= n'' \u2212 dn'' \\\\ &= (n' + bn\u2032) \u2212 d(n' + bn\u2032) \\\\ &= n'(1 + b \u2212 d \u2212 db) \\\\ &= (n_t +m)(1+b\u2212d\u2212db) \\\\ \\end{aligned} $$ We now have a recursion equation that correctly takes into account the order of the life cycle (migration, birth, death) and the point at which the census is taken (immediately after death). ### v. Analyze the equations - Start by using the equations to simulate and graph the changes to the system over time. - Choose and perform appropriate analyses. - Make sure that the analyses can address the problem. ### vi. Checks and balances - Check the results against data or any known special cases. - Determine how general the results are. - Consider alternatives to the simplest model. - Extend or simplify the model, as appropriate, and repeat steps 2-5. ### vii. Relate the results back to the question - Do the results answer the biological question? - Are the results counter-intuitive? Why? - Interpret the results verbally, and describe conceptually any new insights into the biological process. - Describe potential experiments. <span id='section2'></span> ## 2. Evolution and the Hardy-Weinberg equilibrium <hr> ### Natural selection Natural selection requires three conditions be met ([Lewontin 1970](https://www.annualreviews.org/doi/10.1146/annurev.es.01.110170.000245)): 1. Variation in traits: different individuals have different traits 2. Variation in fitness: different traits have different rates of survival and/or reproduction 3. Fitness is heritable: traits are inherited to some degree Although selection is an important idea, it's impossible to know what kind of observations are consistent with selection if we don't have a baseline to compare against. So let's first construct a model of evolution in the absence of selection using the general steps described in section 1, <a href='#section1'>Constructing a model</a>. ### Evolution in the absence of selection #### i. Formulate the question - **What do you want to know** - How do allele frequencies change over time in the absence of natural selection? - **Boil the question down** - In a diploid population with two variant \"alleles\" of a gene (A and a), how will the frequency of the A allele change over time? - **Simple, biologically reasonable description** - We assume that each diploid genotype (AA, Aa, and aa) has equal fitness and that individuals reproduce and then die (non-overlapping generations). We also assume that individuals produce haploid gametes that form a gamete pool. Gametes within the gamete pool unite at random to produce the next generation of diploid individuals. #### ii. Determine the basic ingredients - **Variables** - $x$ = frequency of AA individuals - $y$ = frequency of Aa individuals - $z$ = frequency of aa individuals - From this we can extract the allele frequencies - the frequency of A is $p = x + y/2$ (ie, all of the alleles in genotype AA ($x$) are A but only 1/2 of the alleles in genotype Aa ($y$) are A) - the frequency of a is $q = 1 - p = y/2 + z$ - **Constraints on these variables** - $x$, $y$, $z$ are $\u22650$ and $\u22641$ - $x+y+z=1$ - **How we'll treat time** - We will follow the genotype frequencies from one generation to the next, using a discrete-time model - **Parameters** - there are no parameters in this model (which is a little bit unusual) #### iii. Qualitatively describe the biological system Gametes unite at random in the gamete pool to produce diploid offspring. These offspring grow into adults that undergo meiosis, creating gametes, and die. Below we draw the life-cycle diagram. <center> ```mermaid graph LR; A((random<br> union)) --diploid--> B((meiosis)); B --haploid--> A;","title":"iii. Qualitatively describe the biological system"},{"location":"lectures/lecture-02/#iv-quantiatively-describe-the-biological-system","text":"To calculate the frequency of each genoytpe in the next generation we use a table of events. The \"Union\" column indicates the pair of gametes that are meeting each other, the \"Frequency\" column indicates the proportion of gamete pairs in the population with this particular union, and the remaining columns indicate which diploid genotype is created by the union. Union Frequency AA Aa aa A x A \\(p^2\\) 1 0 0 A x a \\(pq\\) 0 1 0 a x A \\(qp\\) 0 1 0 a x a \\(q^2\\) 0 0 1 \\(p^2\\) \\(2pq\\) \\(q^2\\) The genotype frequencies denoted in red are known as the Hardy-Weinberg frequencies . Note that this table shows that populations not at \"Hardy-Weinberg equilibrium\" reach Hardy-Weinberg equilibrium after only one generation of random mating. We now have the frequency of genotypes in the next generation in terms of the allele frequencies in the previous generation, \\[x' = p^2\\] \\[y' = 2pq\\] \\[z' = q^2\\] To understand how the genotype frequencies at Hardy-Weinberg vary with allele frequency, we can plot them. from sympy import * var('p') x = p**2 #frequency of AA at Hardy-Weinberg as a function of the frequency of allele A y = 2*p*(1-p) #freq of Aa z = (1-p)**2 #freq of aa p = plot(x, y, z, #functions that we are plotting (p,0,1), #plot as a function of allele frequency from 0 to 1 xlabel=\"allele frequency, $p$\", ylabel=\"genotype frequency\", legend=True, show=False ) p[0].label='AA' #give legend genotype labels p[1].label='Aa' p[2].label='aa' p.show() From this plot we see, for example, that the frequency of heterozygotes, Aa, is maximized at intermediate allele frequencies. Subbing in \\(p=x+y/2\\) and \\(q=y/2+z\\) we get the recursion equations describing the frequency of diploid genotypes in the next generation as a function of the diploid genotypes in the current generation, \\[x' = (x + y/2)^2\\] \\[y' = 2(x + y/2)(y/2 + z)\\] \\[z' = (y/2 + z)^2\\]","title":"iv. Quantiatively describe the biological system"},{"location":"lectures/lecture-02/#v-analyze-the-equations","text":"Now back to our question. What is the frequency of allele A in the next generation? \\[p' = x' + y'/2\\] \\[p' = p^2 + 2pq/2\\] \\[p' = p(p + q)\\] \\[p' = p\\]","title":"v. Analyze the equations"},{"location":"lectures/lecture-02/#vi-checks-and-balances","text":"Does \\(x' + y' + z' = 1\\) ? \\[ \\begin{aligned} x' + y' + z' &= p^2 + 2pq + q^2\\\\ &= (p+q)^2\\\\ &= (1)^2\\\\ &= 1 \\end{aligned} \\]","title":"vi. Checks and balances"},{"location":"lectures/lecture-02/#vii-relate-the-results-back-to-the-question","text":"How do allele frequencies change over time in the absence of natural selection? They don't Data example: blood types Below is a table describing the frequency of three different blood types (and their associated genotype) in a sample of humans from the USA. Blood type M MN N Genotype MM MN NN Observed frequency in USA 0.292 0.496 0.212 We ask, are these genotypes near Hardy-Weinberg equilibrium? First, write the frequency of each genotype as \\(x\\) , \\(y\\) , and \\(z\\) \\[x = 0.292\\] \\[y = 0.496\\] \\[z = 0.212\\] Next, convert the genotype frequencies into allele frequencies \\[p = x + y/2 = 0.540\\] \\[q = y/2 + z = 0.460\\] Now predict what the genotype frequencies would be at Hardy-Weinberg equilibrium \\[x = p^2 = 0.2916\\] \\[y = 2pq = 0.4985\\] \\[z = q^2 = 0.2116\\] These predicted genotype frequencies are exceptionally close to those actually observed, indicating Hardy-Weinberg equilibrium and suggesting an absence of selection on this phenotype.","title":"vii. Relate the results back to the question"},{"location":"lectures/lecture-03/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 3: Exponential & logistic growth Run notes interactively? Lecture overview Introduction Exponential growth Logistic growth 1. Introduction In nature, population sizes change over time in response to a myriad of factors, such as weather competition, predation, disease, ... resource availability The simplest models describing changes in population size are exponential growth and logistic growth which assume a constant environment no interactions with other species The exponential model also assumes no competition among the members of a species for the available resources ( density-independent growth ), while the logistic model includes competition within a species ( density-dependent growth ). Both of these models can be described in discrete and continuous time. We\u2019ll start with the simpler exponential model. 2. Exponential growth Exponential growth in discrete time Imagine we start with \\(n_t\\) individuals at some time \\(t\\) . If we assume that each of these individuals produces \\(b\\) offspring, the number of individuals after reproduction is \\(n_t + n_t b = n_t(1 + b)\\) . If we then assume a fraction \\(d\\) die, the number of individuals remaining after death is \\(n_t(1+b) - n_t(1+b)d = n_t(1+b)(1-d)\\) . With no further events in the life-cycle, this is the expected number of individuals in the next generation, \\(n_{t+1}\\) , which we can write as \\[ \\begin{aligned} n_{t+1} &= n_t(1+b)(1-d) \\\\ &= n_t R \\\\ \\end{aligned} \\] where \\(R=(1+b)(1-d)\\) is a constant referred to as the reproductive factor . This equation, \\(n_{t+1}=n_t R\\) , is the recursion equation for exponential growth. Exponential vs geometric growth Technically this recursion equation describes \"geometric\" growth, since \\(n_t\\) will grow with \\(t\\) as a geometric series, but here we simply call it \"exponential growth in discrete time\" to make a clear connection with exponential growth in continuous time. We can also describe the change in the number of individuals by subtracting off the current number, \\[ \\begin{aligned} \\Delta n &= n_{t+1} - n_t\\\\ &= n_t(R-1) \\end{aligned} \\] This is the difference equation for exponential growth, with discrete-time growth rate \\(r_d = R-1 = (1+b)(1-d)-1 = b - d - bd\\) . Let's plot the dynamics described by these equations for a particular set of parameter values. import numpy as np import matplotlib.pyplot as plt def exponential_discrete(nt, b, d): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt * (1 + b) * (1 - d) # Grow population nd, nt, b, d = [], 1, 0.2, 0.1 #define empty list nd to store population sizes and choose parameter values for t in np.arange(0,100): #for time from 0 to 99 nt = exponential_discrete(nt, b, d) #get the next population size from the recursion equation nd.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show() Exponential growth in continuous time Now assume that each individual continuously gives birth at rate \\(b\\) and dies at rate \\(d\\) . If there are \\(n_t\\) individuals in the population at time \\(t\\) , then the rate of change in the number of individuals is \\[ \\begin{aligned} \\frac{\\mathrm{d} n_t}{\\mathrm{d} t} &= n_t b - n_t d\\\\ &= n_t (b - d)\\\\ &= n_t r_c \\end{aligned} \\] This is the differential equation for exponential growth with continuous-time growth rate \\(r_c = b - d\\) . Note that the growth rate in the discrete-time model was \\(r_d = b - d - b d\\) . The difference between the two growth rates reflects the fact that birth and death cannot happen at the exact same time in the continuous-time model (so there is no \\(b d\\) term), while offspring that are born can die before the next generation in the discrete-time model (causing the \\(b d\\) term). Let's also plot these dynamics. Approximating a differential equation The differential equation describes the change in the population size in an \"infinitesimally\" small amount of time, \\(\\mathrm{d}t\\) . To plot these dynamics we therefore make an approximation, taking \\(\\mathrm{d}t\\) to be small, but not infinitely so. Rearranging the differential equation gives \\(\\mathrm{d}n_t = n_t(b-d)\\mathrm{d}t\\) and the population size after \\(\\mathrm{d}t\\) is therefore \\(n_{t+\\mathrm{d}t} = n_t + \\mathrm{d}n_t\\) . This is a recursion equation that approximates our differential equation. import numpy as np import matplotlib.pyplot as plt def exponential_continuous(nt, b, d, dt): '''approximation of the differential equation giving population size after a small time interval, dt, as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt + nt * (b - d) * dt # Grow population nc, nt, dt = [], 1, 0.1 #define empty list nc to store population sizes and choose parameter values (keep b and d as above) for t in np.arange(0,100,dt): #for time from 0 to 99 by increments of dt nt = exponential_continuous(nt, b, d, dt) #get the next population size from the recursion equation nc.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100, dt), nc) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show() We can now combine our two plots to compare these two predictions, exponential growth in discrete vs. continuous time. # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd, label='discrete time') #plot population size at each time ax.scatter(np.arange(0, 100, dt), nc, label='continuous time') #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.legend() plt.show() Why the difference? The predictions are very similar at first but then noticeably diverge. The time of this divergence depends on the values of parameters \\(b\\) and \\(d\\) . Try increasing or decreasing both \\(b\\) and \\(d\\) and think about why it has that effect. Also think about why the continuous time model predicts a larger population size than the discrete time model. Hint: remember the difference in the growth rates between the two models is \\(bd\\) . The trouble with exponential growth Exponential growth cannot continue indefinitely. Take for example a population of pheasants on an island off the coast of Washington State. Just 8 pheasants were introduced in 1937, but the population then grew exponentially, tripling in size every year ( \\(R=3\\) ) for the first 5 years. Had this population continued to grow exponentially there would have been 7 million of them by the year 1950 and \\(10^{28}\\) by now \u2013 which at 2 kg per pheasant is 3000 times the mass of the earth!! In fact, Lack observed that \u201cthe figures suggest that the increase was slowing down and was about to cease, but at this point the island was occupied by the military and many of the birds shot.\u201d Although populations may initially experience exponential growth, resources eventually become depleted and competition becomes more severe. This suggests that we should change our model assumptions. 3. Logistic growth Exponential growth assumes the growth rate ( \\(r_d\\) , \\(r_c\\) ) is constant. Logistic growth relaxes this assumption, and instead assumes that the growth rate decreases linearly with population size, due to competition for resources within the population. Logistic growth in discrete time In discrete-time, the reproductive factor under logistic growth can be written as \\[R(n_t) = 1 + r\\left(1 - \\frac{n_t}{K}\\right)\\] Notice that each individual is expected to have one offspring ( \\(R=1\\) ) if the intrinsic growth rate (ie, growth rate when rare) is zero, \\(r = 0\\) , or if the population size is at carrying capacity , \\(n_t=K\\) . Try plotting the reproductive factor as a function of \\(n_t\\) for a few different values of \\(r\\) and \\(K\\) . # Reproductive factor for logistic growth def logistic_discrete(nt, r, K): '''reproductive factor in discrete logistic model with growth rate r and carrying capacity k''' return 1 + r * (1 - nt/K) # Compare a few different growth rates and carrying capacities fig, ax = plt.subplots() for r, K in zip([1, 2, 1], [100, 100, 50]): #for each pair of r and K values nt = np.linspace(0, 200) #for a range of population sizes from 0 to 200 R = logistic_discrete(nt, r, K) #calculate the reproductive factor ax.plot(nt, R, label=f\"r = {r}, K = {K}\") #and plot ax.plot(nt, [1 for i in nt], '--', color='gray') #1 line for reference ax.set_xlabel('Population size, $n_t$') ax.set_ylabel('Reproductive factor, $R$') ax.legend(frameon=False) plt.ylim(0,None) plt.show() The population size in the next generation is the expected number of offspring per parent times the the total number of parents \\[n_{t+1} = \\left(1 + r\\left(1-\\frac{n_t}{K}\\right)\\right)n_t\\] This is the recursion equation for logistic growth. This recursion is a non-linear function of \\(n_t\\) ( non-linear means that there is a term in the equation where the term is taken to some power other than 1; here if we expand out the recursion we get a \\(n_t^2\\) term ). This reflects the fact that logistic growth models an interaction between individuals (competition). The change in population size from one generation to the next, \\(\\Delta n\\) , is therefore \\[\\Delta n = n_{t+1} - n_t = r\\left(1 - \\frac{n_t}{K}\\right)n_t\\] Based on this difference equation, when will the population grow in size? Test out your answer by plotting population size over time in the discrete-time logistic model. Try changing the initial population size or carrying capacity so that \\(n_t > K\\) . # Initialize parameters n, nt, r, K = [], 1, 0.1, 100 #list, initial population size, intrinsic growth rate, carrying capacity # Grow population under logistic growth for t in np.arange(0, 100): nt = nt * logistic_discrete(nt, r, K) n.append(nt) # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), n) ax.axhline(100, label=f\"K = {K}\", linestyle='dashed', color='gray') #carrying capacity as dashed line # Add annotations ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show() Logistic growth in continuous time The model of logistic growth in continuous time, as with discrete time, follows from the assumption that each individual has a growth rate that decreases as a linear function of the population size \\(r(1 - n_t/K)\\) . If there are \\(n_t\\) individuals in the population at time \\(t\\) , then the rate of change of the population size will be \\[\\frac{\\mathrm{d}n_t}{\\mathrm{d}t} = r\\left(1 - \\frac{n_t}{K}\\right)n_t\\] This is a differential equation of logistic growth. Note that in both discrete and continuous time the logistic growth model reduces to the exponential growth model as \\(n_t/K\\) approaches 0, i.e., when the population size is much smaller than the carrying capacity \\(n_t << K\\) . An example of logistic growth Dr. Sarah Otto cultured haploid (one copy of each chromosome) and diploid (two copies of each chromosome) populations of Saccharomyces cereviseae . She observed the following population sizes for the two types of cells: Figure. The size of haploid and diploid yeast populations in a controlled laboratory experiment. Although the populations grow nearly exponentially at first, growth decreased as population size increased (i.e., density-dependent growth was observed). The carrying capacity ( \\(K\\) ) is clearly larger for the haploid cells, but do haploid and diploid cells have different intrinsic growth rates ( \\(r\\) )? By fitting the logistic growth model described above to the data, Dr. Otto estimated the parameter values to be Haploid: r = 0.55, K = 3.7 x 10^8 Diploid: r = 0.55, K = 2.3 x 10^8 The growth rates therefore do not differ (visibly or statistically). With these parameter estimates, the logistic model nicely fits the data: Figure. Population size of haploid and diploid yeast fit with logistic growth models. Note: This may be a bit misleading, as such excellent model fits are rarely observed, especially outside the lab! To do: make the above plots in Python","title":"Lecture 3"},{"location":"lectures/lecture-03/#lecture-3-exponential-logistic-growth","text":"Run notes interactively?","title":"Lecture 3: Exponential &amp; logistic growth"},{"location":"lectures/lecture-03/#lecture-overview","text":"Introduction Exponential growth Logistic growth","title":"Lecture overview"},{"location":"lectures/lecture-03/#1-introduction","text":"In nature, population sizes change over time in response to a myriad of factors, such as weather competition, predation, disease, ... resource availability The simplest models describing changes in population size are exponential growth and logistic growth which assume a constant environment no interactions with other species The exponential model also assumes no competition among the members of a species for the available resources ( density-independent growth ), while the logistic model includes competition within a species ( density-dependent growth ). Both of these models can be described in discrete and continuous time. We\u2019ll start with the simpler exponential model.","title":"1. Introduction"},{"location":"lectures/lecture-03/#2-exponential-growth","text":"","title":"2. Exponential growth"},{"location":"lectures/lecture-03/#exponential-growth-in-discrete-time","text":"Imagine we start with \\(n_t\\) individuals at some time \\(t\\) . If we assume that each of these individuals produces \\(b\\) offspring, the number of individuals after reproduction is \\(n_t + n_t b = n_t(1 + b)\\) . If we then assume a fraction \\(d\\) die, the number of individuals remaining after death is \\(n_t(1+b) - n_t(1+b)d = n_t(1+b)(1-d)\\) . With no further events in the life-cycle, this is the expected number of individuals in the next generation, \\(n_{t+1}\\) , which we can write as \\[ \\begin{aligned} n_{t+1} &= n_t(1+b)(1-d) \\\\ &= n_t R \\\\ \\end{aligned} \\] where \\(R=(1+b)(1-d)\\) is a constant referred to as the reproductive factor . This equation, \\(n_{t+1}=n_t R\\) , is the recursion equation for exponential growth. Exponential vs geometric growth Technically this recursion equation describes \"geometric\" growth, since \\(n_t\\) will grow with \\(t\\) as a geometric series, but here we simply call it \"exponential growth in discrete time\" to make a clear connection with exponential growth in continuous time. We can also describe the change in the number of individuals by subtracting off the current number, \\[ \\begin{aligned} \\Delta n &= n_{t+1} - n_t\\\\ &= n_t(R-1) \\end{aligned} \\] This is the difference equation for exponential growth, with discrete-time growth rate \\(r_d = R-1 = (1+b)(1-d)-1 = b - d - bd\\) . Let's plot the dynamics described by these equations for a particular set of parameter values. import numpy as np import matplotlib.pyplot as plt def exponential_discrete(nt, b, d): '''recursion equation giving population size in next time step as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt * (1 + b) * (1 - d) # Grow population nd, nt, b, d = [], 1, 0.2, 0.1 #define empty list nd to store population sizes and choose parameter values for t in np.arange(0,100): #for time from 0 to 99 nt = exponential_discrete(nt, b, d) #get the next population size from the recursion equation nd.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show()","title":"Exponential growth in discrete time"},{"location":"lectures/lecture-03/#exponential-growth-in-continuous-time","text":"Now assume that each individual continuously gives birth at rate \\(b\\) and dies at rate \\(d\\) . If there are \\(n_t\\) individuals in the population at time \\(t\\) , then the rate of change in the number of individuals is \\[ \\begin{aligned} \\frac{\\mathrm{d} n_t}{\\mathrm{d} t} &= n_t b - n_t d\\\\ &= n_t (b - d)\\\\ &= n_t r_c \\end{aligned} \\] This is the differential equation for exponential growth with continuous-time growth rate \\(r_c = b - d\\) . Note that the growth rate in the discrete-time model was \\(r_d = b - d - b d\\) . The difference between the two growth rates reflects the fact that birth and death cannot happen at the exact same time in the continuous-time model (so there is no \\(b d\\) term), while offspring that are born can die before the next generation in the discrete-time model (causing the \\(b d\\) term). Let's also plot these dynamics. Approximating a differential equation The differential equation describes the change in the population size in an \"infinitesimally\" small amount of time, \\(\\mathrm{d}t\\) . To plot these dynamics we therefore make an approximation, taking \\(\\mathrm{d}t\\) to be small, but not infinitely so. Rearranging the differential equation gives \\(\\mathrm{d}n_t = n_t(b-d)\\mathrm{d}t\\) and the population size after \\(\\mathrm{d}t\\) is therefore \\(n_{t+\\mathrm{d}t} = n_t + \\mathrm{d}n_t\\) . This is a recursion equation that approximates our differential equation. import numpy as np import matplotlib.pyplot as plt def exponential_continuous(nt, b, d, dt): '''approximation of the differential equation giving population size after a small time interval, dt, as a function of the population size at this time, nt, and the birth and death rates, b and d.''' return nt + nt * (b - d) * dt # Grow population nc, nt, dt = [], 1, 0.1 #define empty list nc to store population sizes and choose parameter values (keep b and d as above) for t in np.arange(0,100,dt): #for time from 0 to 99 by increments of dt nt = exponential_continuous(nt, b, d, dt) #get the next population size from the recursion equation nc.append(nt) #and append it to the list # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100, dt), nc) #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show() We can now combine our two plots to compare these two predictions, exponential growth in discrete vs. continuous time. # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), nd, label='discrete time') #plot population size at each time ax.scatter(np.arange(0, 100, dt), nc, label='continuous time') #plot population size at each time ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.legend() plt.show() Why the difference? The predictions are very similar at first but then noticeably diverge. The time of this divergence depends on the values of parameters \\(b\\) and \\(d\\) . Try increasing or decreasing both \\(b\\) and \\(d\\) and think about why it has that effect. Also think about why the continuous time model predicts a larger population size than the discrete time model. Hint: remember the difference in the growth rates between the two models is \\(bd\\) .","title":"Exponential growth in continuous time"},{"location":"lectures/lecture-03/#the-trouble-with-exponential-growth","text":"Exponential growth cannot continue indefinitely. Take for example a population of pheasants on an island off the coast of Washington State. Just 8 pheasants were introduced in 1937, but the population then grew exponentially, tripling in size every year ( \\(R=3\\) ) for the first 5 years. Had this population continued to grow exponentially there would have been 7 million of them by the year 1950 and \\(10^{28}\\) by now \u2013 which at 2 kg per pheasant is 3000 times the mass of the earth!! In fact, Lack observed that \u201cthe figures suggest that the increase was slowing down and was about to cease, but at this point the island was occupied by the military and many of the birds shot.\u201d Although populations may initially experience exponential growth, resources eventually become depleted and competition becomes more severe. This suggests that we should change our model assumptions.","title":"The trouble with exponential growth"},{"location":"lectures/lecture-03/#3-logistic-growth","text":"Exponential growth assumes the growth rate ( \\(r_d\\) , \\(r_c\\) ) is constant. Logistic growth relaxes this assumption, and instead assumes that the growth rate decreases linearly with population size, due to competition for resources within the population.","title":"3. Logistic growth"},{"location":"lectures/lecture-03/#logistic-growth-in-discrete-time","text":"In discrete-time, the reproductive factor under logistic growth can be written as \\[R(n_t) = 1 + r\\left(1 - \\frac{n_t}{K}\\right)\\] Notice that each individual is expected to have one offspring ( \\(R=1\\) ) if the intrinsic growth rate (ie, growth rate when rare) is zero, \\(r = 0\\) , or if the population size is at carrying capacity , \\(n_t=K\\) . Try plotting the reproductive factor as a function of \\(n_t\\) for a few different values of \\(r\\) and \\(K\\) . # Reproductive factor for logistic growth def logistic_discrete(nt, r, K): '''reproductive factor in discrete logistic model with growth rate r and carrying capacity k''' return 1 + r * (1 - nt/K) # Compare a few different growth rates and carrying capacities fig, ax = plt.subplots() for r, K in zip([1, 2, 1], [100, 100, 50]): #for each pair of r and K values nt = np.linspace(0, 200) #for a range of population sizes from 0 to 200 R = logistic_discrete(nt, r, K) #calculate the reproductive factor ax.plot(nt, R, label=f\"r = {r}, K = {K}\") #and plot ax.plot(nt, [1 for i in nt], '--', color='gray') #1 line for reference ax.set_xlabel('Population size, $n_t$') ax.set_ylabel('Reproductive factor, $R$') ax.legend(frameon=False) plt.ylim(0,None) plt.show() The population size in the next generation is the expected number of offspring per parent times the the total number of parents \\[n_{t+1} = \\left(1 + r\\left(1-\\frac{n_t}{K}\\right)\\right)n_t\\] This is the recursion equation for logistic growth. This recursion is a non-linear function of \\(n_t\\) ( non-linear means that there is a term in the equation where the term is taken to some power other than 1; here if we expand out the recursion we get a \\(n_t^2\\) term ). This reflects the fact that logistic growth models an interaction between individuals (competition). The change in population size from one generation to the next, \\(\\Delta n\\) , is therefore \\[\\Delta n = n_{t+1} - n_t = r\\left(1 - \\frac{n_t}{K}\\right)n_t\\] Based on this difference equation, when will the population grow in size? Test out your answer by plotting population size over time in the discrete-time logistic model. Try changing the initial population size or carrying capacity so that \\(n_t > K\\) . # Initialize parameters n, nt, r, K = [], 1, 0.1, 100 #list, initial population size, intrinsic growth rate, carrying capacity # Grow population under logistic growth for t in np.arange(0, 100): nt = nt * logistic_discrete(nt, r, K) n.append(nt) # Plot growth fig, ax = plt.subplots() ax.scatter(np.arange(0, 100), n) ax.axhline(100, label=f\"K = {K}\", linestyle='dashed', color='gray') #carrying capacity as dashed line # Add annotations ax.set_xlabel('Time, $t$') ax.set_ylabel('Population size, $n_t$') plt.show()","title":"Logistic growth in discrete time"},{"location":"lectures/lecture-03/#logistic-growth-in-continuous-time","text":"The model of logistic growth in continuous time, as with discrete time, follows from the assumption that each individual has a growth rate that decreases as a linear function of the population size \\(r(1 - n_t/K)\\) . If there are \\(n_t\\) individuals in the population at time \\(t\\) , then the rate of change of the population size will be \\[\\frac{\\mathrm{d}n_t}{\\mathrm{d}t} = r\\left(1 - \\frac{n_t}{K}\\right)n_t\\] This is a differential equation of logistic growth. Note that in both discrete and continuous time the logistic growth model reduces to the exponential growth model as \\(n_t/K\\) approaches 0, i.e., when the population size is much smaller than the carrying capacity \\(n_t << K\\) . An example of logistic growth Dr. Sarah Otto cultured haploid (one copy of each chromosome) and diploid (two copies of each chromosome) populations of Saccharomyces cereviseae . She observed the following population sizes for the two types of cells: Figure. The size of haploid and diploid yeast populations in a controlled laboratory experiment. Although the populations grow nearly exponentially at first, growth decreased as population size increased (i.e., density-dependent growth was observed). The carrying capacity ( \\(K\\) ) is clearly larger for the haploid cells, but do haploid and diploid cells have different intrinsic growth rates ( \\(r\\) )? By fitting the logistic growth model described above to the data, Dr. Otto estimated the parameter values to be Haploid: r = 0.55, K = 3.7 x 10^8 Diploid: r = 0.55, K = 2.3 x 10^8 The growth rates therefore do not differ (visibly or statistically). With these parameter estimates, the logistic model nicely fits the data: Figure. Population size of haploid and diploid yeast fit with logistic growth models. Note: This may be a bit misleading, as such excellent model fits are rarely observed, especially outside the lab! To do: make the above plots in Python","title":"Logistic growth in continuous time"},{"location":"lectures/lecture-10/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 10: Linear algebra I Run notes interactively? Lecture overview Linear algebra review What are vectors? What is a matrix? Vector and matrix operations Inverse matrices Solving systems of equations 1. Linear algebra review Until now, we have been dealing with problems in a single variable changing over time. Often, dynamical systems involve more than one variable. For instance, the Lotka-Volterra model describes the situation in which there are competing species, whose growth rates depend on exactly how many individuals of each species are present. As a simple example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 at time \\(t\\) be \\(x_t\\) and let the number of birds on island 2 at time \\(t\\) be \\(y_t\\) . Further, let the fraction dispersing from island 1 to island 2 be \\(\\alpha\\) and let the fraction dispersing from island 2 to island 1 be \\(\\beta\\) . The number of birds on each island in the next generation will be \\[ x_{t+1} = (1-\\alpha) x_{t} + \\beta y_{t} \\\\ y_{t+1} = \\alpha x_{t} + (1-\\beta) y_{t} \\] These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(x\\) and \\(y\\) and nothing more complicated such as \\(x^2\\) or \\(e^x\\) ). \\[ x_{t+1} = (1-\\alpha) x_{t} + \\beta y_{t} \\\\ y_{t+1} = \\alpha x_{t} + (1-\\beta) y_{t} \\] Linear systems of equations like these can also be written in matrix form: \\[ \\begin{equation*} \\underbrace{\\begin{bmatrix} x_{t+1} \\\\ y_{t+1} \\end{bmatrix}}_{\\text{vector}, \\vec{v}_{t+1}} = \\underbrace{\\begin{bmatrix} 1-\\alpha & \\beta \\alpha & 1-\\beta \\end{bmatrix}}_{\\text{matrix},\\ \\textbf{M}} \\underbrace{\\begin{bmatrix} x_{t} \\\\ y_{t} \\end{bmatrix}}_{\\text{vector}, \\vec{v}_t} \\end{equation*} \\] That is, the vector representing the number of birds on each island is written as the product of a matrix times the vector in the previous time step. There are rules of linear algebra that can help us solve this set of linear equations as well as any other set of linear equations. First we have to review some basics of linear algebra. 2. What are vectors? Vectors are lists of elements (elements being numbers, parameters, functions, or variables) A column vector has elements arranged from top to bottom \\[ \\begin{equation*} \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 5 \\\\ 9 \\\\ 7 \\end{bmatrix}, \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{equation*} \\] A row vector has elements arranged from left to right \\[ [5,2], [1,5,9,7], [x,y], [x,y,z], [x_1,x_2,\\cdots,x_n], \\begin{bmatrix}x_1 & x_2 & \\cdots & x_n\\end{bmatrix} \\] We will write vectors with an arrow on top \\[ \\vec{x} = [x_1,x_2,\\cdots,x_n] \\] The number of elements in the vector indicates its dimension , \\(n\\) . For instance, the row vector \\([x,y]\\) has dimension \\(n=2\\) . You can represent a vector with a line in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by the vector 3. What is a matrix? An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns \\[ \\begin{equation*} \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{bmatrix}, \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\begin{bmatrix} 75 & 67 \\\\ 66 & 34 \\\\ 12 & 14 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{equation*} \\] The last example is a special type of matrix known as an identity matrix, with \\(1\\) on the diagonal and \\(0\\) everywhere else. We will write matrices in boldface \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{bmatrix} \\] Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors. Note that row vectors are matrices with only one row, \\(m=1\\) , and column vectors are matrices with only one column, \\(n=1\\) . A matrix with an equal number of rows and columns, \\(m=n\\) , is a square matrix . A matrix with zeros everywhere except along the diagonal is called a diagonal matrix \\[ \\begin{bmatrix} a & 0 & 0 \\\\ 0 & b & 0 \\\\ 0 & 0 & c \\end{bmatrix} \\] And a special case of this with 1s along the diagonal is called the identity matrix \\[ \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\] A matrix with all zeros below the diagonal is called an upper trianglular matrix \\[ \\begin{bmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{bmatrix} \\] A matrix with all zeros above the diagonal is called an lower trianglular matrix \\[ \\begin{bmatrix} a & 0 & 0 \\\\ b & d & 0 \\\\ c & e & f \\end{bmatrix} \\] It is sometimes useful to chop a matrix up into multiple blocks \\[ \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix} \\] where \\(\\mathbf{A}=\\begin{bmatrix} a & b \\\\ d & e\\end{bmatrix}\\) , \\(\\mathbf{B}=\\begin{bmatrix} c\\\\ f\\end{bmatrix}\\) , \\(\\mathbf{C}=\\begin{bmatrix} g & h \\end{bmatrix}\\) , and \\(\\mathbf{D}=\\begin{bmatrix} i\\end{bmatrix}\\) . This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros. For instance, when \\(\\mathbf{B}=\\begin{bmatrix} 0\\\\0\\end{bmatrix}\\) or \\(\\mathbf{C}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\) , we have a block triangular matrix . And when \\(\\mathbf{B}=\\begin{bmatrix} 0\\\\0\\end{bmatrix}\\) and \\(\\mathbf{C}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\) , we have a block diagonal matrix . Finally, it is sometimes useful to \"transpose\" a matrix, which exchanges the rows and columns \\[ \\begin{bmatrix} a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{bmatrix}^\\intercal = \\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ a_3 & b_3 \\end{bmatrix} \\] 4. Vector and matrix operations Matrix \"arithmetic\" Vector and matrix addition (and subtraction) is straightforward, entry-by-entry: \\[ \\begin{equation*} \\begin{bmatrix} a \\\\ b \\end{bmatrix} + \\begin{bmatrix} c \\\\ d \\end{bmatrix} = \\begin{bmatrix} a+c \\\\ b+d \\end{bmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} + \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix} a+e & b+f \\\\ c+g & d+h \\end{bmatrix} \\end{equation*} \\] Warning: The vectors (or matrices) added together must have the same dimension. Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward: \\[ \\begin{equation*} \\alpha * \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\alpha a \\\\ \\alpha b \\end{bmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha * \\begin{bmatrix} a & b\\\\ c & d \\end{bmatrix} = \\begin{bmatrix} \\alpha a & \\alpha b\\\\ \\alpha c & \\alpha d \\end{bmatrix} \\end{equation*} \\] Vector and matrix \\textbf{multiplication} is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum: \\[ \\begin{equation*} [a, b, c] \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = ax + by + cz \\end{equation*} \\] This is referred to as the dot product . To multiply a matrix by a vector, this procedure is repeated first for the first row of the matrix, then for the second row of the matrix, etc: \\[ \\begin{equation*} \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} ax + by + cz \\\\ dx + ey + fz \\\\ gx + hy + iz \\\\ \\end{bmatrix} \\end{equation*} \\] To multiply a matrix by a matrix, this procedure is then repeated first for the first column of the second matrix and then for the second column of the second matrix, etc: \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{bmatrix} \\end{equation*} \\] Warning: The \\(m \\times n\\) matrix \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix. The resulting matrix will then be an \\(m \\times p\\) matrix. As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\) . This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\) , by \\(\\mathbf{D}\\) , we need to do so on the same side, \\(\\mathbf{ABD} = \\mathbf{CD}\\) or \\(\\mathbf{DAB} = \\mathbf{DC}\\) . On the other hand, matrix multiplication does satisfy the following laws: \\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law) \\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law) \\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law) \\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars) Multiplication between the identity matrix and any vector, \\(\\vec{v}\\) , or square matrix, \\(\\mathbf{M}\\) , has no effect \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\] Other useful operations The trace of a matrix is the sum of the diagonal elements \\[ \\mathrm{Tr} \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{bmatrix} = a + e + i \\] The determinant of a \\(2 \\times 2\\) matrix is: \\[ \\begin{equation*} \\text{Det} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{vmatrix} a & b\\\\ c & d \\end{vmatrix} =ad-bc \\end{equation*} \\] The determinant of a \\(3 \\times 3\\) matrix is: \\[ \\begin{equation*} \\begin{vmatrix} x_{11} & x_{12} & x_{13} \\\\ x_{21} & x_{22} & x_{23} \\\\ x_{31} & x_{32} & x_{33} \\end{vmatrix} = x_{11} \\begin{vmatrix} x_{22} & x_{23} \\\\ x_{32} & x_{33} \\end{vmatrix} - x_{12} \\begin{vmatrix} x_{21} & x_{23} \\\\ x_{31} & x_{33} \\end{vmatrix} + x_{13} \\begin{vmatrix} x_{21} & x_{22} \\\\ x_{31} & x_{32} \\end{vmatrix} \\end{equation*} \\] The determinant of an \\(n \\times n\\) matrix is obtained by taking the first row and multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on \\[ \\begin{vmatrix} \\mathbf{M} \\end{vmatrix} = \\sum_{j=1}^n (-1)^{j+1} x_{1j} \\begin{vmatrix} \\mathbf{M}_{1j} \\end{vmatrix} \\] where \\(\\mathbf{M}_{1j}\\) is the matrix \\(\\mathbf{M}\\) with the first row deleted and the \\(j^{th}\\) column deleted. More generally, we can move along any row \\(k\\) in a square matrix of any dimension \\(n\\) \\[ |\\mathbf{M}| = (-1)^{k+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{kj} |\\mathbf{M_{kj}}| \\] One can analogously move along column \\(k\\) \\[ |\\mathbf{M}| = (-1)^{k+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{jk} |\\mathbf{M_{jk}}| \\] A few useful rules emerge from this: The determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\) The determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii}\\) The determinant of a block-diagonal or -triangular matrix is the product of the determinants of the diagonal submatrices When \\(|\\mathbf{M}|=0\\) it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\) . But in the end, for our purposes, it is often much more convenient to simply use a computer program such as SymPy or SageMath to calculate the determinant, especially if the matrix is larger than 2x2. 5. Inverse matrices For matrices, there is no such thing as division, but the analogy is the inverse . A square \\(m\\times m\\) matrix \\(\\mathbf{A}\\) is \\textbf{invertible} if it may be multiplied by another matrix to get the identity matrix. We call this second matrix the inverse of the first: \\[ \\begin{equation*} \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I} = \\mathbf{A}^{-1}\\mathbf{A} \\end{equation*} \\] There are rules to find the inverse of a matrix (when it is invertible), one of which is just to ask SymPy/SageMath, but for a \\(2 \\times 2\\) matrix you should memorize the inverse: \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} =\\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} = \\begin{bmatrix} \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\ \\frac{-c}{ad-bc} & \\frac{a}{ad-bc} \\end{bmatrix} \\end{equation*} \\] One other simple case is when a \\(n\\times n\\) matrix, \\(\\mathbf{M}\\) , is diagonal \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} 1/m_{11} & 0 & \\cdots & 0\\\\ 0 & 1/m_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 1/m_{nn}\\\\ \\end{bmatrix} \\] Note that when the determinant is zero, no inverse exist and the matrix is non-invertible or singular . 5. Solving systems of equations With all this in hand, let's start to get an idea of why this might be useful. Consider a model for the number of tRNA molecules unbound or bound to an amino acid. Let the number of unbound tRNA molecules within a cell be \\(n_1\\) and the number bound be \\(n_2\\) . Further, let \\(\\alpha\\) be the rate of tRNA production by transcription, \\(\\beta\\) be the rate of amino acid binding, \\(\\gamma\\) be the rate of amino acid loss, and \\(\\delta\\) be the rate of tRNA degredation. Then we might model this system as \\[ \\begin{align*} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= \\alpha - \\beta n_1 + \\gamma n_2 - \\delta n_1\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= \\beta n_1 - \\gamma n_2 - \\delta n_2 \\end{align*} \\] We can write this is matrix form as \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} - \\vec{v} \\] with \\(\\vec{n} = \\begin{bmatrix}n_1 \\\\ n_2 \\end{bmatrix}\\) , \\(\\mathbf{M} = \\begin{bmatrix}-\\beta-\\delta & \\gamma \\\\ \\beta & -\\gamma-\\delta \\end{bmatrix}\\) , and \\(\\vec{v} = \\begin{bmatrix}-\\alpha \\\\ 0 \\end{bmatrix}\\) . The equilibria, \\(\\hat{\\vec{n}}\\) , are then found by setting \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\) and using the inverse \\[ \\begin{align*} 0 &= \\mathbf{M}\\hat{\\vec{n}} - \\vec{v}\\\\ \\vec{v} &= \\mathbf{M}\\hat{\\vec{n}}\\\\ \\mathbf{M}^{-1}\\vec{v} &= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ \\mathbf{M}^{-1}\\vec{v} &= \\hat{\\vec{n}} \\end{align*} \\] Since \\(\\mathbf{M} = \\begin{bmatrix}-\\beta-\\delta & \\gamma \\\\ \\beta & -\\gamma-\\delta \\end{bmatrix}\\) , we have \\[ |\\mathbf{M}| = (\\beta+\\delta)(\\gamma+\\delta) - \\beta \\gamma = \\delta(\\beta + \\gamma + \\delta) \\] and \\[ \\mathbf{M}^{-1} = \\frac{1}{|\\mathbf{M}|}\\begin{bmatrix}-\\gamma-\\delta & -\\gamma \\\\ -\\beta & -\\beta-\\delta \\end{bmatrix} \\] Multiplying by \\(\\vec{v}\\) gives the equilibrium \\[ \\hat{\\vec{n}} = \\mathbf{M}^{-1} \\vec{v} = \\frac{1}{|\\mathbf{M}|}\\begin{bmatrix}-\\gamma-\\delta & -\\gamma \\\\ -\\beta & -\\beta-\\delta \\end{bmatrix} \\begin{bmatrix} -\\alpha \\\\ 0 \\end{bmatrix} = \\frac{1}{|\\mathbf{M}|} \\begin{bmatrix}\\alpha(\\gamma+\\delta) \\\\ \\alpha \\beta\\end{bmatrix} \\]","title":"Lecture 10"},{"location":"lectures/lecture-10/#lecture-10-linear-algebra-i","text":"Run notes interactively?","title":"Lecture 10: Linear algebra I"},{"location":"lectures/lecture-10/#lecture-overview","text":"Linear algebra review What are vectors? What is a matrix? Vector and matrix operations Inverse matrices Solving systems of equations","title":"Lecture overview"},{"location":"lectures/lecture-10/#1-linear-algebra-review","text":"Until now, we have been dealing with problems in a single variable changing over time. Often, dynamical systems involve more than one variable. For instance, the Lotka-Volterra model describes the situation in which there are competing species, whose growth rates depend on exactly how many individuals of each species are present. As a simple example with more than one variable, consider a model tracking the number of birds on two islands. Let the number of birds on island 1 at time \\(t\\) be \\(x_t\\) and let the number of birds on island 2 at time \\(t\\) be \\(y_t\\) . Further, let the fraction dispersing from island 1 to island 2 be \\(\\alpha\\) and let the fraction dispersing from island 2 to island 1 be \\(\\beta\\) . The number of birds on each island in the next generation will be \\[ x_{t+1} = (1-\\alpha) x_{t} + \\beta y_{t} \\\\ y_{t+1} = \\alpha x_{t} + (1-\\beta) y_{t} \\] These equations are linear functions of the variables (i.e., they contain only constant multiples of \\(x\\) and \\(y\\) and nothing more complicated such as \\(x^2\\) or \\(e^x\\) ). \\[ x_{t+1} = (1-\\alpha) x_{t} + \\beta y_{t} \\\\ y_{t+1} = \\alpha x_{t} + (1-\\beta) y_{t} \\] Linear systems of equations like these can also be written in matrix form: \\[ \\begin{equation*} \\underbrace{\\begin{bmatrix} x_{t+1} \\\\ y_{t+1} \\end{bmatrix}}_{\\text{vector}, \\vec{v}_{t+1}} = \\underbrace{\\begin{bmatrix} 1-\\alpha & \\beta \\alpha & 1-\\beta \\end{bmatrix}}_{\\text{matrix},\\ \\textbf{M}} \\underbrace{\\begin{bmatrix} x_{t} \\\\ y_{t} \\end{bmatrix}}_{\\text{vector}, \\vec{v}_t} \\end{equation*} \\] That is, the vector representing the number of birds on each island is written as the product of a matrix times the vector in the previous time step. There are rules of linear algebra that can help us solve this set of linear equations as well as any other set of linear equations. First we have to review some basics of linear algebra.","title":"1. Linear algebra review"},{"location":"lectures/lecture-10/#2-what-are-vectors","text":"Vectors are lists of elements (elements being numbers, parameters, functions, or variables) A column vector has elements arranged from top to bottom \\[ \\begin{equation*} \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 5 \\\\ 9 \\\\ 7 \\end{bmatrix}, \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\end{equation*} \\] A row vector has elements arranged from left to right \\[ [5,2], [1,5,9,7], [x,y], [x,y,z], [x_1,x_2,\\cdots,x_n], \\begin{bmatrix}x_1 & x_2 & \\cdots & x_n\\end{bmatrix} \\] We will write vectors with an arrow on top \\[ \\vec{x} = [x_1,x_2,\\cdots,x_n] \\] The number of elements in the vector indicates its dimension , \\(n\\) . For instance, the row vector \\([x,y]\\) has dimension \\(n=2\\) . You can represent a vector with a line in \\(n\\) dimensions, connecting the origin with a point whose coordinates are given by the vector","title":"2. What are vectors?"},{"location":"lectures/lecture-10/#3-what-is-a-matrix","text":"An \\(m \\times n\\) matrix has \\(m\\) rows and \\(n\\) columns \\[ \\begin{equation*} \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{bmatrix}, \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\begin{bmatrix} 75 & 67 \\\\ 66 & 34 \\\\ 12 & 14 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{equation*} \\] The last example is a special type of matrix known as an identity matrix, with \\(1\\) on the diagonal and \\(0\\) everywhere else. We will write matrices in boldface \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n}\\\\ x_{21} & x_{22} & \\cdots & x_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ x_{m1} & x_{m2} & \\cdots & x_{mn}\\\\ \\end{bmatrix} \\] Like vectors, matrices have a graphical/geometrical interpretation: they stretch and rotate vectors. Note that row vectors are matrices with only one row, \\(m=1\\) , and column vectors are matrices with only one column, \\(n=1\\) . A matrix with an equal number of rows and columns, \\(m=n\\) , is a square matrix . A matrix with zeros everywhere except along the diagonal is called a diagonal matrix \\[ \\begin{bmatrix} a & 0 & 0 \\\\ 0 & b & 0 \\\\ 0 & 0 & c \\end{bmatrix} \\] And a special case of this with 1s along the diagonal is called the identity matrix \\[ \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\] A matrix with all zeros below the diagonal is called an upper trianglular matrix \\[ \\begin{bmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{bmatrix} \\] A matrix with all zeros above the diagonal is called an lower trianglular matrix \\[ \\begin{bmatrix} a & 0 & 0 \\\\ b & d & 0 \\\\ c & e & f \\end{bmatrix} \\] It is sometimes useful to chop a matrix up into multiple blocks \\[ \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{C} & \\mathbf{D} \\end{bmatrix} \\] where \\(\\mathbf{A}=\\begin{bmatrix} a & b \\\\ d & e\\end{bmatrix}\\) , \\(\\mathbf{B}=\\begin{bmatrix} c\\\\ f\\end{bmatrix}\\) , \\(\\mathbf{C}=\\begin{bmatrix} g & h \\end{bmatrix}\\) , and \\(\\mathbf{D}=\\begin{bmatrix} i\\end{bmatrix}\\) . This is especially helpful when the block form has off-diagonal submatrices consisting of all zeros. For instance, when \\(\\mathbf{B}=\\begin{bmatrix} 0\\\\0\\end{bmatrix}\\) or \\(\\mathbf{C}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\) , we have a block triangular matrix . And when \\(\\mathbf{B}=\\begin{bmatrix} 0\\\\0\\end{bmatrix}\\) and \\(\\mathbf{C}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\) , we have a block diagonal matrix . Finally, it is sometimes useful to \"transpose\" a matrix, which exchanges the rows and columns \\[ \\begin{bmatrix} a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{bmatrix}^\\intercal = \\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ a_3 & b_3 \\end{bmatrix} \\]","title":"3. What is a matrix?"},{"location":"lectures/lecture-10/#4-vector-and-matrix-operations","text":"","title":"4. Vector and matrix operations"},{"location":"lectures/lecture-10/#matrix-arithmetic","text":"Vector and matrix addition (and subtraction) is straightforward, entry-by-entry: \\[ \\begin{equation*} \\begin{bmatrix} a \\\\ b \\end{bmatrix} + \\begin{bmatrix} c \\\\ d \\end{bmatrix} = \\begin{bmatrix} a+c \\\\ b+d \\end{bmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} + \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix} a+e & b+f \\\\ c+g & d+h \\end{bmatrix} \\end{equation*} \\] Warning: The vectors (or matrices) added together must have the same dimension. Vector and matrix multiplication by a scalar (which may be a constant, a variable, or a function, but not a matrix or a vector) is also straightforward: \\[ \\begin{equation*} \\alpha * \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\alpha a \\\\ \\alpha b \\end{bmatrix} \\end{equation*} \\] \\[ \\begin{equation*} \\alpha * \\begin{bmatrix} a & b\\\\ c & d \\end{bmatrix} = \\begin{bmatrix} \\alpha a & \\alpha b\\\\ \\alpha c & \\alpha d \\end{bmatrix} \\end{equation*} \\] Vector and matrix \\textbf{multiplication} is a bit trickier, but is based on the fact that a row vector times a column vector is equal to the sum: \\[ \\begin{equation*} [a, b, c] \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = ax + by + cz \\end{equation*} \\] This is referred to as the dot product . To multiply a matrix by a vector, this procedure is repeated first for the first row of the matrix, then for the second row of the matrix, etc: \\[ \\begin{equation*} \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} ax + by + cz \\\\ dx + ey + fz \\\\ gx + hy + iz \\\\ \\end{bmatrix} \\end{equation*} \\] To multiply a matrix by a matrix, this procedure is then repeated first for the first column of the second matrix and then for the second column of the second matrix, etc: \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix} ae + bg & af + bh \\\\ ce + dg & cf + dh \\end{bmatrix} \\end{equation*} \\] Warning: The \\(m \\times n\\) matrix \\(\\mathbf{A}\\) can be multiplied on the right by \\(\\mathbf{B}\\) only if \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix. The resulting matrix will then be an \\(m \\times p\\) matrix. As opposed to basic algebra, matrix multiplication is not commutative. That is, \\(\\mathbf{AB}\\) does not generally equal \\(\\mathbf{BA}\\) . This means that if we want to multiply both sides of an equation, e.g., \\(\\mathbf{AB} = \\mathbf{C}\\) , by \\(\\mathbf{D}\\) , we need to do so on the same side, \\(\\mathbf{ABD} = \\mathbf{CD}\\) or \\(\\mathbf{DAB} = \\mathbf{DC}\\) . On the other hand, matrix multiplication does satisfy the following laws: \\((\\mathbf{AB})\\mathbf{C} = \\mathbf{A}(\\mathbf{BC})\\) (associative law) \\(\\mathbf{A}(\\mathbf{B+C}) = \\mathbf{AB}+\\mathbf{AC}\\) (distributive law) \\((\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC}+\\mathbf{BC}\\) (distributive law) \\(\\alpha(\\mathbf{AB}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B}) = (\\mathbf{A}\\mathbf{B})\\alpha\\) (commutative law for scalars) Multiplication between the identity matrix and any vector, \\(\\vec{v}\\) , or square matrix, \\(\\mathbf{M}\\) , has no effect \\[ \\mathbf{I}\\vec{v}=\\vec{v} \\mathbf{I}\\mathbf{M}=\\mathbf{M}\\mathbf{I}=\\mathbf{M} \\]","title":"Matrix \"arithmetic\""},{"location":"lectures/lecture-10/#other-useful-operations","text":"The trace of a matrix is the sum of the diagonal elements \\[ \\mathrm{Tr} \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ \\end{bmatrix} = a + e + i \\] The determinant of a \\(2 \\times 2\\) matrix is: \\[ \\begin{equation*} \\text{Det} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{vmatrix} a & b\\\\ c & d \\end{vmatrix} =ad-bc \\end{equation*} \\] The determinant of a \\(3 \\times 3\\) matrix is: \\[ \\begin{equation*} \\begin{vmatrix} x_{11} & x_{12} & x_{13} \\\\ x_{21} & x_{22} & x_{23} \\\\ x_{31} & x_{32} & x_{33} \\end{vmatrix} = x_{11} \\begin{vmatrix} x_{22} & x_{23} \\\\ x_{32} & x_{33} \\end{vmatrix} - x_{12} \\begin{vmatrix} x_{21} & x_{23} \\\\ x_{31} & x_{33} \\end{vmatrix} + x_{13} \\begin{vmatrix} x_{21} & x_{22} \\\\ x_{31} & x_{32} \\end{vmatrix} \\end{equation*} \\] The determinant of an \\(n \\times n\\) matrix is obtained by taking the first row and multiplying the first element of the first row by the determinant of the matrix created by deleting the first row and first column minus the second element of the first row times the determinant of the matrix created by deleting the first row and second column plus the third element... and so on \\[ \\begin{vmatrix} \\mathbf{M} \\end{vmatrix} = \\sum_{j=1}^n (-1)^{j+1} x_{1j} \\begin{vmatrix} \\mathbf{M}_{1j} \\end{vmatrix} \\] where \\(\\mathbf{M}_{1j}\\) is the matrix \\(\\mathbf{M}\\) with the first row deleted and the \\(j^{th}\\) column deleted. More generally, we can move along any row \\(k\\) in a square matrix of any dimension \\(n\\) \\[ |\\mathbf{M}| = (-1)^{k+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{kj} |\\mathbf{M_{kj}}| \\] One can analogously move along column \\(k\\) \\[ |\\mathbf{M}| = (-1)^{k+1}\\sum_{j=1}^{n}(-1)^{j+1}m_{jk} |\\mathbf{M_{jk}}| \\] A few useful rules emerge from this: The determinant of a matrix is the same as the determinant of its transpose, \\(|\\mathbf{M}| = |\\mathbf{M}^\\intercal|\\) The determinant of a diagonal or triangular matrix is the product of the diagonal elements, \\(|\\mathbf{M}| = \\prod_{i=1}^n m_{ii}\\) The determinant of a block-diagonal or -triangular matrix is the product of the determinants of the diagonal submatrices When \\(|\\mathbf{M}|=0\\) it means that the rows are not linearly independent, that is, some row \\(\\vec{r}_k\\) can be written as \\(a_1 \\vec{r}_1 + \\cdots + a_{k-1} \\vec{r}_{k-1} + a_{k+1} \\vec{r}_{k+1} + \\cdots + a_n \\vec{r}_n\\) . But in the end, for our purposes, it is often much more convenient to simply use a computer program such as SymPy or SageMath to calculate the determinant, especially if the matrix is larger than 2x2.","title":"Other useful operations"},{"location":"lectures/lecture-10/#5-inverse-matrices","text":"For matrices, there is no such thing as division, but the analogy is the inverse . A square \\(m\\times m\\) matrix \\(\\mathbf{A}\\) is \\textbf{invertible} if it may be multiplied by another matrix to get the identity matrix. We call this second matrix the inverse of the first: \\[ \\begin{equation*} \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I} = \\mathbf{A}^{-1}\\mathbf{A} \\end{equation*} \\] There are rules to find the inverse of a matrix (when it is invertible), one of which is just to ask SymPy/SageMath, but for a \\(2 \\times 2\\) matrix you should memorize the inverse: \\[ \\begin{equation*} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} =\\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} = \\begin{bmatrix} \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\ \\frac{-c}{ad-bc} & \\frac{a}{ad-bc} \\end{bmatrix} \\end{equation*} \\] One other simple case is when a \\(n\\times n\\) matrix, \\(\\mathbf{M}\\) , is diagonal \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} 1/m_{11} & 0 & \\cdots & 0\\\\ 0 & 1/m_{22} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 1/m_{nn}\\\\ \\end{bmatrix} \\] Note that when the determinant is zero, no inverse exist and the matrix is non-invertible or singular .","title":"5. Inverse matrices"},{"location":"lectures/lecture-10/#5-solving-systems-of-equations","text":"With all this in hand, let's start to get an idea of why this might be useful. Consider a model for the number of tRNA molecules unbound or bound to an amino acid. Let the number of unbound tRNA molecules within a cell be \\(n_1\\) and the number bound be \\(n_2\\) . Further, let \\(\\alpha\\) be the rate of tRNA production by transcription, \\(\\beta\\) be the rate of amino acid binding, \\(\\gamma\\) be the rate of amino acid loss, and \\(\\delta\\) be the rate of tRNA degredation. Then we might model this system as \\[ \\begin{align*} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= \\alpha - \\beta n_1 + \\gamma n_2 - \\delta n_1\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= \\beta n_1 - \\gamma n_2 - \\delta n_2 \\end{align*} \\] We can write this is matrix form as \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M}\\vec{n} - \\vec{v} \\] with \\(\\vec{n} = \\begin{bmatrix}n_1 \\\\ n_2 \\end{bmatrix}\\) , \\(\\mathbf{M} = \\begin{bmatrix}-\\beta-\\delta & \\gamma \\\\ \\beta & -\\gamma-\\delta \\end{bmatrix}\\) , and \\(\\vec{v} = \\begin{bmatrix}-\\alpha \\\\ 0 \\end{bmatrix}\\) . The equilibria, \\(\\hat{\\vec{n}}\\) , are then found by setting \\(\\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t}=0\\) and using the inverse \\[ \\begin{align*} 0 &= \\mathbf{M}\\hat{\\vec{n}} - \\vec{v}\\\\ \\vec{v} &= \\mathbf{M}\\hat{\\vec{n}}\\\\ \\mathbf{M}^{-1}\\vec{v} &= \\mathbf{M}^{-1}\\mathbf{M}\\hat{\\vec{n}}\\\\ \\mathbf{M}^{-1}\\vec{v} &= \\hat{\\vec{n}} \\end{align*} \\] Since \\(\\mathbf{M} = \\begin{bmatrix}-\\beta-\\delta & \\gamma \\\\ \\beta & -\\gamma-\\delta \\end{bmatrix}\\) , we have \\[ |\\mathbf{M}| = (\\beta+\\delta)(\\gamma+\\delta) - \\beta \\gamma = \\delta(\\beta + \\gamma + \\delta) \\] and \\[ \\mathbf{M}^{-1} = \\frac{1}{|\\mathbf{M}|}\\begin{bmatrix}-\\gamma-\\delta & -\\gamma \\\\ -\\beta & -\\beta-\\delta \\end{bmatrix} \\] Multiplying by \\(\\vec{v}\\) gives the equilibrium \\[ \\hat{\\vec{n}} = \\mathbf{M}^{-1} \\vec{v} = \\frac{1}{|\\mathbf{M}|}\\begin{bmatrix}-\\gamma-\\delta & -\\gamma \\\\ -\\beta & -\\beta-\\delta \\end{bmatrix} \\begin{bmatrix} -\\alpha \\\\ 0 \\end{bmatrix} = \\frac{1}{|\\mathbf{M}|} \\begin{bmatrix}\\alpha(\\gamma+\\delta) \\\\ \\alpha \\beta\\end{bmatrix} \\]","title":"5. Solving systems of equations"},{"location":"lectures/lecture-11/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 11: Linear algebra II Run notes interactively? Lecture overview Eigenvalues and eigenvectors 1. Eigenvalues and eigenvectors What are eigenvalues and eigenvectors? A number \\(\\lambda\\) is an eigenvalue of matrix \\(\\textbf{M}\\) if there exists a non-zero vector, \\(\\vec{v}\\) , that satisfies the equation: \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] Every non-zero vector \\(\\vec{v}\\) satisfying this relation is an (right) eigenvector of \\(\\mathbf{M}\\) belonging to the eigenvalue, \\(\\lambda\\) . Finding eigenvalues of a matrix To find the eigenvalues of a matrix, we can rearrange the above equation, using the distributive law for matrix multiplication: \\[ \\begin{aligned} \\mathbf{M}\\vec{v} &= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &= \\vec{0}\\\\ \\vec{v} &= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix , and \\(\\vec{0}\\) is a vector of zeros. This suggests that \\(\\vec{v}\\) is a vector of zeros, \\(\\vec{v}=\\vec{0}\\) . But we've said this is not the case. So this last step must not be valid \\(\\implies (\\mathbf{M} - \\lambda\\mathbf{I})\\) is not invertible. Therefore the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) must be zero! \\(\\implies\\) solve for \\(\\lambda\\) without \\(\\vec{v}\\) The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\) , the roots of which are the eigenvalues of the matrix \\(\\mathbf{M}\\) : \\(\\lambda_1,\\lambda_2,...\\lambda_n\\) . For example, in the \\(n=2\\) case \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\\\ &= \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} - \\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix}\\\\ &= \\begin{bmatrix} m_{11} - \\lambda & m_{12} \\\\ m_{21} & m_{22} - \\lambda \\end{bmatrix} \\end{aligned} \\] so that \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | &= 0\\\\ (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12} &= 0\\\\ \\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12}) &= 0\\\\ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}) &= 0 \\end{aligned} \\] The two roots can be found using the quadratic formula \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2} \\] Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices. But there are some helpful properties of determinants that come in handy The eigenvalues of a diagonal or triangular matrix are simply the diagonal elements \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & m_{12} & m_{13} \\\\ m_{21} & m_{22} - \\lambda & m_{23} \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= \\begin{vmatrix} m_{11} - \\lambda & 0 & 0 \\\\ m_{21} & m_{22} - \\lambda & 0 \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] Similarly, the eigenvalues of a block diagonal or triangular matrix are the eigenvalues of the submatrices along the diagonal \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{bmatrix} \\begin{bmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\ \\begin{bmatrix} m_{31} & m_{32} \\end{bmatrix} & \\begin{bmatrix} m_{33} - \\lambda \\end{bmatrix} \\end{bmatrix}\\\\ |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] Finding eigenvectors of a matrix Now how do we find the eigenvectors? If \\(\\vec{v}\\) is an eigenvector of the matrix \\(\\mathbf{M}\\) corresponding to the eigenvalue \\(\\lambda\\) , it must satisfy: \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] We would like to just solve for \\(\\vec{v}\\) \\[ \\vec{v} = (\\mathbf{M} - \\lambda\\mathbf{I})^{-1} \\vec{0} \\] But we can't since \\(\\mathbf{M} - \\lambda\\mathbf{I}\\) is singular. Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another. For a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) you can find an eigenvector for \\(\\lambda_1\\) , for example, by solving \\[ \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\lambda_1 \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\] The system of equations determining the eigenvector associated with \\(\\lambda_1\\) is then \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &= \\lambda_1 v_2 \\end{aligned} \\] Note from the matrix form above that we can multiply \\(\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) by any constant and that will also be a solution. So there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. Let's choose \\(v_1 = 1\\) . Now we have just one unknown, \\(v_2\\) , so choose either of the equations and solve for \\(v_2\\) \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &= \\lambda_1 1 \\\\ v_2 &= (\\lambda_1 - m_{11}) / m_{12} \\end{aligned} \\] Multiplying both entries by \\(m_{12}\\) we therefore have \\(\\vec{v} = \\begin{bmatrix} m_{12} \\\\ \\lambda_1 - m_{11}\\end{bmatrix}\\) associated with \\(\\lambda_1\\) . Worked example To make this more concrete, we will work through an example of finding the eigenvalues and eigenvectors of a matrix Let's find the eigenvalue(s) for the matrix: \\[ \\mathbf{M}= \\begin{bmatrix} 2 & 1\\\\ 1 & 2\\\\ \\end{bmatrix} \\] Now solving \\(|\\mathbf{M}-\\lambda \\mathbf{I}| = 0\\) for \\(\\lambda\\) \\[ \\begin{equation} \\begin{split} |\\mathbf{M}-\\lambda \\mathbf{I}|&=0\\\\ \\begin{vmatrix} 2-\\lambda & 1\\\\ 1 & 2-\\lambda\\\\ \\end{vmatrix} &=0\\\\ (2-\\lambda)(2-\\lambda)-(1)(1) &= 0\\\\ \\lambda^2-4\\lambda+3 &= 0\\\\ (\\lambda-1)(\\lambda-3) &= 0\\\\ \\end{split} \\end{equation} \\] and so the eigenvalues are \\(\\lambda_1=1\\) and \\(\\lambda_2=3\\) . Let's next find the corresponding eigenvectors: \\(\\lambda_1=1\\) : An eigenvector must satisfy the equation: \\(\\begin{bmatrix}2 & 1\\\\1 & 2\\\\\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\end{bmatrix} =\\) \\(1\\) \\(\\begin{bmatrix}v_1\\\\v_2\\\\\\end{bmatrix}\\) This gives us two equations: \\[ 2v_1 + v_2 = v_1 \\] and \\[ v_1 + 2v_2= v_2 \\] The first tells us that \\[ v_2 = - v_1 \\] If we let \\(v_1=1\\) , \\(v_2\\) must then equal \\(-1\\) and, thus, \\(\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda=1\\) . Now try showing yourself that \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda=3\\) . Now let's take a look at this graphically. import numpy as np import matplotlib.pyplot as plt M = [[2, 1], [1,2]] # First initialize functiosn for plotting the eigenvectors l1, l2 = 1, 3 e1, e2 = [1, -1], [1, 1] fe1, fe2 = lambda x: np.array(x * e1[1]/e1[0]), lambda x: np.array(x * e2[1]/e2[0]) # Plot the eigenvectors x = np.linspace(0, 1, 25) fig, ax = plt.subplots() ax.plot(x, fe1(x), label=f\"$\\lambda$ = {l1}, $\\overrightarrow v$ = {e1}\") ax.plot(x, fe2(x), label=f\"$\\lambda$ = {l2}, $\\overrightarrow v$ = {e2}\") # Now plot some initial condition n0 = [1/2, -1/3] fn0 = lambda x: np.array(x * n0[1]/n0[0]) ax.plot(x[0:15], fn0(x[0:15]), label=f\"$\\overrightarrow n0$ = {n0}\", c='green') # And then multiply the initial condition by the matrix Mn = np.array(M) @ np.array(n0) fMn = lambda x: np.array(x * Mn[1]/Mn[0]) ax.plot(x[0:15], fMn(x[0:15]), label=f\"$\\overrightarrow Mn$ = {Mn}\", c='darkred') # Draw parallel lines shift = 0.5 ax.plot(x + shift, fe2(x) + fe1(shift), color='black') shift = 0.225 ax.plot(x + shift, fe1(x) + fe2(shift), color='black') ax.legend(frameon=False, loc=(1.1,0.35)) <matplotlib.legend.Legend at 0x7f96026e31f0> And if we keep multiplying by \\(\\mathbf{M}\\) we see that the direction of \\(\\mathbf{M}^{m}\\vec{n}\\) approaches that of the eigenvector associated with the eigenvalue of largest absolute value, and the length of \\(\\mathbf{M}^{m}\\vec{n}\\) changes by a factor equal to the magnitude of that eigenvalue.","title":"Lecture 11"},{"location":"lectures/lecture-11/#lecture-11-linear-algebra-ii","text":"Run notes interactively?","title":"Lecture 11: Linear algebra II"},{"location":"lectures/lecture-11/#lecture-overview","text":"Eigenvalues and eigenvectors","title":"Lecture overview"},{"location":"lectures/lecture-11/#1-eigenvalues-and-eigenvectors","text":"","title":"1. Eigenvalues and eigenvectors"},{"location":"lectures/lecture-11/#what-are-eigenvalues-and-eigenvectors","text":"A number \\(\\lambda\\) is an eigenvalue of matrix \\(\\textbf{M}\\) if there exists a non-zero vector, \\(\\vec{v}\\) , that satisfies the equation: \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] Every non-zero vector \\(\\vec{v}\\) satisfying this relation is an (right) eigenvector of \\(\\mathbf{M}\\) belonging to the eigenvalue, \\(\\lambda\\) .","title":"What are eigenvalues and eigenvectors?"},{"location":"lectures/lecture-11/#finding-eigenvalues-of-a-matrix","text":"To find the eigenvalues of a matrix, we can rearrange the above equation, using the distributive law for matrix multiplication: \\[ \\begin{aligned} \\mathbf{M}\\vec{v} &= \\lambda\\vec{v}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\vec{v} &= \\vec{0}\\\\ \\mathbf{M}\\vec{v} - \\lambda\\mathbf{I}\\vec{v} &= \\vec{0} \\\\ (\\mathbf{M} - \\lambda\\mathbf{I})\\vec{v} &= \\vec{0}\\\\ \\vec{v} &= (\\mathbf{M} - \\lambda\\mathbf{I})^{-1}\\vec{0} \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix , and \\(\\vec{0}\\) is a vector of zeros. This suggests that \\(\\vec{v}\\) is a vector of zeros, \\(\\vec{v}=\\vec{0}\\) . But we've said this is not the case. So this last step must not be valid \\(\\implies (\\mathbf{M} - \\lambda\\mathbf{I})\\) is not invertible. Therefore the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) must be zero! \\(\\implies\\) solve for \\(\\lambda\\) without \\(\\vec{v}\\) The determinant of the \\(n\\times n\\) matrix \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) is an \\(n^{th}\\) degree polynomial in \\(\\lambda\\) , the roots of which are the eigenvalues of the matrix \\(\\mathbf{M}\\) : \\(\\lambda_1,\\lambda_2,...\\lambda_n\\) . For example, in the \\(n=2\\) case \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\\\ &= \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} - \\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix}\\\\ &= \\begin{bmatrix} m_{11} - \\lambda & m_{12} \\\\ m_{21} & m_{22} - \\lambda \\end{bmatrix} \\end{aligned} \\] so that \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I} | &= 0\\\\ (m_{11}-\\lambda)(m_{22}-\\lambda)-m_{21} m_{12} &= 0\\\\ \\lambda^2 - (m_{11}+m_{22})\\lambda + (m_{11}m_{22}-m_{21}m_{12}) &= 0\\\\ \\lambda^2 - \\mathrm{Tr}(\\mathbf{M})\\lambda + \\mathrm{Det}(\\mathbf{M}) &= 0 \\end{aligned} \\] The two roots can be found using the quadratic formula \\[ \\lambda = \\frac{\\mathrm{Tr}(\\mathbf{M}) \\pm \\sqrt{\\mathrm{Tr}(\\mathbf{M})^2 - 4\\mathrm{Det}(\\mathbf{M})}}{2} \\] Finding the determinant of \\((\\mathbf{M} - \\lambda\\mathbf{I})\\) becomes trickier for larger matrices. But there are some helpful properties of determinants that come in handy The eigenvalues of a diagonal or triangular matrix are simply the diagonal elements \\[ \\begin{aligned} |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & m_{12} & m_{13} \\\\ m_{21} & m_{22} - \\lambda & m_{23} \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= \\begin{vmatrix} m_{11} - \\lambda & 0 & 0 \\\\ m_{21} & m_{22} - \\lambda & 0 \\\\ m_{31} & m_{32} & m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\] Similarly, the eigenvalues of a block diagonal or triangular matrix are the eigenvalues of the submatrices along the diagonal \\[ \\begin{aligned} \\mathbf{M} - \\lambda \\mathbf{I} &= \\begin{bmatrix} \\begin{bmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\ \\begin{bmatrix} m_{31} & m_{32} \\end{bmatrix} & \\begin{bmatrix} m_{33} - \\lambda \\end{bmatrix} \\end{bmatrix}\\\\ |\\mathbf{M} - \\lambda \\mathbf{I}| &= \\begin{vmatrix} m_{11} - \\lambda & 0 \\\\ m_{21} & m_{22} - \\lambda \\end{vmatrix} \\begin{vmatrix} m_{33} - \\lambda \\end{vmatrix}\\\\ &= (m_{11} - \\lambda) (m_{22} - \\lambda) (m_{33} - \\lambda) \\end{aligned} \\]","title":"Finding eigenvalues of a matrix"},{"location":"lectures/lecture-11/#finding-eigenvectors-of-a-matrix","text":"Now how do we find the eigenvectors? If \\(\\vec{v}\\) is an eigenvector of the matrix \\(\\mathbf{M}\\) corresponding to the eigenvalue \\(\\lambda\\) , it must satisfy: \\[ \\mathbf{M}\\vec{v} = \\lambda \\vec{v} \\] We would like to just solve for \\(\\vec{v}\\) \\[ \\vec{v} = (\\mathbf{M} - \\lambda\\mathbf{I})^{-1} \\vec{0} \\] But we can't since \\(\\mathbf{M} - \\lambda\\mathbf{I}\\) is singular. Instead we need to write out the system of equations represented by \\(\\mathbf{M}\\vec{v} = \\lambda \\vec{v}\\) and solve for one variable after another. For a \\(2 \\times 2\\) matrix \\(\\mathbf{M}\\) with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) you can find an eigenvector for \\(\\lambda_1\\) , for example, by solving \\[ \\begin{bmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\lambda_1 \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\] The system of equations determining the eigenvector associated with \\(\\lambda_1\\) is then \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{21} v_1 + m_{22} v_2 &= \\lambda_1 v_2 \\end{aligned} \\] Note from the matrix form above that we can multiply \\(\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) by any constant and that will also be a solution. So there are an infinite number of eigenvectors associated with an eigenvalue and we can set one of the elements to an arbitrary value. Let's choose \\(v_1 = 1\\) . Now we have just one unknown, \\(v_2\\) , so choose either of the equations and solve for \\(v_2\\) \\[ \\begin{aligned} m_{11} v_1 + m_{12} v_2 &= \\lambda_1 v_1 \\\\ m_{11} 1 + m_{12} v_2 &= \\lambda_1 1 \\\\ v_2 &= (\\lambda_1 - m_{11}) / m_{12} \\end{aligned} \\] Multiplying both entries by \\(m_{12}\\) we therefore have \\(\\vec{v} = \\begin{bmatrix} m_{12} \\\\ \\lambda_1 - m_{11}\\end{bmatrix}\\) associated with \\(\\lambda_1\\) .","title":"Finding eigenvectors of a matrix"},{"location":"lectures/lecture-11/#worked-example","text":"To make this more concrete, we will work through an example of finding the eigenvalues and eigenvectors of a matrix Let's find the eigenvalue(s) for the matrix: \\[ \\mathbf{M}= \\begin{bmatrix} 2 & 1\\\\ 1 & 2\\\\ \\end{bmatrix} \\] Now solving \\(|\\mathbf{M}-\\lambda \\mathbf{I}| = 0\\) for \\(\\lambda\\) \\[ \\begin{equation} \\begin{split} |\\mathbf{M}-\\lambda \\mathbf{I}|&=0\\\\ \\begin{vmatrix} 2-\\lambda & 1\\\\ 1 & 2-\\lambda\\\\ \\end{vmatrix} &=0\\\\ (2-\\lambda)(2-\\lambda)-(1)(1) &= 0\\\\ \\lambda^2-4\\lambda+3 &= 0\\\\ (\\lambda-1)(\\lambda-3) &= 0\\\\ \\end{split} \\end{equation} \\] and so the eigenvalues are \\(\\lambda_1=1\\) and \\(\\lambda_2=3\\) . Let's next find the corresponding eigenvectors: \\(\\lambda_1=1\\) : An eigenvector must satisfy the equation: \\(\\begin{bmatrix}2 & 1\\\\1 & 2\\\\\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\end{bmatrix} =\\) \\(1\\) \\(\\begin{bmatrix}v_1\\\\v_2\\\\\\end{bmatrix}\\) This gives us two equations: \\[ 2v_1 + v_2 = v_1 \\] and \\[ v_1 + 2v_2= v_2 \\] The first tells us that \\[ v_2 = - v_1 \\] If we let \\(v_1=1\\) , \\(v_2\\) must then equal \\(-1\\) and, thus, \\(\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda=1\\) . Now try showing yourself that \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda=3\\) . Now let's take a look at this graphically. import numpy as np import matplotlib.pyplot as plt M = [[2, 1], [1,2]] # First initialize functiosn for plotting the eigenvectors l1, l2 = 1, 3 e1, e2 = [1, -1], [1, 1] fe1, fe2 = lambda x: np.array(x * e1[1]/e1[0]), lambda x: np.array(x * e2[1]/e2[0]) # Plot the eigenvectors x = np.linspace(0, 1, 25) fig, ax = plt.subplots() ax.plot(x, fe1(x), label=f\"$\\lambda$ = {l1}, $\\overrightarrow v$ = {e1}\") ax.plot(x, fe2(x), label=f\"$\\lambda$ = {l2}, $\\overrightarrow v$ = {e2}\") # Now plot some initial condition n0 = [1/2, -1/3] fn0 = lambda x: np.array(x * n0[1]/n0[0]) ax.plot(x[0:15], fn0(x[0:15]), label=f\"$\\overrightarrow n0$ = {n0}\", c='green') # And then multiply the initial condition by the matrix Mn = np.array(M) @ np.array(n0) fMn = lambda x: np.array(x * Mn[1]/Mn[0]) ax.plot(x[0:15], fMn(x[0:15]), label=f\"$\\overrightarrow Mn$ = {Mn}\", c='darkred') # Draw parallel lines shift = 0.5 ax.plot(x + shift, fe2(x) + fe1(shift), color='black') shift = 0.225 ax.plot(x + shift, fe1(x) + fe2(shift), color='black') ax.legend(frameon=False, loc=(1.1,0.35)) <matplotlib.legend.Legend at 0x7f96026e31f0> And if we keep multiplying by \\(\\mathbf{M}\\) we see that the direction of \\(\\mathbf{M}^{m}\\vec{n}\\) approaches that of the eigenvector associated with the eigenvalue of largest absolute value, and the length of \\(\\mathbf{M}^{m}\\vec{n}\\) changes by a factor equal to the magnitude of that eigenvalue.","title":"Worked example"},{"location":"lectures/lecture-12/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 12: Equilibria and stability (linear multivariate) Run notes interactively? Lecture overview Linear multivariate models Equilibria Stability Discrete time Summary 1. Linear multivariate models Here, we'll begin to use linear algebra to analyze linear multivariate models. To keep the visuals simple, we'll look at the general \\(2\\times2\\) case, but the same techniques hold for any dimension. A \\(2\\times2\\) linear multivariate model in continuous time can be written as \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= a n_1 + b n_2\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= c n_1 + d n_2\\\\ \\end{aligned} \\] In matrix notation \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M} \\vec{n} \\end{aligned} \\] 2. Equilibria The equilibria of this model, \\(\\hat{\\vec{n}}\\) , are found as in the univariate case Linear models \\[ \\begin{aligned} \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} \\end{aligned} \\] If \\(\\mathbf{M}\\) is invertible \\[ \\begin{aligned} \\mathbf{M}^{-1} \\vec{0} &= \\hat{\\vec{n}}\\\\ \\vec{0} &= \\hat{\\vec{n}} \\end{aligned} \\] This is reminiscent to our univariate model of exponential growth \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn \\implies \\hat{n}=0\\) provided \\(r\\neq0\\) . If \\(\\mathbf{M}\\) is non-invertible (singular, ie \\(\\mathbf{M}\\) has a determinant of 0), then there is an infinite number of equilibria. To see this notice that if \\[ \\mathbf{M} = \\begin{pmatrix} a & b \\\\ ca & cb \\end{pmatrix} \\] then as long as \\(n_2 = (-a/b)n_1\\) we are at equilibrium, regardless the value of \\(n_1\\) . Note that this is the case where the two null clines lie exactly on top of one another. This is reminiscent of exponential growth \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn\\) with \\(r=0\\) . Affine models Now what if we have a multivariate affine model? \\[ \\begin{aligned} \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M} \\vec{n} + \\vec{c} \\end{aligned} \\] Well the equilibria of this model, \\(\\hat{\\vec{n}}\\) , are also found as in the univariate case \\[ \\begin{aligned} \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} + \\vec{c}\\\\ -\\vec{c} &= \\mathbf{M} \\hat{\\vec{n}} \\end{aligned} \\] If \\(\\mathbf{M}\\) is invertible (otherwise there are no equilibria or an infinite number) \\[ \\begin{aligned} -\\mathbf{M}^{-1} \\vec{c} &= \\hat{\\vec{n}} \\end{aligned} \\] This is reminiscent to our univariate model of exponential growth with migration \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn+m \\implies \\hat{n}=-m/r\\) provided \\(r\\neq0\\) . Fortunately we can convert affine models into linear models with a transformation \\(\\vec{n} \\rightarrow \\vec{\\delta} = \\vec{n} - \\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\vec{\\delta}}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{\\vec{n}}}{\\mathrm{d}t} \\\\ &= \\mathbf{M} \\vec{n} + \\vec{c}\\\\ &= \\mathbf{M} (\\vec{\\delta} + \\hat{\\vec{n}}) + \\vec{c}\\\\ &= \\mathbf{M} (\\vec{\\delta} - \\mathbf{M}^{-1} \\vec{c}) + \\vec{c}\\\\ &= \\mathbf{M} \\vec{\\delta} \\end{aligned} \\] 3. Stability Real-valued eigenvalues To consider stability recall the definition of the eigenvalues and eigenvectors \\[ \\mathbf{M}\\vec{v} = \\lambda\\vec{v} \\] Combining this with \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M} \\vec{n} \\] we see that \\[ \\frac{\\mathrm{d}\\vec{v}}{\\mathrm{d}t} = \\lambda\\vec{v} \\] i.e., once the system reaches an eigenvector, \\(\\vec{n}=\\vec{v}\\) , it will remain on that eigenvector forever and change in magnitude with rate \\(\\lambda\\) . The system can therefore only be stable when movement along each eigenvector approaches the equilibrium, i.e, all \\(\\lambda<0\\) . \\[ \\mathbf{M_1} = \\begin{pmatrix} 2 & 1 \\\\ 1/2 & 1 \\end{pmatrix}; 0 < \\lambda_1, \\lambda_2 \\ \\ \\ \\mathbf{M_2} = \\begin{pmatrix} -2 & -1 \\\\ 1/2 & 1 \\end{pmatrix}; \\lambda_1 < 0 < \\lambda_2 \\ \\ \\ \\mathbf{M_3} = \\begin{pmatrix} -2 & -1 \\\\ -1/2 & -1 \\end{pmatrix}; \\lambda_1, \\lambda_2 < 0 \\ \\ \\ \\] import sympy import numpy as np import matplotlib.pyplot as plt # Write out vector field plotting function (see Lecture 6) def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None], ax=None): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) # Plot figure if ax is None: fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) if show == True: plt.show() else: return ax # Initialize matrices M1, M2, M3 = ( sympy.Matrix([[2,1],[1/2,1]]), sympy.Matrix([[-2,-1],[1/2,1]]), sympy.Matrix([[-2,-1],[-1/2,-1]]) ) # Initialize vector of variables n1, n2 = sympy.symbols('n1, n2') n = sympy.Matrix([n1, n2]) # Plot data fig, ax = plt.subplots(1, 3) fig.set_size_inches(14, 4) for i, M in enumerate([M1, M2, M3]): # Compute derivatives dn = M * n # Plot vector field plot_vector_field(*dn, xlim=(-1,1),ylim=(-1,1), ax=ax[i]) # Get eigenvectors and make functions for plotting r1, r2 = np.array(M.eigenvects()[0][-1]).flatten(), np.array(M.eigenvects()[1][-1]).flatten() f1, f2 = lambda x: x * r1[1]/r1[0], lambda x: x * r2[1]/r2[0] ax[i].plot(np.linspace(-1,1,25), f1(np.linspace(-1, 1, 25))) ax[i].plot(np.linspace(-1,1,25), f2(np.linspace(-1, 1, 25))) # Add title ax[i].set_title(f\"$M_{i + 1}$\") ax[i].set_xlim(-1, 1) ax[i].set_ylim(-1, 1) plt.show() Complex eigenvalues But what happens if there are complex eigenvalues? (Remember that in the \\(2\\times2\\) case this occurs whenever \\(\\mathrm{Tr}(\\mathbf{M})^2 < 4 \\mathrm{Det}(\\mathbf{M})\\) .) Well we just saw that movement along an eigenvector is exponential, \\(\\frac{\\mathrm{d}\\vec{v}}{\\mathrm{d}t} = \\lambda\\vec{v}\\) , shrinking towards the equilibrium or growing without bound like \\(e^{\\lambda t}\\) . We can write a complex eigenvalue (or any complex number) like \\(A + B i\\) , where \\(i=\\sqrt{-1}\\) . We call \\(A\\) the real part and \\(B\\) the imaginary part. So if the same dynamics hold, then we should expect to grow or shrink along an eigenvector with a complex eigenvalue like \\(e^{(A + B i)t}\\) . We can use Euler's equation to write this like \\[ \\begin{aligned} e^{(A + B i)t} &= e^{At} e^{B i t}\\\\ &= e^{At} (\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] Since \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded between -1 and 1, this suggests the dynamics will decay to the equilibrium if and only if \\(A<0\\) . And because \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) fluctuate up and down, this suggests the dynamics will oscillate, regardless of whether we move away from or towards the equilibrium. Examples with complex eigenvalues Here are two examples with complex eigenvalues (note we can no longer draw the eigenvectors since they are also complex). In the \\(2\\times2\\) case the two eigenvalues can be written \\(A\\pm Bi\\) , meaning they have the same real part, \\(A=\\mathrm{Tr}(\\mathbf{M})/2\\) . On the left the real part is less than 0 and the equilibrium is stable. On the right the real part is greater than 0 and the equilibrium is unstable. \\[ \\mathbf{M_1} = \\begin{pmatrix} -2 & -1 \\\\ 1 & -2 \\end{pmatrix}; A < 0 \\ \\ \\ \\mathbf{M_2} = \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix}; A > 0 \\] Although we can't plot these eigenvectors, we can still plot the vector field. # Initialize matrices M1, M2 = ( sympy.Matrix([[-2,-1],[1,-2]]), sympy.Matrix([[2,-1],[1,2]]) ) # Initialize vector of variables n1, n2 = sympy.symbols('n1, n2') n = sympy.Matrix([n1, n2]) # Plot data fig, ax = plt.subplots(1, 2) fig.set_size_inches(14, 4) for i, M in enumerate([M1, M2]): # Compute derivatives dn = M * n # Plot vector field plot_vector_field(*dn, xlim=(-1,1),ylim=(-1,1), ax=ax[i]) plt.show() Notice anything different about these vector fields compared to the matrices with real-valued eigenvectors? Interpreting stability with real-valued or complex eigenvalues We can summarize the results for both real and complex eigenvalues in continuous time for any dimension with: the equilibrium is stable if and only if the real parts of all eigenvalues are negative if we call the eigenvalue with the largest real part the \\textbf{leading eigenvalue}, stability requires the leading eigenvalue to have a negative real part if there are eigenvalues with non-zero imaginary parts there will be cycling In 2 dimensions we can draw another useful plot that characterizes both eigenvalues in terms of the trace and determinant of M 4. Discrete time So far we've been looking at continuous time models. The analysis of discrete time models is similar. Equilibrium - Linear models Our system of equations for a linear model is now written \\[ \\begin{aligned} n_1(t+1) &= a n_1(t) + b n_2(t) \\\\ n_2(t+1) &= c n_1(t) + d n_2(t) \\end{aligned} \\] In matrix form this is \\[ \\begin{aligned} \\vec{n}(t+1) &= \\mathbf{M} \\vec{n}(t) \\end{aligned} \\] and we call \\(\\mathbf{M} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) a transition matrix . So if we want to solve for the equilibria, \\(\\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\hat{\\vec{n}} &= \\mathbf{M} \\hat{\\vec{n}}\\\\ \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} - \\hat{\\vec{n}}\\\\ &= \\mathbf{M} \\hat{\\vec{n}} - \\mathbf{I}\\hat{\\vec{n}}\\\\ &= (\\mathbf{M} - \\mathbf{I}) \\hat{\\vec{n}} \\end{aligned} \\] We therefore conclude that either - \\(|\\mathbf{M} - \\mathbf{I}|\\neq0\\) and \\(\\hat{\\vec{n}}=\\vec{0}\\) - \\(|\\mathbf{M} - \\mathbf{I}|=0\\) and there are infinite equilibria Equilibrium - Affine models If we have an affine model we can solve for the equilibria \\[ \\begin{aligned} \\hat{\\vec{n}} &= \\mathbf{M} \\hat{\\vec{n}} + \\vec{c}\\\\ -\\vec{c} &= \\mathbf{M} \\hat{\\vec{n}} - \\hat{\\vec{n}}\\\\ -\\vec{c} &= (\\mathbf{M} - \\mathbf{I}) \\hat{\\vec{n}}\\\\ -(\\mathbf{M} - \\mathbf{I})^{-1} \\vec{c} &= \\hat{\\vec{n}} \\end{aligned} \\] (provided \\(\\mathbf{M} - \\mathbf{I}\\) is invertible) We can then write the system as a linear model in terms of \\(\\vec{\\delta}(t) = \\vec{n}(t) - \\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\vec{\\delta}(t+1) &= \\vec{n}(t+1) - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M} \\vec{n}(t) + \\vec{c} - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M} (\\vec{\\delta}(t) + \\hat{\\vec{n}}) + \\vec{c} - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) + (\\mathbf{M} - \\mathbf{I})\\hat{\\vec{n}} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) - (\\mathbf{M} - \\mathbf{I})(\\mathbf{M} - \\mathbf{I})^{-1} \\vec{c} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) - \\vec{c} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) \\end{aligned} \\] So, moving on to stability, we can just consider linear models . Stability Determining stability in discrete time is analogous to determining stability in continuous time -- we need to consider how the system moves along it's eigenvectors. Since the eigenvalues and eigenvectors are defined by \\(\\mathbf{M} \\vec{v} = \\lambda \\vec{v}\\) and the dynamics follow \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t)\\) , once the system is on an eigenvector \\(\\vec{n} = \\vec{v}\\) the dynamics are \\[ \\vec{v}(t+1) = \\mathbf{M} \\vec{v}(t) = \\lambda \\vec{v} \\] This is just exponential growth in discrete time and so we need \\(|\\lambda|<1\\) for stability. This suggests that a discrete time model is stable when the eigenvalue with the largest absolute value, which we call the leading eigenvalue in discrete time, has an absolute value less than one. Note that for complex eigenvalues, \\(|A + B i| = \\sqrt{A^2 + B^2}\\) , and so in discrete time the complex part also influences stability. We can also summarize stability in discrete time in terms of the real and imaginary parts of the eigenvalues 5. Summary We can determine the stability of linear multivariate models with their eigenvalues continuous time - the leading eigenvalue of \\(\\textbf{M}\\) is the one with the largest real part - if the leading eigenvalue has a negative real part the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling discrete time - the leading eigenvalue of \\(\\textbf{M}\\) is the one with the largest absolute value - if the leading eigenvalue has an absolute value less than one the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling","title":"Lecture 12"},{"location":"lectures/lecture-12/#lecture-12-equilibria-and-stability-linear-multivariate","text":"Run notes interactively?","title":"Lecture 12: Equilibria and stability (linear multivariate)"},{"location":"lectures/lecture-12/#lecture-overview","text":"Linear multivariate models Equilibria Stability Discrete time Summary","title":"Lecture overview"},{"location":"lectures/lecture-12/#1-linear-multivariate-models","text":"Here, we'll begin to use linear algebra to analyze linear multivariate models. To keep the visuals simple, we'll look at the general \\(2\\times2\\) case, but the same techniques hold for any dimension. A \\(2\\times2\\) linear multivariate model in continuous time can be written as \\[ \\begin{aligned} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} &= a n_1 + b n_2\\\\ \\frac{\\mathrm{d}n_2}{\\mathrm{d}t} &= c n_1 + d n_2\\\\ \\end{aligned} \\] In matrix notation \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}n_1}{\\mathrm{d}t} \\end{pmatrix} &= \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\begin{pmatrix} n_1 \\\\ n_2 \\end{pmatrix}\\\\ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M} \\vec{n} \\end{aligned} \\]","title":"1. Linear multivariate models"},{"location":"lectures/lecture-12/#2-equilibria","text":"The equilibria of this model, \\(\\hat{\\vec{n}}\\) , are found as in the univariate case","title":"2. Equilibria"},{"location":"lectures/lecture-12/#linear-models","text":"\\[ \\begin{aligned} \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} \\end{aligned} \\] If \\(\\mathbf{M}\\) is invertible \\[ \\begin{aligned} \\mathbf{M}^{-1} \\vec{0} &= \\hat{\\vec{n}}\\\\ \\vec{0} &= \\hat{\\vec{n}} \\end{aligned} \\] This is reminiscent to our univariate model of exponential growth \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn \\implies \\hat{n}=0\\) provided \\(r\\neq0\\) . If \\(\\mathbf{M}\\) is non-invertible (singular, ie \\(\\mathbf{M}\\) has a determinant of 0), then there is an infinite number of equilibria. To see this notice that if \\[ \\mathbf{M} = \\begin{pmatrix} a & b \\\\ ca & cb \\end{pmatrix} \\] then as long as \\(n_2 = (-a/b)n_1\\) we are at equilibrium, regardless the value of \\(n_1\\) . Note that this is the case where the two null clines lie exactly on top of one another. This is reminiscent of exponential growth \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn\\) with \\(r=0\\) .","title":"Linear models"},{"location":"lectures/lecture-12/#affine-models","text":"Now what if we have a multivariate affine model? \\[ \\begin{aligned} \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} &= \\mathbf{M} \\vec{n} + \\vec{c} \\end{aligned} \\] Well the equilibria of this model, \\(\\hat{\\vec{n}}\\) , are also found as in the univariate case \\[ \\begin{aligned} \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} + \\vec{c}\\\\ -\\vec{c} &= \\mathbf{M} \\hat{\\vec{n}} \\end{aligned} \\] If \\(\\mathbf{M}\\) is invertible (otherwise there are no equilibria or an infinite number) \\[ \\begin{aligned} -\\mathbf{M}^{-1} \\vec{c} &= \\hat{\\vec{n}} \\end{aligned} \\] This is reminiscent to our univariate model of exponential growth with migration \\(\\frac{\\mathrm{d}n}{\\mathrm{d}t}=rn+m \\implies \\hat{n}=-m/r\\) provided \\(r\\neq0\\) . Fortunately we can convert affine models into linear models with a transformation \\(\\vec{n} \\rightarrow \\vec{\\delta} = \\vec{n} - \\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\vec{\\delta}}{\\mathrm{d}t} &= \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{\\vec{n}}}{\\mathrm{d}t} \\\\ &= \\mathbf{M} \\vec{n} + \\vec{c}\\\\ &= \\mathbf{M} (\\vec{\\delta} + \\hat{\\vec{n}}) + \\vec{c}\\\\ &= \\mathbf{M} (\\vec{\\delta} - \\mathbf{M}^{-1} \\vec{c}) + \\vec{c}\\\\ &= \\mathbf{M} \\vec{\\delta} \\end{aligned} \\]","title":"Affine models"},{"location":"lectures/lecture-12/#3-stability","text":"","title":"3. Stability"},{"location":"lectures/lecture-12/#real-valued-eigenvalues","text":"To consider stability recall the definition of the eigenvalues and eigenvectors \\[ \\mathbf{M}\\vec{v} = \\lambda\\vec{v} \\] Combining this with \\[ \\frac{\\mathrm{d}\\vec{n}}{\\mathrm{d}t} = \\mathbf{M} \\vec{n} \\] we see that \\[ \\frac{\\mathrm{d}\\vec{v}}{\\mathrm{d}t} = \\lambda\\vec{v} \\] i.e., once the system reaches an eigenvector, \\(\\vec{n}=\\vec{v}\\) , it will remain on that eigenvector forever and change in magnitude with rate \\(\\lambda\\) . The system can therefore only be stable when movement along each eigenvector approaches the equilibrium, i.e, all \\(\\lambda<0\\) . \\[ \\mathbf{M_1} = \\begin{pmatrix} 2 & 1 \\\\ 1/2 & 1 \\end{pmatrix}; 0 < \\lambda_1, \\lambda_2 \\ \\ \\ \\mathbf{M_2} = \\begin{pmatrix} -2 & -1 \\\\ 1/2 & 1 \\end{pmatrix}; \\lambda_1 < 0 < \\lambda_2 \\ \\ \\ \\mathbf{M_3} = \\begin{pmatrix} -2 & -1 \\\\ -1/2 & -1 \\end{pmatrix}; \\lambda_1, \\lambda_2 < 0 \\ \\ \\ \\] import sympy import numpy as np import matplotlib.pyplot as plt # Write out vector field plotting function (see Lecture 6) def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None], ax=None): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) # Plot figure if ax is None: fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) if show == True: plt.show() else: return ax # Initialize matrices M1, M2, M3 = ( sympy.Matrix([[2,1],[1/2,1]]), sympy.Matrix([[-2,-1],[1/2,1]]), sympy.Matrix([[-2,-1],[-1/2,-1]]) ) # Initialize vector of variables n1, n2 = sympy.symbols('n1, n2') n = sympy.Matrix([n1, n2]) # Plot data fig, ax = plt.subplots(1, 3) fig.set_size_inches(14, 4) for i, M in enumerate([M1, M2, M3]): # Compute derivatives dn = M * n # Plot vector field plot_vector_field(*dn, xlim=(-1,1),ylim=(-1,1), ax=ax[i]) # Get eigenvectors and make functions for plotting r1, r2 = np.array(M.eigenvects()[0][-1]).flatten(), np.array(M.eigenvects()[1][-1]).flatten() f1, f2 = lambda x: x * r1[1]/r1[0], lambda x: x * r2[1]/r2[0] ax[i].plot(np.linspace(-1,1,25), f1(np.linspace(-1, 1, 25))) ax[i].plot(np.linspace(-1,1,25), f2(np.linspace(-1, 1, 25))) # Add title ax[i].set_title(f\"$M_{i + 1}$\") ax[i].set_xlim(-1, 1) ax[i].set_ylim(-1, 1) plt.show()","title":"Real-valued eigenvalues"},{"location":"lectures/lecture-12/#complex-eigenvalues","text":"But what happens if there are complex eigenvalues? (Remember that in the \\(2\\times2\\) case this occurs whenever \\(\\mathrm{Tr}(\\mathbf{M})^2 < 4 \\mathrm{Det}(\\mathbf{M})\\) .) Well we just saw that movement along an eigenvector is exponential, \\(\\frac{\\mathrm{d}\\vec{v}}{\\mathrm{d}t} = \\lambda\\vec{v}\\) , shrinking towards the equilibrium or growing without bound like \\(e^{\\lambda t}\\) . We can write a complex eigenvalue (or any complex number) like \\(A + B i\\) , where \\(i=\\sqrt{-1}\\) . We call \\(A\\) the real part and \\(B\\) the imaginary part. So if the same dynamics hold, then we should expect to grow or shrink along an eigenvector with a complex eigenvalue like \\(e^{(A + B i)t}\\) . We can use Euler's equation to write this like \\[ \\begin{aligned} e^{(A + B i)t} &= e^{At} e^{B i t}\\\\ &= e^{At} (\\cos(Bt) + i\\sin(Bt)) \\end{aligned} \\] Since \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) are bounded between -1 and 1, this suggests the dynamics will decay to the equilibrium if and only if \\(A<0\\) . And because \\(\\cos(Bt)\\) and \\(\\sin(Bt)\\) fluctuate up and down, this suggests the dynamics will oscillate, regardless of whether we move away from or towards the equilibrium.","title":"Complex eigenvalues"},{"location":"lectures/lecture-12/#examples-with-complex-eigenvalues","text":"Here are two examples with complex eigenvalues (note we can no longer draw the eigenvectors since they are also complex). In the \\(2\\times2\\) case the two eigenvalues can be written \\(A\\pm Bi\\) , meaning they have the same real part, \\(A=\\mathrm{Tr}(\\mathbf{M})/2\\) . On the left the real part is less than 0 and the equilibrium is stable. On the right the real part is greater than 0 and the equilibrium is unstable. \\[ \\mathbf{M_1} = \\begin{pmatrix} -2 & -1 \\\\ 1 & -2 \\end{pmatrix}; A < 0 \\ \\ \\ \\mathbf{M_2} = \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix}; A > 0 \\] Although we can't plot these eigenvectors, we can still plot the vector field. # Initialize matrices M1, M2 = ( sympy.Matrix([[-2,-1],[1,-2]]), sympy.Matrix([[2,-1],[1,2]]) ) # Initialize vector of variables n1, n2 = sympy.symbols('n1, n2') n = sympy.Matrix([n1, n2]) # Plot data fig, ax = plt.subplots(1, 2) fig.set_size_inches(14, 4) for i, M in enumerate([M1, M2]): # Compute derivatives dn = M * n # Plot vector field plot_vector_field(*dn, xlim=(-1,1),ylim=(-1,1), ax=ax[i]) plt.show() Notice anything different about these vector fields compared to the matrices with real-valued eigenvectors?","title":"Examples with complex eigenvalues"},{"location":"lectures/lecture-12/#interpreting-stability-with-real-valued-or-complex-eigenvalues","text":"We can summarize the results for both real and complex eigenvalues in continuous time for any dimension with: the equilibrium is stable if and only if the real parts of all eigenvalues are negative if we call the eigenvalue with the largest real part the \\textbf{leading eigenvalue}, stability requires the leading eigenvalue to have a negative real part if there are eigenvalues with non-zero imaginary parts there will be cycling In 2 dimensions we can draw another useful plot that characterizes both eigenvalues in terms of the trace and determinant of M","title":"Interpreting stability with real-valued or complex eigenvalues"},{"location":"lectures/lecture-12/#4-discrete-time","text":"So far we've been looking at continuous time models. The analysis of discrete time models is similar.","title":"4. Discrete time"},{"location":"lectures/lecture-12/#equilibrium-linear-models","text":"Our system of equations for a linear model is now written \\[ \\begin{aligned} n_1(t+1) &= a n_1(t) + b n_2(t) \\\\ n_2(t+1) &= c n_1(t) + d n_2(t) \\end{aligned} \\] In matrix form this is \\[ \\begin{aligned} \\vec{n}(t+1) &= \\mathbf{M} \\vec{n}(t) \\end{aligned} \\] and we call \\(\\mathbf{M} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) a transition matrix . So if we want to solve for the equilibria, \\(\\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\hat{\\vec{n}} &= \\mathbf{M} \\hat{\\vec{n}}\\\\ \\vec{0} &= \\mathbf{M} \\hat{\\vec{n}} - \\hat{\\vec{n}}\\\\ &= \\mathbf{M} \\hat{\\vec{n}} - \\mathbf{I}\\hat{\\vec{n}}\\\\ &= (\\mathbf{M} - \\mathbf{I}) \\hat{\\vec{n}} \\end{aligned} \\] We therefore conclude that either - \\(|\\mathbf{M} - \\mathbf{I}|\\neq0\\) and \\(\\hat{\\vec{n}}=\\vec{0}\\) - \\(|\\mathbf{M} - \\mathbf{I}|=0\\) and there are infinite equilibria","title":"Equilibrium - Linear models"},{"location":"lectures/lecture-12/#equilibrium-affine-models","text":"If we have an affine model we can solve for the equilibria \\[ \\begin{aligned} \\hat{\\vec{n}} &= \\mathbf{M} \\hat{\\vec{n}} + \\vec{c}\\\\ -\\vec{c} &= \\mathbf{M} \\hat{\\vec{n}} - \\hat{\\vec{n}}\\\\ -\\vec{c} &= (\\mathbf{M} - \\mathbf{I}) \\hat{\\vec{n}}\\\\ -(\\mathbf{M} - \\mathbf{I})^{-1} \\vec{c} &= \\hat{\\vec{n}} \\end{aligned} \\] (provided \\(\\mathbf{M} - \\mathbf{I}\\) is invertible) We can then write the system as a linear model in terms of \\(\\vec{\\delta}(t) = \\vec{n}(t) - \\hat{\\vec{n}}\\) \\[ \\begin{aligned} \\vec{\\delta}(t+1) &= \\vec{n}(t+1) - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M} \\vec{n}(t) + \\vec{c} - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M} (\\vec{\\delta}(t) + \\hat{\\vec{n}}) + \\vec{c} - \\hat{\\vec{n}}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) + (\\mathbf{M} - \\mathbf{I})\\hat{\\vec{n}} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) - (\\mathbf{M} - \\mathbf{I})(\\mathbf{M} - \\mathbf{I})^{-1} \\vec{c} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) - \\vec{c} + \\vec{c}\\\\ \\vec{\\delta}(t+1) &= \\mathbf{M}\\vec{\\delta}(t) \\end{aligned} \\] So, moving on to stability, we can just consider linear models .","title":"Equilibrium - Affine models"},{"location":"lectures/lecture-12/#stability","text":"Determining stability in discrete time is analogous to determining stability in continuous time -- we need to consider how the system moves along it's eigenvectors. Since the eigenvalues and eigenvectors are defined by \\(\\mathbf{M} \\vec{v} = \\lambda \\vec{v}\\) and the dynamics follow \\(\\vec{n}(t+1) = \\mathbf{M} \\vec{n}(t)\\) , once the system is on an eigenvector \\(\\vec{n} = \\vec{v}\\) the dynamics are \\[ \\vec{v}(t+1) = \\mathbf{M} \\vec{v}(t) = \\lambda \\vec{v} \\] This is just exponential growth in discrete time and so we need \\(|\\lambda|<1\\) for stability. This suggests that a discrete time model is stable when the eigenvalue with the largest absolute value, which we call the leading eigenvalue in discrete time, has an absolute value less than one. Note that for complex eigenvalues, \\(|A + B i| = \\sqrt{A^2 + B^2}\\) , and so in discrete time the complex part also influences stability. We can also summarize stability in discrete time in terms of the real and imaginary parts of the eigenvalues","title":"Stability"},{"location":"lectures/lecture-12/#5-summary","text":"We can determine the stability of linear multivariate models with their eigenvalues continuous time - the leading eigenvalue of \\(\\textbf{M}\\) is the one with the largest real part - if the leading eigenvalue has a negative real part the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling discrete time - the leading eigenvalue of \\(\\textbf{M}\\) is the one with the largest absolute value - if the leading eigenvalue has an absolute value less than one the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling","title":"5. Summary"},{"location":"lectures/lecture-13/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 13: Equilibria and stability (nonlinear multivariate) Run notes interactively? Lecture overview Nonlinear multivariate models Discrete time Summary 1. Nonlinear multivariate models Now we turn our attention to the most common type of model: one with multiple interacting variables. For example, a model of susceptible, \\(S\\) , and infected, \\(I\\) , individuals often includes the interaction between these two variables, \\(SI\\) , describing the rate at which these two classes of individuals meet one another (and potentially spread disease). In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\) , we can write any continuous time model like \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\) , we set all these equations to 0 and solve for one variable at a time. Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters. For example, in a simple model of disease we might have \\[ \\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &= \\sigma I - a c S I\\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &= a c S I - \\sigma I \\end{aligned} \\] describing recovery \\(\\sigma I\\) and infection \\(a c S I\\) . If we try to write the right hand side in matrix form we could do \\[ \\begin{pmatrix} -a c I & \\sigma \\\\ acI & -\\sigma \\end{pmatrix}\\begin{pmatrix} S \\\\ I\\end{pmatrix} \\] But now our matrix contains variables -- this is not very useful for analysis because our matrix (and its eigenvalues and eigenvectors) will now change in time with \\(S\\) and \\(I\\) . In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system. \"Linearizing\" a multivariate model As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium \\(\\delta = n - \\hat{n}\\) . Then assuming that the deviation from equilibrium, \\(\\delta\\) , is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system. To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &= f(a_1, a_2, ..., a_n)\\\\ &+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &+ \\cdots \\end{aligned} \\] Then when the difference between each variable and its value, \\(x_i-a_i\\) , is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\) , and we are left with a linear approximation of \\(f\\) . So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\) . Then we can write a system of equations describing the change in the deviations for all of our variables \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\) . So we now have a linear system \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] We can now write our approximate system around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] This is a special matrix called the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] And now that we have a linear system around an equilibrium, we can assess its local stability as we did with linear multivariate models We evaluate the Jacobian at an equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\) We calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) We set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\) We look to see if the eigenvalue with the largest real part (the leading eigenvalue ) has a real part that is greater (unstable) or less (stable) than 0 (complex eigenvalues again indicate cycling) 2. Discrete time We can do something very similar for nonlinear multivariate models in discrete time \\[ \\begin{aligned} x_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &\\vdots\\\\ x_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time. To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\) , giving \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\ \\end{aligned} \\] And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have \\[ \\begin{aligned} \\epsilon_1(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\epsilon_2(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\end{aligned} \\] We can therefore write our approximation around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] As in continuous time, the dynamics are described by the Jacobian matrix \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalue with the largest absolute value, the leading eigenvalue . If the leading eigenvalue has an absolute value less than 1 the equilibrium is stable. If there are complex eigenvalues there will be some cycling. 3. Summary We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium\\pause continuous time - the leading eigenvalue is the one with the largest real part - if the leading eigenvalue has a negative real part the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling discrete time - the leading eigenvalue is the one with the largest absolute value - if the leading eigenvalue has an absolute value less than one the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling","title":"Lecture 13"},{"location":"lectures/lecture-13/#lecture-13-equilibria-and-stability-nonlinear-multivariate","text":"Run notes interactively?","title":"Lecture 13: Equilibria and stability (nonlinear multivariate)"},{"location":"lectures/lecture-13/#lecture-overview","text":"Nonlinear multivariate models Discrete time Summary","title":"Lecture overview"},{"location":"lectures/lecture-13/#1-nonlinear-multivariate-models","text":"Now we turn our attention to the most common type of model: one with multiple interacting variables. For example, a model of susceptible, \\(S\\) , and infected, \\(I\\) , individuals often includes the interaction between these two variables, \\(SI\\) , describing the rate at which these two classes of individuals meet one another (and potentially spread disease). In general, if we have \\(n\\) interacting variables, \\(x_1, x_2, ..., x_n\\) , we can write any continuous time model like \\[ \\begin{aligned} \\frac{\\mathrm{d}x_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}x_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}x_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] If we then want to find the equilibria, \\(\\hat{x}_1,\\hat{x}_2, ..., \\hat{x}_n\\) , we set all these equations to 0 and solve for one variable at a time. Now note that we can no longer write this system of equations in matrix form with a matrix composed only of parameters. For example, in a simple model of disease we might have \\[ \\begin{aligned} \\frac{\\mathrm{d}S}{\\mathrm{d}t} &= \\sigma I - a c S I\\\\ \\frac{\\mathrm{d}I}{\\mathrm{d}t} &= a c S I - \\sigma I \\end{aligned} \\] describing recovery \\(\\sigma I\\) and infection \\(a c S I\\) . If we try to write the right hand side in matrix form we could do \\[ \\begin{pmatrix} -a c I & \\sigma \\\\ acI & -\\sigma \\end{pmatrix}\\begin{pmatrix} S \\\\ I\\end{pmatrix} \\] But now our matrix contains variables -- this is not very useful for analysis because our matrix (and its eigenvalues and eigenvectors) will now change in time with \\(S\\) and \\(I\\) . In order to use what we've learned about eigenvalues and eigenvectors we're first going to have to linearize the system.","title":"1. Nonlinear multivariate models"},{"location":"lectures/lecture-13/#linearizing-a-multivariate-model","text":"As we saw in nonlinear univariate models, one useful way to linearize a system is to measure the system relative to equilibrium \\(\\delta = n - \\hat{n}\\) . Then assuming that the deviation from equilibrium, \\(\\delta\\) , is small, we used a Taylor series expansion to approximate the nonlinear system with a linear system. To do that with multivariate models we'll need to know how to take a Taylor series expansion of multivariate functions \\[ \\begin{aligned} f(x_1, x_2, ..., x_n) &= f(a_1, a_2, ..., a_n)\\\\ &+ \\sum_{i=1}^{n} \\left( \\frac{\\partial f}{\\partial x_i} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)\\\\ &+ \\sum_{i=1}^{n}\\sum_{j=1}^n \\left( \\frac{\\partial f}{\\partial x_i \\partial x_j} \\bigg|_{x_1=a_1, x_2=a_2, ..., x_n=a_n} \\right) (x_i - a_i)(x_j - a_j)\\\\ &+ \\cdots \\end{aligned} \\] Then when the difference between each variable and its value, \\(x_i-a_i\\) , is small enough we can ignore all the terms with a \\((x_i-a_i)(x_j-a_j)\\) , and we are left with a linear approximation of \\(f\\) . So let \\(\\epsilon_i = x_i - \\hat{x}_i\\) be the deviation of variable \\(x_i\\) from its equilibrium value, \\(\\hat{x}_i\\) . Then we can write a system of equations describing the change in the deviations for all of our variables \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &= f_1(x_1, x_2, ..., x_n)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &= f_2(x_1, x_2, ..., x_n)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &= f_n(x_1, x_2, ..., x_n) \\end{aligned} \\] And then we can take a Taylor series around \\(x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n\\) to get a linear approximation of our system near the equilibrium \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] And then note that all \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n)=0\\) by definition of a equilibrium, leaving \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i)\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i - \\hat{x}_i) \\end{aligned} \\] Each of the partial derivatives \\(\\frac{\\partial f_i}{\\partial x_j}\\) is evaluated at the equilibrium, so these are constants. And \\(x_i - \\hat{x}_i = \\epsilon_i\\) . So we now have a linear system \\[ \\begin{aligned} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i\\\\ &\\vdots\\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} &\\approx \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i \\end{aligned} \\] We can now write our approximate system around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\frac{\\mathrm{d}\\epsilon_1}{\\mathrm{d}t} \\\\ \\frac{\\mathrm{d}\\epsilon_2}{\\mathrm{d}t} \\\\ \\vdots \\\\ \\frac{\\mathrm{d}\\epsilon_n}{\\mathrm{d}t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix} \\end{aligned} \\] This is a special matrix called the Jacobian \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] And now that we have a linear system around an equilibrium, we can assess its local stability as we did with linear multivariate models We evaluate the Jacobian at an equilibrium of interest, \\(\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n}\\) We calculate the characteristic polynomial \\(|\\mathbf{J}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} - \\lambda\\mathbf{I}|\\) We set the characteristic polynomial equal to 0 and solve for the \\(n\\) eigenvalues, \\(\\lambda\\) We look to see if the eigenvalue with the largest real part (the leading eigenvalue ) has a real part that is greater (unstable) or less (stable) than 0 (complex eigenvalues again indicate cycling)","title":"\"Linearizing\" a multivariate model"},{"location":"lectures/lecture-13/#2-discrete-time","text":"We can do something very similar for nonlinear multivariate models in discrete time \\[ \\begin{aligned} x_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t))\\\\ x_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t))\\\\ &\\vdots\\\\ x_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t))\\\\ \\end{aligned} \\] Now the equilibria are found by setting all \\(x_i(t+1) = x_i(t) = \\hat{x}_i\\) and solving for the \\(\\hat{x}_i\\) one at a time. To linearize the system around an equilibrium we again measure the system in terms of deviation from the equilibrium, \\(\\epsilon_i(t) = x_i(t) - \\hat{x}_i\\) , giving \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(x_1(t), x_2(t), ..., x_n(t)) - \\hat{x}_1\\\\ \\end{aligned} \\] Then taking the Taylor series of each \\(f_i\\) around \\(x_1(t) = \\hat{x}_1, ..., x_n(t) = \\hat{x}_n\\) we can approximate our system near the equilibrium as \\[ \\begin{aligned} \\epsilon_1(t+1) &= f_1(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_1\\\\ \\epsilon_2(t+1) &= f_2(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_2\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= f_n(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) + \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) (x_i(t) - \\hat{x}_i) - \\hat{x}_n\\\\ \\end{aligned} \\] And noting that \\(f_i(\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_n) = \\hat{x}_i\\) and \\(x_i(t) - \\hat{x}_i = \\epsilon_i(t)\\) we have \\[ \\begin{aligned} \\epsilon_1(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_1}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\epsilon_2(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_2}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ &\\vdots\\\\ \\epsilon_n(t+1) &= \\sum_{i=1}^{n} \\left( \\frac{\\partial f_n}{\\partial x_i} \\bigg|_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\right) \\epsilon_i(t)\\\\ \\end{aligned} \\] We can therefore write our approximation around the equilibrium in matrix form \\[ \\begin{aligned} \\begin{pmatrix} \\epsilon_1(t+1) \\\\ \\epsilon_2(t+1) \\\\ \\vdots \\\\ \\epsilon_n(t+1) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix}_{x_1=\\hat{x}_1, x_2=\\hat{x}_2, ..., x_n=\\hat{x}_n} \\begin{pmatrix} \\epsilon_1(t) \\\\ \\epsilon_2(t) \\\\ \\vdots \\\\ \\epsilon_n(t) \\end{pmatrix} \\end{aligned} \\] As in continuous time, the dynamics are described by the Jacobian matrix \\[ \\mathbf{J} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_n}{\\partial x_2} & \\cdots & \\frac{\\partial f_n}{\\partial x_n} \\end{pmatrix} \\] We therefore assess the local stability of an equilibrium by evaluating the Jacobian at that equilibrium and finding the eigenvalue with the largest absolute value, the leading eigenvalue . If the leading eigenvalue has an absolute value less than 1 the equilibrium is stable. If there are complex eigenvalues there will be some cycling.","title":"2. Discrete time"},{"location":"lectures/lecture-13/#3-summary","text":"We can determine the stability of nonlinear multivariate models with the eigenvalues of the Jacobian evaluated at an equilibrium\\pause continuous time - the leading eigenvalue is the one with the largest real part - if the leading eigenvalue has a negative real part the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling discrete time - the leading eigenvalue is the one with the largest absolute value - if the leading eigenvalue has an absolute value less than one the equilibrium is stable - if any eigenvalue has a non-zero complex part there will be cycling","title":"3. Summary"},{"location":"lectures/lecture-14/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 14: General solutions (multivariate) Run notes interactively? Lecture overview Linear multivariate models Summary 1. Linear multivariate models Discrete time Let's investigate the dynamics of a linear system of equations in more than one variable in discrete time. If there are \\(n\\) variables, then there will be \\(n\\) equations: \\[ \\begin{eqnarray*} x_1(t+1) &=& m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &=& m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots& \\\\ x_n(t+1) &=& m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{eqnarray*} \\] e.g., in a predator-prey model \\(n=2\\) because we track both the number of predators and the number of prey. Because the equations are linear, we can also write these equations in matrix form: \\[ \\begin{eqnarray*} x_1(t+1) &=& m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &=& m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots& \\\\ x_n(t+1) &=& m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{eqnarray*} \\] becomes \\[ \\begin{eqnarray*} \\left( \\begin{array}{c} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{array} \\right) = \\left( \\begin{array}{cccc} m_{11} & m_{12} & \\cdots & m_{1n} \\\\ m_{21} & m_{22} & \\cdots & m_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ m_{n1} & m_{n2} & \\cdots & m_{nn} \\end{array} \\right) \\left( \\begin{array}{c} x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t) \\end{array} \\right) \\end{eqnarray*} \\] which becomes \\[ \\vec{x}(t+1) = \\mathbf{M} \\vec{x}(t) \\] To find out where the population will be at some generation \\(t\\) we can use the equation \\(\\vec{x}(t+1) = \\mathbf{M} \\vec{x}(t)\\) over and over again: \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}\\vec{x}(t-1)\\\\ &= \\mathbf{M}^2\\vec{x}(t-2)\\\\ & \\vdots \\\\ &= \\mathbf{M}^t\\vec{x}(0) \\end{aligned} \\] In most cases, it will be hard to find out what \\(\\mathbf{M}^t\\) equals directly but we can use what we know about eigenvalues and eigenvectors to help. Remember the equation for the eigenvalues and right eigenvectors \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] For our \\(n\\) -dimensional model, there will be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (provided \\(\\mathbf{M}\\) is not defective). We can write all \\(n\\) of these equations in matrix form \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{pmatrix} &= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 & \\lambda_2 \\vec{v}_2 & \\cdots & \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\[ \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0\\\\ 0 & \\lambda_2 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n\\\\ \\end{pmatrix} \\] is a diagonal matrix of the eigenvalues. Now multiply this equation by \\(\\mathbf{A}^{-1}\\) on the right \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\ \\mathbf{M} &= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] Now, we can write \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0) \\end{aligned} \\] And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate. Specifically \\[ \\mathbf{D}^t = \\left( \\begin{array}{cccc} \\lambda_1^t & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^t & \\cdots & 0\\\\ \\vdots & \\vdots & & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n^t \\end{array} \\right) \\] It would not have been so easy to find \\(\\mathbf{M}^t\\) ! This enables us to write the {\\it general solution} to the recursion equations \\[ \\vec{x}(t) = \\mathbf{M}^t\\vec{x}(0) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0) \\] Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\) , which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\) . And now we see why our stability analyses work! Only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|<1\\) , will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time goes to infinity.\\ Further, as time goes to infinity \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we call the \\textbf{leading eigenvalue}. To see this, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\) \\[ \\mathbf{D}^t = \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & (\\lambda_2/\\lambda_1)^t & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & (\\lambda_n/\\lambda_1)^t\\\\ \\end{pmatrix} \\] Since \\(|\\lambda_i/\\lambda_1|<1\\) for all \\(i\\) , for large \\(t\\) these all go to zero and we have \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t \\equiv \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 0\\\\ \\end{pmatrix} \\] We can then approximate \\(\\vec{x}(t)\\) as \\(\\tilde{\\vec{x}}(t) = \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) . This means that \\(\\vec{n}(t)\\) will grow like \\(\\lambda_1^t\\) , where \\(\\lambda_1\\) is the leading eigenvalue \\(\\vec{n}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\) \\(\\vec{n}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\) , describing the initial size of the system, where \\(\\vec{u}_1\\) is the left eigenvector associated with the leading eigenvalue NEED TO ADD SAME PLOTS FROM LECTURE 11 HERE In this way the eigenvectors form a new, more convenient, coordinate system. We transform into this new coordinate system with \\(\\vec{y}(t) = \\mathbf{A}^{-1}\\vec{x}(t)\\) . And we transform back into our original coordinate system with \\(\\vec{x}(t) = \\mathbf{A}\\vec{y}(t)\\) . We then have \\[ \\begin{aligned} \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\) , as we saw in the previous graph. The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\) ? To do this, we can first use some simple geometry on the complex plane to show that any complex number can be written \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] where \\(R = \\sqrt{A^2 + B^2}\\) is the magnitude of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the horizontal axis. We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\) , to write \\[ A + Bi = R e^{i \\theta} \\] And we can now take powers of \\(\\lambda\\) \\[ \\lambda^t = R^t e^{i \\theta t} \\] Continuous time Now let's end by considering linear multivariate models in continuous time. Given that \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x}\\) the general solution is simply \\(\\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0)\\) . But \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated! Fortunately we can use the same transform as in the discrete time case to write \\(e^{\\mathbf{M}t} = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\) . This is much simpler because \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & e^{\\lambda_2 t} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & e^{\\lambda_n t} \\end{pmatrix} \\] When we have complex eigenvalues we again use Euler's equation \\[ e^{\\lambda t} = e^{(A + Bi) t} = e^{At}e^{Bti} = e^{At}(\\cos(Bt) + i\\sin(Bt)) \\] Our general solution is then \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\) . 2. Summary So summarizing our overview on finding general solutions to linear multivariate systems it may be difficult to iterate/exponentiate matrix \\(\\textbf{M}\\) to determine how a linear system of equations changes over time we can transform our equations into a new coordinate system (specified by the eigenvectors) in which the relevant matrix is diagonal it is then easy to iterate/exponentiate the diagonal matrix we back transform to find out where our system is at any time in the future import sympy from sympy import cos, sin, exp import matplotlib.pyplot as plt # Initialize symbols t = sympy.symbols('t') # Parameter values for sex selection model Gz = 0.15 Gp = 0.8 cc = 0.45 bb = 0.3 aa = 0.95 BB = 0.32 # Matrix M in terms of parameters a = -Gz * cc b = Gz * aa - bb * BB c = - BB * cc d = -Gp * bb + aa * BB # General solution for 2d linear model in continuous time B = (-(a - d)**2 - 4*b*c)**(1/2)/2 ADA = sympy.Matrix([[cos(B*t)+(a-d)/(2*B)*sin(B*t), b/B * sin(B*t)],[c/B * sin(B*t), cos(B*t) - (a-d)/(2*B)*sin(B*t)]]) # Plot n0 = sympy.Matrix([1,0]) nt = exp((a + d)*t/2) * ADA * n0 sympy.plot_parametric(nt[0], nt[1], (t, 0, 500))","title":"Lecture 14"},{"location":"lectures/lecture-14/#lecture-14-general-solutions-multivariate","text":"Run notes interactively?","title":"Lecture 14: General solutions (multivariate)"},{"location":"lectures/lecture-14/#lecture-overview","text":"Linear multivariate models Summary","title":"Lecture overview"},{"location":"lectures/lecture-14/#1-linear-multivariate-models","text":"","title":"1. Linear multivariate models"},{"location":"lectures/lecture-14/#discrete-time","text":"Let's investigate the dynamics of a linear system of equations in more than one variable in discrete time. If there are \\(n\\) variables, then there will be \\(n\\) equations: \\[ \\begin{eqnarray*} x_1(t+1) &=& m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &=& m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots& \\\\ x_n(t+1) &=& m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{eqnarray*} \\] e.g., in a predator-prey model \\(n=2\\) because we track both the number of predators and the number of prey. Because the equations are linear, we can also write these equations in matrix form: \\[ \\begin{eqnarray*} x_1(t+1) &=& m_{11} x_1(t) + m_{12} x_2(t) + \\cdots + m_{1n} x_n(t) \\\\ x_2(t+1) &=& m_{21} x_1(t) + m_{22} x_2(t) + \\cdots + m_{2n} x_n(t) \\\\ &\\vdots& \\\\ x_n(t+1) &=& m_{n1} x_1(t) + m_{n2} x_2(t) + \\cdots + m_{nn} x_n(t) \\end{eqnarray*} \\] becomes \\[ \\begin{eqnarray*} \\left( \\begin{array}{c} x_1(t+1) \\\\ x_2(t+1) \\\\ \\vdots \\\\ x_n(t+1) \\end{array} \\right) = \\left( \\begin{array}{cccc} m_{11} & m_{12} & \\cdots & m_{1n} \\\\ m_{21} & m_{22} & \\cdots & m_{2n}\\\\ \\vdots & \\vdots & & \\vdots\\\\ m_{n1} & m_{n2} & \\cdots & m_{nn} \\end{array} \\right) \\left( \\begin{array}{c} x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_n(t) \\end{array} \\right) \\end{eqnarray*} \\] which becomes \\[ \\vec{x}(t+1) = \\mathbf{M} \\vec{x}(t) \\] To find out where the population will be at some generation \\(t\\) we can use the equation \\(\\vec{x}(t+1) = \\mathbf{M} \\vec{x}(t)\\) over and over again: \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}\\vec{x}(t-1)\\\\ &= \\mathbf{M}^2\\vec{x}(t-2)\\\\ & \\vdots \\\\ &= \\mathbf{M}^t\\vec{x}(0) \\end{aligned} \\] In most cases, it will be hard to find out what \\(\\mathbf{M}^t\\) equals directly but we can use what we know about eigenvalues and eigenvectors to help. Remember the equation for the eigenvalues and right eigenvectors \\[ \\mathbf{M} \\vec{v} = \\lambda \\vec{v} \\] For our \\(n\\) -dimensional model, there will be \\(n\\) eigenvalues and eigenvectors that satisfy this equation (provided \\(\\mathbf{M}\\) is not defective). We can write all \\(n\\) of these equations in matrix form \\[ \\begin{aligned} \\mathbf{M} \\begin{pmatrix} \\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{pmatrix} &= \\begin{pmatrix} \\lambda_1 \\vec{v}_1 & \\lambda_2 \\vec{v}_2 & \\cdots & \\lambda_n \\vec{v}_n \\end{pmatrix}\\\\ \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D} \\end{aligned} \\] where the columns of \\(\\mathbf{A}\\) are the right eigenvectors and \\[ \\mathbf{D} = \\begin{pmatrix} \\lambda_1 & 0 & \\cdots & 0\\\\ 0 & \\lambda_2 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n\\\\ \\end{pmatrix} \\] is a diagonal matrix of the eigenvalues. Now multiply this equation by \\(\\mathbf{A}^{-1}\\) on the right \\[ \\begin{aligned} \\mathbf{M} \\mathbf{A} &= \\mathbf{A} \\mathbf{D}\\\\ \\mathbf{M} \\mathbf{A}\\mathbf{A}^{-1} &= \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\\\ \\mathbf{M} &= \\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1} \\end{aligned} \\] Now, we can write \\[ \\begin{aligned} \\vec{x}(t) &= \\mathbf{M}^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1})^t\\vec{x}(0)\\\\ &= (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\cdots (\\mathbf{A}\\mathbf{D}\\mathbf{A}^{-1}) \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{D}(\\mathbf{A}^{-1}\\mathbf{A}) \\cdots (\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{D}\\mathbf{A}^{-1} \\vec{x}(0)\\\\ &= \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1} \\vec{x}(0) \\end{aligned} \\] And this is great because \\(\\mathbf{D}\\) is a diagonal matrix, meaning \\(\\mathbf{D}^t\\) is easy to calculate. Specifically \\[ \\mathbf{D}^t = \\left( \\begin{array}{cccc} \\lambda_1^t & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^t & \\cdots & 0\\\\ \\vdots & \\vdots & & \\vdots\\\\ 0 & 0 & \\cdots & \\lambda_n^t \\end{array} \\right) \\] It would not have been so easy to find \\(\\mathbf{M}^t\\) ! This enables us to write the {\\it general solution} to the recursion equations \\[ \\vec{x}(t) = \\mathbf{M}^t\\vec{x}(0) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{x}(0) \\] Note that \\(\\mathbf{A}^{-1}\\) is a matrix whose rows are the left eigenvectors of \\(\\textbf{M}\\) , which can be seen by deriving \\(\\mathbf{M} = \\mathbf{A} \\mathbf{D}\\mathbf{A}^{-1}\\) starting with the equation for the left eigenvectors \\(\\vec{u}\\mathbf{M}=\\lambda\\vec{u}\\) . And now we see why our stability analyses work! Only if all the eigenvalues have an absolute value less than one, \\(|\\lambda_i|<1\\) , will all entries of \\(\\mathbf{D}^t\\) decay to zero and hence \\(\\vec{x}(t)\\) decay to \\(\\vec{0}\\) (the equilibrium) as time goes to infinity.\\ Further, as time goes to infinity \\(\\mathbf{D}^t\\) will be dominated by the eigenvalue with the largest absolute value, which we call the \\textbf{leading eigenvalue}. To see this, let the leading eigenvalue be \\(\\lambda_1\\) and factor it out of \\(\\mathbf{D}^t\\) \\[ \\mathbf{D}^t = \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & (\\lambda_2/\\lambda_1)^t & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & (\\lambda_n/\\lambda_1)^t\\\\ \\end{pmatrix} \\] Since \\(|\\lambda_i/\\lambda_1|<1\\) for all \\(i\\) , for large \\(t\\) these all go to zero and we have \\[ \\mathbf{D}^t \\approx \\tilde{\\mathbf{D}}^t \\equiv \\lambda_1^t \\begin{pmatrix} 1 & 0 & \\cdots & 0\\\\ 0 & 0 & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & \\cdots & 0\\\\ \\end{pmatrix} \\] We can then approximate \\(\\vec{x}(t)\\) as \\(\\tilde{\\vec{x}}(t) = \\mathbf{A}\\tilde{\\mathbf{D}}^t\\mathbf{A}^{-1}\\vec{x}(0) = \\lambda_1^t \\vec{v}_1 \\vec{u}_1 \\vec{x}(0)\\) . This means that \\(\\vec{n}(t)\\) will grow like \\(\\lambda_1^t\\) , where \\(\\lambda_1\\) is the leading eigenvalue \\(\\vec{n}(t)\\) will approach the right eigenvector associated with the leading eigenvalue, \\(\\vec{v}_1\\) \\(\\vec{n}(t)\\) is weighted by a constant, \\(\\vec{u}_1 \\vec{x}(0)\\) , describing the initial size of the system, where \\(\\vec{u}_1\\) is the left eigenvector associated with the leading eigenvalue NEED TO ADD SAME PLOTS FROM LECTURE 11 HERE In this way the eigenvectors form a new, more convenient, coordinate system. We transform into this new coordinate system with \\(\\vec{y}(t) = \\mathbf{A}^{-1}\\vec{x}(t)\\) . And we transform back into our original coordinate system with \\(\\vec{x}(t) = \\mathbf{A}\\vec{y}(t)\\) . We then have \\[ \\begin{aligned} \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\vec{x}(t+1)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\mathbf{M}\\vec{x}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}\\mathbf{M}\\mathbf{A}\\vec{y}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{A}^{-1}(\\mathbf{A} \\mathbf{D} \\mathbf{A}^{-1})\\mathbf{A}\\vec{y}(t)\\\\ \\vec{y}(t+1) &= \\mathbf{D}\\vec{y}(t) \\end{aligned} \\] Voila! In the new coordinate system we just move along each axis by a factor \\(\\lambda_i\\) , as we saw in the previous graph. The final complication to discuss is complex eigenvalues. Specifically, what is \\(\\lambda^t\\) when \\(\\lambda = A + B i\\) ? To do this, we can first use some simple geometry on the complex plane to show that any complex number can be written \\[ A + Bi = R (\\cos(\\theta) + i \\sin(\\theta)) \\] where \\(R = \\sqrt{A^2 + B^2}\\) is the magnitude of \\(\\lambda\\) and \\(\\theta = \\arctan(B/A)\\) is the angle between \\(\\lambda\\) and the horizontal axis. We then use Euler's equation, \\(\\cos(\\theta) + i \\sin(\\theta) = e^{i \\theta}\\) , to write \\[ A + Bi = R e^{i \\theta} \\] And we can now take powers of \\(\\lambda\\) \\[ \\lambda^t = R^t e^{i \\theta t} \\]","title":"Discrete time"},{"location":"lectures/lecture-14/#continuous-time","text":"Now let's end by considering linear multivariate models in continuous time. Given that \\(\\frac{\\mathrm{d}\\vec{x}}{\\mathrm{d}t} = \\mathbf{M} \\vec{x}\\) the general solution is simply \\(\\vec{x}(t) = e^{\\mathbf{M}t}\\vec{x}(0)\\) . But \\(e^{\\mathbf{M}t} = \\sum_{i=0}^{\\infty} \\mathbf{M}^i t^i / i!\\) is complicated! Fortunately we can use the same transform as in the discrete time case to write \\(e^{\\mathbf{M}t} = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\) . This is much simpler because \\[ e^{\\mathbf{D}t} = \\begin{pmatrix} e^{\\lambda_1 t} & 0 & \\cdots & 0\\\\ 0 & e^{\\lambda_2 t} & \\cdots & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & \\cdots & 0 & e^{\\lambda_n t} \\end{pmatrix} \\] When we have complex eigenvalues we again use Euler's equation \\[ e^{\\lambda t} = e^{(A + Bi) t} = e^{At}e^{Bti} = e^{At}(\\cos(Bt) + i\\sin(Bt)) \\] Our general solution is then \\(\\vec{x}(t) = \\mathbf{A} e^{\\mathbf{D}t} \\mathbf{A}^{-1}\\vec{x}(0)\\) .","title":"Continuous time"},{"location":"lectures/lecture-14/#2-summary","text":"So summarizing our overview on finding general solutions to linear multivariate systems it may be difficult to iterate/exponentiate matrix \\(\\textbf{M}\\) to determine how a linear system of equations changes over time we can transform our equations into a new coordinate system (specified by the eigenvectors) in which the relevant matrix is diagonal it is then easy to iterate/exponentiate the diagonal matrix we back transform to find out where our system is at any time in the future import sympy from sympy import cos, sin, exp import matplotlib.pyplot as plt # Initialize symbols t = sympy.symbols('t') # Parameter values for sex selection model Gz = 0.15 Gp = 0.8 cc = 0.45 bb = 0.3 aa = 0.95 BB = 0.32 # Matrix M in terms of parameters a = -Gz * cc b = Gz * aa - bb * BB c = - BB * cc d = -Gp * bb + aa * BB # General solution for 2d linear model in continuous time B = (-(a - d)**2 - 4*b*c)**(1/2)/2 ADA = sympy.Matrix([[cos(B*t)+(a-d)/(2*B)*sin(B*t), b/B * sin(B*t)],[c/B * sin(B*t), cos(B*t) - (a-d)/(2*B)*sin(B*t)]]) # Plot n0 = sympy.Matrix([1,0]) nt = exp((a + d)*t/2) * ADA * n0 sympy.plot_parametric(nt[0], nt[1], (t, 0, 500))","title":"2. Summary"},{"location":"lectures/lecture-15/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 15: Demography Run notes interactively? Lecture overview Demography Stage-structure Age-structure 1. Demography We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals . This area of research is called demography . The most general case of this is called stage-structure : we consider some finite number of discrete stages that an individual can fall into, and we use a matrix of transition rates between stages (a projection matrix ) to project how the population size and composition changes over time (we'll assume discrete time). A common special case is age-structure : here we define the stages as the number of time steps an individual has been alive for, which leads to a special, simple projection matrix called a Leslie matrix . 2. Stage-structure In the last lecture we saw that for any linear multivariate model \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] we can compute the general solution \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] or, more conveniently, as \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0) \\] Despite this progress, the eigenvalues ( \\(\\mathbf{D}\\) ) and eigenvectors ( \\(\\mathbf{A}\\) ) are often unobtainable (without specifying parameter values), leaving us to rely on the approximation \\[ \\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0) \\] For this we just need to know the leading eigenvalue ( \\(\\lambda\\) ) and the corresponding right ( \\(\\vec{u}\\) ) and left ( \\(\\vec{v}\\) ) eigenvectors, respectively (represented as column vectors, and normalized such that \\(\\vec{v}^\\intercal \\vec{u} =1\\) ). These three components ( \\(\\lambda\\) , \\(\\vec{u}\\) , \\(\\vec{v}\\) ) are what we'll investigate. Formulation Therefore, if we once again consider \\[ \\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0) \\] the stage-structure is model is specified such that \\(\\lambda\\) is the long-term population growth rate , \\(\\vec{u}\\) describes the stable stage-distribution , \\(\\vec{v}\\) describes the reproductive values of each stage, \\(\\vec{v}^\\intercal \\vec{n}(0)\\) describes the total reproductive value at time 0 This approximation is valid as long as the leading eigenvalue, \\(\\lambda\\) , is real (no cycles in long-term) positive (no oscillations to negative numbers!) larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors) Fortunately we are guaranteed all these conditions in our demographic models ! (Since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\) , this follows from something called the Perron-Frobenius Theorem ). Example: Right whale model For the right whale model described graphically below we have \\[ \\mathbf{M} = \\begin{pmatrix} 0 & 0 & 0 & b \\\\ s_{IC} & s_{II} & 0 & 0\\\\ 0 & s_{MI} & s_{MM} & s_{MR} \\\\ 0 & s_{RI} & s_{RM} & s_{RR} \\end{pmatrix} \\] and if we plug in some parameter values ( \\(s_{IC}=0.92, s_{II}=0.86, s_{MI}=0.08, s_{MM}=0.8, s_{MR}=0.88, s_{RI}=0.02, s_{RM}=0.19, s_{RR}=0, b=0.3\\) ) we can calculate \\[ \\begin{aligned} &\\lambda \\approx 1.003 \\\\ &\\vec{u}^\\intercal \\approx (0.04, 0.23, 0.61, 0.12) \\\\ &\\vec{v}^\\intercal\\approx (0.69, 0.76, 1.07, 1.15) \\end{aligned} \\] which tells us, for example, that, in the long-run, the population will grow ( \\(\\lambda>1\\) ), the majority of individuals will be mature, and mature and reproductive individuals have much higher reproductive values than calves and immature individuals. One question we might now consider is how the long-term growth rate, \\(\\lambda\\) , changes with some parameter, \\(z\\) (e.g., \\(z=b\\) ). We could numerically calculate \\(\\lambda\\) for a range of \\(z\\) values. Or, we could compute \\(\\mathrm{d}\\lambda/\\mathrm{d}z\\) while considering \\(\\mathbf{M}\\) , \\(\\lambda\\) , \\(\\vec{u}\\) , and \\(\\vec{v}\\) as functions of \\(z\\) . We call this the \\textbf{sensitivity} of \\(\\lambda\\) to \\(z\\) . Since \\(\\mathbf{M}\\vec{u} = \\lambda \\vec{u}\\) and \\(\\vec{v}^\\intercal\\mathbf{M} = \\vec{v}^\\intercal \\lambda\\) we have \\[ \\begin{aligned} \\mathbf{M} \\vec{u} &= \\lambda \\vec{u} \\\\ \\vec{v}^\\intercal \\mathbf{M} \\vec{u} &= \\vec{v}^\\intercal \\lambda \\vec{u} \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{v}^\\intercal \\mathbf{M} \\vec{u} \\right)&= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{v}^\\intercal \\lambda \\vec{u} \\right)\\\\ \\frac{\\mathrm{d}\\vec{v}^\\intercal}{\\mathrm{d}z} \\mathbf{M} \\vec{u} + \\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u} + \\vec{v}^\\intercal \\mathbf{M} \\frac{\\mathrm{d}\\vec{u}}{\\mathrm{d}z} &= \\frac{\\mathrm{d}\\vec{v}^\\intercal}{\\mathrm{d}z} \\lambda \\vec{u} + \\vec{v}^\\intercal \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\vec{u} + \\vec{v}^\\intercal \\lambda \\frac{\\mathrm{d}\\vec{u}}{\\mathrm{d}z}\\\\ \\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u} &= \\vec{v}^\\intercal \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\vec{u} \\\\ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} &= \\frac{\\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u}}{\\vec{v}^\\intercal \\vec{u}} \\end{aligned}\\] This will be hard to do in general, but we can evaluate at some particular value \\(z^*\\) \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{v}^\\intercal \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{u}}{\\vec{v}^\\intercal \\vec{u}}} \\] We can now ask (and answer!) a couple of questions: If we wanted to increase the total population size in the future, and we could add one individual to any stage, which stage should it be? We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0)\\) . This will not affect the long-term growth rate ( \\(\\lambda\\) ) or the stable-stage distribution ( \\(\\vec{u}\\) ). We can therefore only increase \\(\\vec{v}^\\intercal \\vec{n}(0) = v_1 n_1(0) + v_2 n_2(0) + ... + v_m n_m(0)\\) . And so we add 1 to the stage with the largest reproductive value, \\(v_i\\) . In the whale example we had \\(\\vec{v}^\\intercal = (0.69, 0.76, 1.07, 1.15)\\) , so we should add a reproductive individual. If we wanted to increase the long-term population growth rate, and we could increase any parameter a little bit, which parameter should it be? Here we want to know which parameter, \\(z\\) , gives the largest value of \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\big|_{z=z^*}\\) . In the whale example increasing \\(s_{RM}\\) , the probability a mature individual becomes reproductive, has the largest effect. 3. Age-structure Now let's look at the special case of age-structure . Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\) . Because of this, the projection matrix is particularly simple \\[ \\mathbf{L} = \\begin{pmatrix} m_1 & m_2 & m_3 & \\cdots & m_d \\\\ p_1 & 0 & 0 & \\cdots & 0 \\\\ 0 & p_2 & 0 & \\cdots & 0 \\\\ \\vdots & & \\vdots & & \\vdots \\\\ 0 & \\cdots & 0 & p_d & 0\\\\ \\end{pmatrix} \\] and we call it a Leslie matrix . Many expressions are now simpler. For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\) , can be calculated using the first row, and after rearranging we get what is known as the Euler-Lotka equation \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] where \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the probability of surviving to age \\(i\\) . Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term growth rate, \\(\\lambda\\) . For example, let's look at a model of stickleback, a small fish. We assume stickleback do not live more than 4 years, and estimate the Leslie matrix as \\[ \\mathbf{L} = \\begin{pmatrix} 2 & 3 & 4 & 4\\\\ 0.6 & 0 & 0 & 0 \\\\ 0 & 0.3 & 0 & 0 \\\\ 0 & 0 & 0.1 & 0 \\end{pmatrix} \\] The first row gives \\(m_1=2, m_2=3, m_3=4, m_4=4\\) and the survival probabilities give \\(l_1=1\\) , \\(l_2=0.6\\) , \\(l_3=(0.6)(0.3)=0.18\\) , \\(l_4=(0.6)(0.3)(0.1)=0.018\\) . The Euler-Lotka equation is then \\[ \\begin{aligned} 1 &= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\ \\end{aligned} \\] which can be solved by SymPy/SageMath (or whatever) to give \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\) . The long-term growth rate is therefore \\(\\lambda=2.75\\) . With age-structure can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and population growth. The proportion of individuals that are age \\(x\\) (in the long-run) is \\[ \\large{u_x = \\frac{l_x \\lambda^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda^{-(i-1)}}} \\] The reproductive value of individuals that are age \\(x\\) , relative to age \\(1\\) , is \\[ \\large{\\frac{v_x}{v_1} = \\frac{\\lambda^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda^{i}}} \\] The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_x} = \\frac{u_x v_{x+1}}{\\vec{v}^\\intercal \\vec{u}}} \\] \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_x} = \\frac{u_x v_1}{\\vec{v}^\\intercal \\vec{u}}} \\] For example, in our stickleback model the proportion of the population that is age \\(x=2\\) , in the long-run, is \\[ \\begin{aligned} u_x &= \\frac{l_x \\lambda^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda^{-(i-1)}}\\\\ u_2 &= \\frac{l_2 \\lambda^{-1}}{\\sum_{i=1}^{4}l_i \\lambda^{-(i-1)}}\\\\ &\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &\\approx 0.18 \\end{aligned} \\] and the relative reproductive value of age \\(x=2\\) individuals is \\[ \\begin{aligned} \\frac{v_x}{v_1} &= \\frac{\\lambda^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda^{i}}\\\\ \\frac{v_2}{v_1} &= \\frac{\\lambda^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda^{i}}\\\\ &= \\frac{\\lambda}{l_2} \\left(\\frac{l_2 m_2}{\\lambda^{2}} + \\frac{l_3 m_3}{\\lambda^{3}} + \\frac{l_4 m_4}{\\lambda^{i}} \\right)\\\\ &\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &\\approx 1.25 \\end{aligned} \\] Repeating these for the other ages we get the stable-age distribution \\[ \\vec{u}^\\intercal \\approx (0.80, 0.18, 0.02, 0.0007) \\] and the reproductive values \\[ \\vec{v}^\\intercal\\approx (1, 1.25, 1.51, 1.45) \\] which we can then use to get the sensitivities of the growth rate to survival ( \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_x} = \\frac{u_x v_{x+1}}{\\vec{v}^\\intercal \\vec{u}}\\) ) \\[ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_1} \\approx 0.95, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_2} \\approx 0.25, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_3} \\approx 0.03 \\] and to fecundity ( \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_x} = \\frac{u_x v_1}{\\vec{v}^\\intercal \\vec{u}}\\) ) \\[ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_1} \\approx 0.76, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_2} \\approx 0.17, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_3} \\approx 0.02, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_4} \\approx 0.0007 \\] And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the biggest effect.","title":"Lecture 15"},{"location":"lectures/lecture-15/#lecture-15-demography","text":"Run notes interactively?","title":"Lecture 15: Demography"},{"location":"lectures/lecture-15/#lecture-overview","text":"Demography Stage-structure Age-structure","title":"Lecture overview"},{"location":"lectures/lecture-15/#1-demography","text":"We're now going to use what we've learned about linear multivariate models to describe the dynamics of a single population that is composed of different types of individuals . This area of research is called demography . The most general case of this is called stage-structure : we consider some finite number of discrete stages that an individual can fall into, and we use a matrix of transition rates between stages (a projection matrix ) to project how the population size and composition changes over time (we'll assume discrete time). A common special case is age-structure : here we define the stages as the number of time steps an individual has been alive for, which leads to a special, simple projection matrix called a Leslie matrix .","title":"1. Demography"},{"location":"lectures/lecture-15/#2-stage-structure","text":"In the last lecture we saw that for any linear multivariate model \\[ \\vec{n}(t+1) = \\mathbf{M}\\vec{n}(t) \\] we can compute the general solution \\[ \\vec{n}(t) = \\mathbf{M}^t\\vec{n}(0) \\] or, more conveniently, as \\[ \\vec{n}(t) = \\mathbf{A}\\mathbf{D}^t\\mathbf{A}^{-1}\\vec{n}(0) \\] Despite this progress, the eigenvalues ( \\(\\mathbf{D}\\) ) and eigenvectors ( \\(\\mathbf{A}\\) ) are often unobtainable (without specifying parameter values), leaving us to rely on the approximation \\[ \\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0) \\] For this we just need to know the leading eigenvalue ( \\(\\lambda\\) ) and the corresponding right ( \\(\\vec{u}\\) ) and left ( \\(\\vec{v}\\) ) eigenvectors, respectively (represented as column vectors, and normalized such that \\(\\vec{v}^\\intercal \\vec{u} =1\\) ). These three components ( \\(\\lambda\\) , \\(\\vec{u}\\) , \\(\\vec{v}\\) ) are what we'll investigate.","title":"2. Stage-structure"},{"location":"lectures/lecture-15/#formulation","text":"Therefore, if we once again consider \\[ \\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0) \\] the stage-structure is model is specified such that \\(\\lambda\\) is the long-term population growth rate , \\(\\vec{u}\\) describes the stable stage-distribution , \\(\\vec{v}\\) describes the reproductive values of each stage, \\(\\vec{v}^\\intercal \\vec{n}(0)\\) describes the total reproductive value at time 0 This approximation is valid as long as the leading eigenvalue, \\(\\lambda\\) , is real (no cycles in long-term) positive (no oscillations to negative numbers!) larger than all other eigenvalues (so that we can ignore the other eigenvalues/vectors) Fortunately we are guaranteed all these conditions in our demographic models ! (Since all entries of \\(\\mathbf{M}\\) are non-negative and all entries of \\(\\mathbf{M}^t\\) are positive for some value of \\(t\\) , this follows from something called the Perron-Frobenius Theorem ).","title":"Formulation"},{"location":"lectures/lecture-15/#example-right-whale-model","text":"For the right whale model described graphically below we have \\[ \\mathbf{M} = \\begin{pmatrix} 0 & 0 & 0 & b \\\\ s_{IC} & s_{II} & 0 & 0\\\\ 0 & s_{MI} & s_{MM} & s_{MR} \\\\ 0 & s_{RI} & s_{RM} & s_{RR} \\end{pmatrix} \\] and if we plug in some parameter values ( \\(s_{IC}=0.92, s_{II}=0.86, s_{MI}=0.08, s_{MM}=0.8, s_{MR}=0.88, s_{RI}=0.02, s_{RM}=0.19, s_{RR}=0, b=0.3\\) ) we can calculate \\[ \\begin{aligned} &\\lambda \\approx 1.003 \\\\ &\\vec{u}^\\intercal \\approx (0.04, 0.23, 0.61, 0.12) \\\\ &\\vec{v}^\\intercal\\approx (0.69, 0.76, 1.07, 1.15) \\end{aligned} \\] which tells us, for example, that, in the long-run, the population will grow ( \\(\\lambda>1\\) ), the majority of individuals will be mature, and mature and reproductive individuals have much higher reproductive values than calves and immature individuals. One question we might now consider is how the long-term growth rate, \\(\\lambda\\) , changes with some parameter, \\(z\\) (e.g., \\(z=b\\) ). We could numerically calculate \\(\\lambda\\) for a range of \\(z\\) values. Or, we could compute \\(\\mathrm{d}\\lambda/\\mathrm{d}z\\) while considering \\(\\mathbf{M}\\) , \\(\\lambda\\) , \\(\\vec{u}\\) , and \\(\\vec{v}\\) as functions of \\(z\\) . We call this the \\textbf{sensitivity} of \\(\\lambda\\) to \\(z\\) . Since \\(\\mathbf{M}\\vec{u} = \\lambda \\vec{u}\\) and \\(\\vec{v}^\\intercal\\mathbf{M} = \\vec{v}^\\intercal \\lambda\\) we have \\[ \\begin{aligned} \\mathbf{M} \\vec{u} &= \\lambda \\vec{u} \\\\ \\vec{v}^\\intercal \\mathbf{M} \\vec{u} &= \\vec{v}^\\intercal \\lambda \\vec{u} \\\\ \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{v}^\\intercal \\mathbf{M} \\vec{u} \\right)&= \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left(\\vec{v}^\\intercal \\lambda \\vec{u} \\right)\\\\ \\frac{\\mathrm{d}\\vec{v}^\\intercal}{\\mathrm{d}z} \\mathbf{M} \\vec{u} + \\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u} + \\vec{v}^\\intercal \\mathbf{M} \\frac{\\mathrm{d}\\vec{u}}{\\mathrm{d}z} &= \\frac{\\mathrm{d}\\vec{v}^\\intercal}{\\mathrm{d}z} \\lambda \\vec{u} + \\vec{v}^\\intercal \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\vec{u} + \\vec{v}^\\intercal \\lambda \\frac{\\mathrm{d}\\vec{u}}{\\mathrm{d}z}\\\\ \\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u} &= \\vec{v}^\\intercal \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\vec{u} \\\\ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} &= \\frac{\\vec{v}^\\intercal \\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\vec{u}}{\\vec{v}^\\intercal \\vec{u}} \\end{aligned}\\] This will be hard to do in general, but we can evaluate at some particular value \\(z^*\\) \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\bigg|_{z=z^*} = \\frac{\\vec{v}^\\intercal \\left(\\frac{\\mathrm{d}\\mathbf{M}}{\\mathrm{d}z} \\big|_{z=z^*} \\right) \\vec{u}}{\\vec{v}^\\intercal \\vec{u}}} \\] We can now ask (and answer!) a couple of questions: If we wanted to increase the total population size in the future, and we could add one individual to any stage, which stage should it be? We want to know what entry of \\(\\vec{n}(0)\\) to add 1 to to maximize \\(\\vec{n}(t) \\approx \\lambda^t \\vec{u} \\vec{v}^\\intercal \\vec{n}(0)\\) . This will not affect the long-term growth rate ( \\(\\lambda\\) ) or the stable-stage distribution ( \\(\\vec{u}\\) ). We can therefore only increase \\(\\vec{v}^\\intercal \\vec{n}(0) = v_1 n_1(0) + v_2 n_2(0) + ... + v_m n_m(0)\\) . And so we add 1 to the stage with the largest reproductive value, \\(v_i\\) . In the whale example we had \\(\\vec{v}^\\intercal = (0.69, 0.76, 1.07, 1.15)\\) , so we should add a reproductive individual. If we wanted to increase the long-term population growth rate, and we could increase any parameter a little bit, which parameter should it be? Here we want to know which parameter, \\(z\\) , gives the largest value of \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}z} \\big|_{z=z^*}\\) . In the whale example increasing \\(s_{RM}\\) , the probability a mature individual becomes reproductive, has the largest effect.","title":"Example: Right whale model"},{"location":"lectures/lecture-15/#3-age-structure","text":"Now let's look at the special case of age-structure . Here individuals in stage \\(i\\) at time \\(t\\) can only contribute to stage \\(i+1\\) (survival) and stage \\(1\\) (reproduction) at time \\(t+1\\) . Because of this, the projection matrix is particularly simple \\[ \\mathbf{L} = \\begin{pmatrix} m_1 & m_2 & m_3 & \\cdots & m_d \\\\ p_1 & 0 & 0 & \\cdots & 0 \\\\ 0 & p_2 & 0 & \\cdots & 0 \\\\ \\vdots & & \\vdots & & \\vdots \\\\ 0 & \\cdots & 0 & p_d & 0\\\\ \\end{pmatrix} \\] and we call it a Leslie matrix . Many expressions are now simpler. For example, the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{L}-\\mathbf{I}\\lambda)=0\\) , can be calculated using the first row, and after rearranging we get what is known as the Euler-Lotka equation \\[ 1 = \\sum_{i=1}^{d} \\frac{l_i m_i}{\\lambda^i} \\] where \\(l_i = p_1 p_2 \\cdots p_{i-1}\\) is the probability of surviving to age \\(i\\) . Given the \\(l_i\\) and \\(m_i\\) we can use this equation to find the long-term growth rate, \\(\\lambda\\) . For example, let's look at a model of stickleback, a small fish. We assume stickleback do not live more than 4 years, and estimate the Leslie matrix as \\[ \\mathbf{L} = \\begin{pmatrix} 2 & 3 & 4 & 4\\\\ 0.6 & 0 & 0 & 0 \\\\ 0 & 0.3 & 0 & 0 \\\\ 0 & 0 & 0.1 & 0 \\end{pmatrix} \\] The first row gives \\(m_1=2, m_2=3, m_3=4, m_4=4\\) and the survival probabilities give \\(l_1=1\\) , \\(l_2=0.6\\) , \\(l_3=(0.6)(0.3)=0.18\\) , \\(l_4=(0.6)(0.3)(0.1)=0.018\\) . The Euler-Lotka equation is then \\[ \\begin{aligned} 1 &= \\sum_{i=1}^{4} \\frac{l_i m_i}{\\lambda^i}\\\\ 1 &= \\frac{l_1 m_1}{\\lambda} + \\frac{l_2 m_2}{\\lambda^2} + \\frac{l_3 m_3}{\\lambda^3} + \\frac{l_4 m_4}{\\lambda^4}\\\\ 1 &= \\frac{2}{\\lambda} + \\frac{1.8}{\\lambda^2} + \\frac{0.72}{\\lambda^3} + \\frac{0.072}{\\lambda^4}\\\\ \\end{aligned} \\] which can be solved by SymPy/SageMath (or whatever) to give \\(\\lambda\\approx2.75, -0.3 \\pm 0.3i, -0.14\\) . The long-term growth rate is therefore \\(\\lambda=2.75\\) . With age-structure can also write the stable age distribution, reproductive values, and sensitivities as functions of survival, fecundity, and population growth. The proportion of individuals that are age \\(x\\) (in the long-run) is \\[ \\large{u_x = \\frac{l_x \\lambda^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda^{-(i-1)}}} \\] The reproductive value of individuals that are age \\(x\\) , relative to age \\(1\\) , is \\[ \\large{\\frac{v_x}{v_1} = \\frac{\\lambda^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda^{i}}} \\] The sensitivities of the long-term growth rate to survival and reproduction at age \\(x\\) are \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_x} = \\frac{u_x v_{x+1}}{\\vec{v}^\\intercal \\vec{u}}} \\] \\[ \\large{\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_x} = \\frac{u_x v_1}{\\vec{v}^\\intercal \\vec{u}}} \\] For example, in our stickleback model the proportion of the population that is age \\(x=2\\) , in the long-run, is \\[ \\begin{aligned} u_x &= \\frac{l_x \\lambda^{-(x-1)}}{\\sum_{i=1}^{d}l_i \\lambda^{-(i-1)}}\\\\ u_2 &= \\frac{l_2 \\lambda^{-1}}{\\sum_{i=1}^{4}l_i \\lambda^{-(i-1)}}\\\\ &\\approx \\frac{0.6 (2.75)^{-1}}{1 (2.75)^{0} + 0.6 (2.75)^{-1} + 0.18 (2.75)^{-2} + 0.018 (2.75)^{-3}}\\\\ &\\approx 0.18 \\end{aligned} \\] and the relative reproductive value of age \\(x=2\\) individuals is \\[ \\begin{aligned} \\frac{v_x}{v_1} &= \\frac{\\lambda^{x-1}}{l_x}\\sum_{i=x}^{d}\\frac{l_i m_i}{\\lambda^{i}}\\\\ \\frac{v_2}{v_1} &= \\frac{\\lambda^{2-1}}{l_2}\\sum_{i=2}^{4}\\frac{l_i m_i}{\\lambda^{i}}\\\\ &= \\frac{\\lambda}{l_2} \\left(\\frac{l_2 m_2}{\\lambda^{2}} + \\frac{l_3 m_3}{\\lambda^{3}} + \\frac{l_4 m_4}{\\lambda^{i}} \\right)\\\\ &\\approx \\frac{2.75}{0.6} \\left(\\frac{(0.6) (3)}{2.75^{2}} + \\frac{(0.18) (4)}{2.75^{3}} + \\frac{(0.018) (4)}{2.75^{4}} \\right)\\\\ &\\approx 1.25 \\end{aligned} \\] Repeating these for the other ages we get the stable-age distribution \\[ \\vec{u}^\\intercal \\approx (0.80, 0.18, 0.02, 0.0007) \\] and the reproductive values \\[ \\vec{v}^\\intercal\\approx (1, 1.25, 1.51, 1.45) \\] which we can then use to get the sensitivities of the growth rate to survival ( \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_x} = \\frac{u_x v_{x+1}}{\\vec{v}^\\intercal \\vec{u}}\\) ) \\[ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_1} \\approx 0.95, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_2} \\approx 0.25, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p_3} \\approx 0.03 \\] and to fecundity ( \\(\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_x} = \\frac{u_x v_1}{\\vec{v}^\\intercal \\vec{u}}\\) ) \\[ \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_1} \\approx 0.76, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_2} \\approx 0.17, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_3} \\approx 0.02, \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}m_4} \\approx 0.0007 \\] And so we see that, because such a large proportion of the population is expected to be age 1, increasing the survival and fecundity of that age has the biggest effect.","title":"3. Age-structure"},{"location":"lectures/lecture-16/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 16: Evolutionary invasion analysis Run notes interactively? Lecture overview Background Analyzing evolutionary invasion Summary and visualization of evolutionary singular strategies 1. Background In the models we've discussed, we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time [n(t+1)=n(t) R] we took \\(R\\) to be the same for all individuals for all time. But clearly any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis . determine which parameters of our model an evolving trait affects take the population to be fixed for some \"resident\" trait value determine the equilibria and stability of the system with only the resident trait derive an equation for the growth of a rare \"mutant\" allele that affects the trait ask when the mutant will invade look for potential evolutionary endpoints determine the stability of those endpoints 2. Analyzing evolutionary invasion Let's think about this analysis very generally (in discrete time). Let the number of resident alleles in a population be \\(n\\) and the number of mutant alleles \\(n_m\\) (And we'll assume asexual haploids for simplicity.) Further, let the potentially nonlinear dynamics of these populations depend on their respective trait values, \\(z\\) and \\(z_m\\) , \\[ \\begin{aligned} n(t+1) &= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] The Jacobian is \\[ \\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix} \\end{aligned} \\] Now consider some non-zero resident equilibrium, \\(\\hat{n}>0\\) , without the mutant, \\(\\hat n_{m=0}\\) . Assuming that the resident does not produce mutants continually, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big\\vert_{n_m=0}=0\\) , the Jacobian evaluated at this equilibrium simplifies \\[ \\begin{aligned} \\mathbf{J}\\big\\vert_{n_m=0,n=\\hat{n}} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix}_{n_{m=0,n=\\hat{n}}} \\end{aligned} \\] We can immediately see the two eigenvalues of this upper triangular matrix \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg\\vert_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg\\vert_{n_m=0,n=\\hat{n}}\\\\ \\end{aligned} \\] The first, \\(\\lambda_1\\) , determines whether the resident equilibrium, \\(\\hat{n}>0\\) , is stable in the absence of mutants. We'll take \\(0< \\lambda_1 < 1\\) as given. The second, \\(\\lambda_2\\) , determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness , \\(\\lambda(z_m,z)\\) . In particular, the mutant will invade whenever \\(\\lambda(z_m,z) > 1\\) . (Note that in some cases, like in the example from last week about the evolution of dispersal, we can derive \\(\\lambda(z_m,z)\\) from a description of the model without doing the formal stability analysis outlined above.) In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)>1\\) , to determine what values of \\(z_m\\) (relative to \\(z\\) ) can invade. For example, in the evolution of dispersal example, we could prove that \\(\\lambda(z_m,z)>1\\) if and only if the mutant dispersal rate, \\(d_m=d(z_m)\\) , was between the resident dispersal rate \\(d = d(z)\\) and \\(1 - c d\\) (see plots below). import sympy import numpy as np # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants d1, c1 = 1/4, 1/3 d2, c2 = 9/10, 1/3 # Build plots p1 = sympy.plot( w.subs({'d': d1, 'c': c1}), (dm, 0, 1), xlabel=r'$d_m$', ylabel=r'$\\lambda(d_m,d)$', size=(9,4), show=False, ylim=(0.8, 1.2), label=f\"d = {d1}, c = {np.round(c1,2)}\", legend=True ) p2 = sympy.plot( w.subs({'d': d2, 'c': c2}), (dm, 0, 1), xlabel=r'$d_m$', ylabel=r'$\\lambda(d_m,d)$', show=False, ylim=(0.8, 1.2), label=f\"d = {d2}, c = {np.round(c2,2)}\", legend=True ) # Add scatter where intersection occurs solutions = sympy.solve(sympy.Eq(w, 1), dm) print(f\"Both plots intersect y=1 at {solutions}\") # Overlay p2 onto p1 and display plot p1.append(p2[0]) p1.show() In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation. When the mutant trait value is very close to the resident trait value, we can use a first order Taylor series approximation around \\(z_m = z\\) \\[ \\begin{aligned} \\lambda(z_m,z) &\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z} (z_m-z)\\\\ &= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z} (z_m-z) \\end{aligned} \\] This allows us to determine which direction evolution will proceed: \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}>0 \\implies\\) invasion, \\(\\lambda(z_m,z)>1\\) , when \\(z_m>z\\) \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}<0 \\implies\\) invasion, \\(\\lambda(z_m,z)>1\\) , when \\(z_{m} < z\\) We can go back and modify the plot we made above for the evolution of dispersal to see what way evolution will proceed. import sympy import numpy as np import matplotlib.pyplot as plt # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants d1, c1 = 1/4, 1/3 d2, c2 = 9/10, 1/3 # Compute the derivative at the point when resident and mutant trait values are the same s1 = float(sympy.diff(w, dm).subs({'d': d1, 'c': c1, 'dm': d1})) s2 = float(sympy.diff(w, dm).subs({'d': d2, 'c': c2, 'dm': d2})) # Let's make a pythonic function to make plotting a bit more expressive fw1 = sympy.lambdify(dm, w.subs({'d': d1, 'c': c1})) fw2 = sympy.lambdify(dm, w.subs({'d': d2, 'c': c2})) xdm = np.linspace(0, 1, 50) # Initialize plots fig, ax = plt.subplots(1,2, figsize=(9,3)) # Plot the curves ax[0].plot(xdm, fw1(xdm), label=f\"d = {d1}, c = {np.round(c1,2)}\") ax[1].plot(xdm, fw2(xdm), label=f\"d = {d2}, c = {np.round(c2,2)}\") # Add the tangent lines showing the slope ax[0].plot( np.linspace(d1-0.1, d1+0.1, 10), np.linspace(1-(s1*0.1), 1+(s1*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) ax[1].plot( np.linspace(d2-0.1, d2+0.1, 10), np.linspace(1-(s2*0.1), 1+(s2*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) # What way does evolution proceed? ax[0].arrow(x=d1-0.05, y=1.1, dx=np.sign((1 - c1*d1)-d1)*0.1, dy=0, width=.01, color='green') ax[1].arrow(x=d2-0.05, y=1.1, dx=np.sign((1 - c2*d2)-d2)*0.1, dy=0, width=.01, color='green') # Add intersect annotations ax[0].annotate(r'$d$', xy=(d1, 0.97)) ax[0].annotate(r'$1-cd$', xy=(1-c1*d1 - 0.05, 0.97)) ax[0].scatter(d1, 1) ax[0].scatter(1-c1*d1, 1) ax[1].annotate(r'$d$', xy=(d2, 0.97)) ax[1].annotate(r'$1-cd$', xy=(1-c2*d2 - 0.05, 0.97)) ax[1].scatter(d2, 1) ax[1].scatter(1-c2*d2, 1) # Modify title and range of axes and add legend for i in range(2): ax[i].axhline(1, linestyle='dotted', color='black') ax[i].set_xlabel(r'$d_m$') ax[i].set_ylabel(r'$\\lambda(d_m,d)$') ax[i].set_ylim(0.8, 1.2) ax[i].legend(frameon=False) # Display plot fig.tight_layout() plt.show() The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}\\) , the selection gradient . Potential evolutionary endpoints, also called evolutionarily singular strategies , are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] In the evolution of dispersal example, we found \\(\\hat{d} = \\frac{1}{1+c}\\) import sympy import numpy as np import matplotlib.pyplot as plt # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants c3 = 1/3 d3 = 1/(1+c3) # Compute the derivative at the point when resident and mutant trait values are the same s3 = float(sympy.diff(w, dm).subs({'d': d3, 'c': c3, 'dm': d3})) # Let's make a pythonic function to make plotting a bit more expressive fw3 = sympy.lambdify(dm, w.subs({'d': d3, 'c': c3})) xdm = np.linspace(0, 1, 50) # Initialize plots fig, ax = plt.subplots(figsize=(4.5,3)) # Plot the curves ax.plot(xdm, fw3(xdm), label=f\"d = {d3}, c = {np.round(c3,2)}\") # Add the tangent lines showing the slope ax.plot( np.linspace(d3-0.1, d3+0.1, 10), np.linspace(1-(s3*0.1), 1+(s3*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) # What way does evolution proceed? ax.arrow(x=d3, y=1.1, dx=np.sign((1 - c3*d3)-d3)*0.1, dy=0, width=.01, color='green') # Add intersect annotations #ax.annotate(text=r'$d$', xy=(d3, 0.97)) ax.annotate(r'$d = 1-cd$', xy=(1-c3*d3 - 0.11, 0.95)) ax.scatter(d3, 1) ax.scatter(1-c3*d3, 1) # Modify title and range of axes and add legend ax.axhline(1, linestyle='dotted', color='black') ax.set_xlabel(r'$d_m$') ax.set_ylabel(r'$\\lambda(d_m,d)$') ax.set_ylim(0.8, 1.2) ax.legend(frameon=False) # Display plot fig.tight_layout() plt.show() An evolutionarily singular strategy, \\(\\hat{z}\\) , will only be an evolutionarily stable strategy (ESS), \\(z^{\\star}\\) , if it cannot be invaded. In the evolution of dispersal example, we could show that \\(\\hat{d} = \\frac{1}{1+c}\\) was globally stable because \\(\\lambda(d_m,d)\\vert_{d=\\hat{d}}<1\\) for all \\(d_m\\) . Global stability will be impossible to prove for more complex models, and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)\\vert_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg\\vert_{z_m=\\hat{z}, z=\\hat{z}} < 0 \\] To summarize, an evolutionarily stable strategy , \\(z^{\\star}\\) , satisfies both \\[ \\begin{aligned} \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z^{\\star}, z=z^{\\star}} &= 0\\\\ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg\\vert_{z_m=z^{\\star}, z=z^{\\star}} &< 0 \\end{aligned} \\] i.e., \\(z^{\\star}\\) is a (local) fitness maximum. There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\) , we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\) . In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\) \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z} \\right)_{z=\\hat{z}} < 0 \\] Singular strategies that satisfy this criteria are called convergence stable . Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies . Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points . The latter are of particular interest because the system evolves towards a state where multiple strategies can coexist, leading to diversification. In tutorial we'll see an example of an evolutionary branching point. 3. Summary and visualization of evolutionary singulary strategies There are four types of evolutionarily singular strategies \\(\\hat{z}\\) , where \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=\\hat{z},z=\\hat{z}}=0\\) A helpful way to visualize the two types of stability at an evolutionarily singular strategy is called a pairwise invasibility plot (PIP) from sympy import exp from matplotlib.colors import ListedColormap # Initialize symbols sm, s, r, va, vk, km, so, x = sympy.symbols('sm,s,r,va,vk,km,so,x') # Initialize model a = exp(-(sm-s)**2/va) k = km * exp(-(s-so)**2/vk) w = 1 + r - r * a * k.subs({'s': s}) / k.subs({'s': sm}) w0 = w.subs({'r':1.1, 'vk': 1, 'va': 1.5, 'so': 0}) #invasion fitness function # Solve for w0 == 1 sol = sympy.solve(sympy.Eq(w0, 1), s) fs = [sympy.lambdify(sm, j) for j in sol] # Make the sympy model into python function fw = sympy.lambdify((s, sm), w0) # Plotting parameters ymin, ymax, dz = -2, 2, 0.1 z = np.linspace(ymin, ymax, 200) fig, ax = plt.subplots() # Binary invasion mask invasion = np.array([[[fw(s, sm) for s in z] for sm in z]])[0] cmap = ListedColormap(['white', 'blue']) ax.contourf(z, z, invasion > 1, cmap=cmap) # Add plots at w0 == 1 for f in fs: ax.plot(f(z), z, color='black', linewidth=3) # Add scatter point over intersection of w0 == 1 solutions ax.scatter( sympy.solve(sympy.Eq(sol[1], sol[0])), sympy.solve(sympy.Eq(sol[0], sol[1])), color='red', s = 200, zorder=10 ) # Add labels and specify ranges ax.set_xlim(ymin, ymax) ax.set_ylim(ymin, ymax) ax.set_xlabel('resident strategy, $z$') ax.set_ylabel('mutant strategy, $z_m$') plt.show() Above, in the pairwise invasibility plot (PIP) , we colour in the mutant strategies that can invade a given resident strategy. This allows us to see which singular strategies are convergence stable. And whether a singular strategy is evolutionarily stable as well. Below, we summarize all of the evolutionary stable strategies in context with the corresponding PIP.","title":"Lecture 16"},{"location":"lectures/lecture-16/#lecture-16-evolutionary-invasion-analysis","text":"Run notes interactively?","title":"Lecture 16: Evolutionary invasion analysis"},{"location":"lectures/lecture-16/#lecture-overview","text":"Background Analyzing evolutionary invasion Summary and visualization of evolutionary singular strategies","title":"Lecture overview"},{"location":"lectures/lecture-16/#1-background","text":"In the models we've discussed, we've taken the parameters to be fixed. In reality, many of these parameters can evolve. For example, in our model of exponential growth in discrete time [n(t+1)=n(t) R] we took \\(R\\) to be the same for all individuals for all time. But clearly any mutation causing a larger \\(R\\) would increase in frequency, causing the value of \\(R\\) to increase over time. In this lecture we'll explore how to determine the direction of evolution and the stability of evolutionary endpoints for more complex models using a technique called evolutionary invasion analysis . determine which parameters of our model an evolving trait affects take the population to be fixed for some \"resident\" trait value determine the equilibria and stability of the system with only the resident trait derive an equation for the growth of a rare \"mutant\" allele that affects the trait ask when the mutant will invade look for potential evolutionary endpoints determine the stability of those endpoints","title":"1. Background"},{"location":"lectures/lecture-16/#2-analyzing-evolutionary-invasion","text":"Let's think about this analysis very generally (in discrete time). Let the number of resident alleles in a population be \\(n\\) and the number of mutant alleles \\(n_m\\) (And we'll assume asexual haploids for simplicity.) Further, let the potentially nonlinear dynamics of these populations depend on their respective trait values, \\(z\\) and \\(z_m\\) , \\[ \\begin{aligned} n(t+1) &= n(t) R(n(t), n_m(t), z, z_m)\\\\ n_m(t+1) &= n_m(t) R_m(n(t), n_m(t), z, z_m) \\end{aligned} \\] The Jacobian is \\[ \\begin{aligned} \\mathbf{J} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix} \\end{aligned} \\] Now consider some non-zero resident equilibrium, \\(\\hat{n}>0\\) , without the mutant, \\(\\hat n_{m=0}\\) . Assuming that the resident does not produce mutants continually, \\(\\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n(t)}\\big\\vert_{n_m=0}=0\\) , the Jacobian evaluated at this equilibrium simplifies \\[ \\begin{aligned} \\mathbf{J}\\big\\vert_{n_m=0,n=\\hat{n}} &= \\begin{pmatrix} \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)} & \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n_m(t)} \\\\ 0 & \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)} \\end{pmatrix}_{n_{m=0,n=\\hat{n}}} \\end{aligned} \\] We can immediately see the two eigenvalues of this upper triangular matrix \\[ \\begin{aligned} \\lambda_1 &= \\frac{\\mathrm{d}n(t+1)}{\\mathrm{d}n(t)}\\bigg\\vert_{n_m=0,n=\\hat{n}}\\\\ \\lambda_2 &= \\frac{\\mathrm{d}n_m(t+1)}{\\mathrm{d}n_m(t)}\\bigg\\vert_{n_m=0,n=\\hat{n}}\\\\ \\end{aligned} \\] The first, \\(\\lambda_1\\) , determines whether the resident equilibrium, \\(\\hat{n}>0\\) , is stable in the absence of mutants. We'll take \\(0< \\lambda_1 < 1\\) as given. The second, \\(\\lambda_2\\) , determines whether the resident equilibrium is stable in the presence of a small number of mutants. We call \\(\\lambda_2\\) the invasion fitness , \\(\\lambda(z_m,z)\\) . In particular, the mutant will invade whenever \\(\\lambda(z_m,z) > 1\\) . (Note that in some cases, like in the example from last week about the evolution of dispersal, we can derive \\(\\lambda(z_m,z)\\) from a description of the model without doing the formal stability analysis outlined above.) In some simple cases we might be able to use the invasion criterium, \\(\\lambda(z_m,z)>1\\) , to determine what values of \\(z_m\\) (relative to \\(z\\) ) can invade. For example, in the evolution of dispersal example, we could prove that \\(\\lambda(z_m,z)>1\\) if and only if the mutant dispersal rate, \\(d_m=d(z_m)\\) , was between the resident dispersal rate \\(d = d(z)\\) and \\(1 - c d\\) (see plots below). import sympy import numpy as np # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants d1, c1 = 1/4, 1/3 d2, c2 = 9/10, 1/3 # Build plots p1 = sympy.plot( w.subs({'d': d1, 'c': c1}), (dm, 0, 1), xlabel=r'$d_m$', ylabel=r'$\\lambda(d_m,d)$', size=(9,4), show=False, ylim=(0.8, 1.2), label=f\"d = {d1}, c = {np.round(c1,2)}\", legend=True ) p2 = sympy.plot( w.subs({'d': d2, 'c': c2}), (dm, 0, 1), xlabel=r'$d_m$', ylabel=r'$\\lambda(d_m,d)$', show=False, ylim=(0.8, 1.2), label=f\"d = {d2}, c = {np.round(c2,2)}\", legend=True ) # Add scatter where intersection occurs solutions = sympy.solve(sympy.Eq(w, 1), dm) print(f\"Both plots intersect y=1 at {solutions}\") # Overlay p2 onto p1 and display plot p1.append(p2[0]) p1.show() In most cases, however, \\(\\lambda(z_m,z)\\) will be complex enough that this will not be possible, so we rely on a simple approximation. When the mutant trait value is very close to the resident trait value, we can use a first order Taylor series approximation around \\(z_m = z\\) \\[ \\begin{aligned} \\lambda(z_m,z) &\\approx \\lambda(z,z) + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z} (z_m-z)\\\\ &= 1 + \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z} (z_m-z) \\end{aligned} \\] This allows us to determine which direction evolution will proceed: \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}>0 \\implies\\) invasion, \\(\\lambda(z_m,z)>1\\) , when \\(z_m>z\\) \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}<0 \\implies\\) invasion, \\(\\lambda(z_m,z)>1\\) , when \\(z_{m} < z\\) We can go back and modify the plot we made above for the evolution of dispersal to see what way evolution will proceed. import sympy import numpy as np import matplotlib.pyplot as plt # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants d1, c1 = 1/4, 1/3 d2, c2 = 9/10, 1/3 # Compute the derivative at the point when resident and mutant trait values are the same s1 = float(sympy.diff(w, dm).subs({'d': d1, 'c': c1, 'dm': d1})) s2 = float(sympy.diff(w, dm).subs({'d': d2, 'c': c2, 'dm': d2})) # Let's make a pythonic function to make plotting a bit more expressive fw1 = sympy.lambdify(dm, w.subs({'d': d1, 'c': c1})) fw2 = sympy.lambdify(dm, w.subs({'d': d2, 'c': c2})) xdm = np.linspace(0, 1, 50) # Initialize plots fig, ax = plt.subplots(1,2, figsize=(9,3)) # Plot the curves ax[0].plot(xdm, fw1(xdm), label=f\"d = {d1}, c = {np.round(c1,2)}\") ax[1].plot(xdm, fw2(xdm), label=f\"d = {d2}, c = {np.round(c2,2)}\") # Add the tangent lines showing the slope ax[0].plot( np.linspace(d1-0.1, d1+0.1, 10), np.linspace(1-(s1*0.1), 1+(s1*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) ax[1].plot( np.linspace(d2-0.1, d2+0.1, 10), np.linspace(1-(s2*0.1), 1+(s2*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) # What way does evolution proceed? ax[0].arrow(x=d1-0.05, y=1.1, dx=np.sign((1 - c1*d1)-d1)*0.1, dy=0, width=.01, color='green') ax[1].arrow(x=d2-0.05, y=1.1, dx=np.sign((1 - c2*d2)-d2)*0.1, dy=0, width=.01, color='green') # Add intersect annotations ax[0].annotate(r'$d$', xy=(d1, 0.97)) ax[0].annotate(r'$1-cd$', xy=(1-c1*d1 - 0.05, 0.97)) ax[0].scatter(d1, 1) ax[0].scatter(1-c1*d1, 1) ax[1].annotate(r'$d$', xy=(d2, 0.97)) ax[1].annotate(r'$1-cd$', xy=(1-c2*d2 - 0.05, 0.97)) ax[1].scatter(d2, 1) ax[1].scatter(1-c2*d2, 1) # Modify title and range of axes and add legend for i in range(2): ax[i].axhline(1, linestyle='dotted', color='black') ax[i].set_xlabel(r'$d_m$') ax[i].set_ylabel(r'$\\lambda(d_m,d)$') ax[i].set_ylim(0.8, 1.2) ax[i].legend(frameon=False) # Display plot fig.tight_layout() plt.show() The direction of evolution by small steps is given by \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}\\) , the selection gradient . Potential evolutionary endpoints, also called evolutionarily singular strategies , are the resident trait values \\(z=\\hat{z}\\) where there is no directional selection \\[ \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=\\hat{z}, z=\\hat{z}} = 0 \\] In the evolution of dispersal example, we found \\(\\hat{d} = \\frac{1}{1+c}\\) import sympy import numpy as np import matplotlib.pyplot as plt # Initialize symbols dm, d, c = sympy.symbols('dm, d, c') # Specifiy evolution of dispersal model w = (1-dm) / ((1-dm) + d*(1-c)) + dm*(1-c)/((1-d) + d*(1-c)) # Set constants c3 = 1/3 d3 = 1/(1+c3) # Compute the derivative at the point when resident and mutant trait values are the same s3 = float(sympy.diff(w, dm).subs({'d': d3, 'c': c3, 'dm': d3})) # Let's make a pythonic function to make plotting a bit more expressive fw3 = sympy.lambdify(dm, w.subs({'d': d3, 'c': c3})) xdm = np.linspace(0, 1, 50) # Initialize plots fig, ax = plt.subplots(figsize=(4.5,3)) # Plot the curves ax.plot(xdm, fw3(xdm), label=f\"d = {d3}, c = {np.round(c3,2)}\") # Add the tangent lines showing the slope ax.plot( np.linspace(d3-0.1, d3+0.1, 10), np.linspace(1-(s3*0.1), 1+(s3*0.1), 10), color='red', label=r'$\\delta \\lambda / \\delta z_m}$' ) # What way does evolution proceed? ax.arrow(x=d3, y=1.1, dx=np.sign((1 - c3*d3)-d3)*0.1, dy=0, width=.01, color='green') # Add intersect annotations #ax.annotate(text=r'$d$', xy=(d3, 0.97)) ax.annotate(r'$d = 1-cd$', xy=(1-c3*d3 - 0.11, 0.95)) ax.scatter(d3, 1) ax.scatter(1-c3*d3, 1) # Modify title and range of axes and add legend ax.axhline(1, linestyle='dotted', color='black') ax.set_xlabel(r'$d_m$') ax.set_ylabel(r'$\\lambda(d_m,d)$') ax.set_ylim(0.8, 1.2) ax.legend(frameon=False) # Display plot fig.tight_layout() plt.show() An evolutionarily singular strategy, \\(\\hat{z}\\) , will only be an evolutionarily stable strategy (ESS), \\(z^{\\star}\\) , if it cannot be invaded. In the evolution of dispersal example, we could show that \\(\\hat{d} = \\frac{1}{1+c}\\) was globally stable because \\(\\lambda(d_m,d)\\vert_{d=\\hat{d}}<1\\) for all \\(d_m\\) . Global stability will be impossible to prove for more complex models, and so we often focus on local stability, which requires that \\(\\lambda(z_m,z)\\vert_{z=\\hat{z}}\\) is concave at \\(z_m=\\hat{z}\\) \\[ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg\\vert_{z_m=\\hat{z}, z=\\hat{z}} < 0 \\] To summarize, an evolutionarily stable strategy , \\(z^{\\star}\\) , satisfies both \\[ \\begin{aligned} \\frac{\\partial \\lambda}{\\partial z_m}\\bigg\\vert_{z_m=z^{\\star}, z=z^{\\star}} &= 0\\\\ \\frac{\\partial^2 \\lambda}{\\partial z_m^2}\\bigg\\vert_{z_m=z^{\\star}, z=z^{\\star}} &< 0 \\end{aligned} \\] i.e., \\(z^{\\star}\\) is a (local) fitness maximum. There is one more characteristic of evolutionarily singular strategies that we care about, and that is whether evolution actually leads to that strategy or not. For evolution to move the trait value towards a singular strategy, \\(\\hat{z}\\) , we need evolution to increase the trait value when it is less than \\(\\hat{z}\\) and decrease the trait value when it is greater than \\(\\hat{z}\\) . In other words, we need the selection gradient \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z}\\) to decrease as we move through \\(z=\\hat{z}\\) \\[ \\frac{\\mathrm{d}}{\\mathrm{d} z}\\left( \\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=z} \\right)_{z=\\hat{z}} < 0 \\] Singular strategies that satisfy this criteria are called convergence stable . Interestingly, not all evolutionarily stable strategies are convergence stable and not all convergence stable singular strategies are evolutionarily stable! Evolutionarily stable strategies that are not convergence stable are called Garden of Eden strategies . Singular strategies that are convergence stable but not evolutionarily stable are called evolutionary branching points . The latter are of particular interest because the system evolves towards a state where multiple strategies can coexist, leading to diversification. In tutorial we'll see an example of an evolutionary branching point.","title":"2. Analyzing evolutionary invasion"},{"location":"lectures/lecture-16/#3-summary-and-visualization-of-evolutionary-singulary-strategies","text":"There are four types of evolutionarily singular strategies \\(\\hat{z}\\) , where \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big\\vert_{z_m=\\hat{z},z=\\hat{z}}=0\\) A helpful way to visualize the two types of stability at an evolutionarily singular strategy is called a pairwise invasibility plot (PIP) from sympy import exp from matplotlib.colors import ListedColormap # Initialize symbols sm, s, r, va, vk, km, so, x = sympy.symbols('sm,s,r,va,vk,km,so,x') # Initialize model a = exp(-(sm-s)**2/va) k = km * exp(-(s-so)**2/vk) w = 1 + r - r * a * k.subs({'s': s}) / k.subs({'s': sm}) w0 = w.subs({'r':1.1, 'vk': 1, 'va': 1.5, 'so': 0}) #invasion fitness function # Solve for w0 == 1 sol = sympy.solve(sympy.Eq(w0, 1), s) fs = [sympy.lambdify(sm, j) for j in sol] # Make the sympy model into python function fw = sympy.lambdify((s, sm), w0) # Plotting parameters ymin, ymax, dz = -2, 2, 0.1 z = np.linspace(ymin, ymax, 200) fig, ax = plt.subplots() # Binary invasion mask invasion = np.array([[[fw(s, sm) for s in z] for sm in z]])[0] cmap = ListedColormap(['white', 'blue']) ax.contourf(z, z, invasion > 1, cmap=cmap) # Add plots at w0 == 1 for f in fs: ax.plot(f(z), z, color='black', linewidth=3) # Add scatter point over intersection of w0 == 1 solutions ax.scatter( sympy.solve(sympy.Eq(sol[1], sol[0])), sympy.solve(sympy.Eq(sol[0], sol[1])), color='red', s = 200, zorder=10 ) # Add labels and specify ranges ax.set_xlim(ymin, ymax) ax.set_ylim(ymin, ymax) ax.set_xlabel('resident strategy, $z$') ax.set_ylabel('mutant strategy, $z_m$') plt.show() Above, in the pairwise invasibility plot (PIP) , we colour in the mutant strategies that can invade a given resident strategy. This allows us to see which singular strategies are convergence stable. And whether a singular strategy is evolutionarily stable as well. Below, we summarize all of the evolutionary stable strategies in context with the corresponding PIP.","title":"3. Summary and visualization of evolutionary singulary strategies"},{"location":"lectures/lecture-17/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 17: Evolutionary invasion analysis - structured populations Run notes interactively? Lecture overview Background Analyzing evolutionary invasion in structured populations 1. Background In this lecture we're going to add a small complication to the evolutionary invasion analyses we've already seen, which will greatly increase the scope of questions we can address. Specifically, we'll now consider populations that are structured, causing the dynamics of both the resident and mutant to each be determined by multiple equations. For example, we may want to structure a population into juveniles and adults, so that the dynamics of residents and mutants are determined by \\[ \\begin{aligned} J(t+1) &= f(J(t), A(t), J_m(t), A_m(t))\\\\ A(t+1) &= g(J(t), A(t), J_m(t), A_m(t))\\\\ J_m(t+1) &= f_m(J(t), A(t), J_m(t), A_m(t))\\\\ A_m(t+1) &= g_m(J(t), A(t), J_m(t), A_m(t)) \\end{aligned} \\] Fortunately, we can essentially follow the same procedure as we did in the unstructured case... 2. Analyzing evolutionary invasion in structured populations First, we find an equilibrium with no mutants. Using the notation of our example, \\[ \\hat{J}>0, \\hat{A}>0, \\hat{J}_m = 0, \\hat{A}_m=0 \\] We then calculate the Jacobian and evaluate it at this resident equilibrium. This is no longer guaranteed to be an upper triangular matrix, but it can be written as an upper triangular block matrix \\[ \\mathbf{J}|_{\\hat{J}>0, \\hat{A}>0, \\hat{J}_m = 0, \\hat{A}_m=0} = \\begin{pmatrix} \\mathbf{J}_r & \\mathbf{V} \\\\ \\mathbf{0} & \\mathbf{J}_m \\end{pmatrix} \\] This means that the eigenvalues are determined by the eigenvalues of the submatrices along the diagonal, \\(\\mathbf{J}_r\\) and \\(\\mathbf{J}_m\\) . \\(\\mathbf{J}_r\\) is a submatrix describing the stability of the resident equilibrium to perturbations in the number of residents, and it's leading eigenvalue determines the stability of the resident equilibrium in the absence of the mutant. \\(\\mathbf{J}_m\\) is a submatrix describing the stability of the resident equilibrium to perturbations in the number of mutants, and it's leading eigenvalue is the mutant's invasion fitness . OK, so the leading eigenvalue of \\(\\mathbf{J}_m\\) gives the invasion fitness of a mutant, \\(\\lambda(z_m,z)\\) . In some cases it might be possible to write this leading eigenvalue in a simple form, so that we can work with it as we have in previous invasion analyses, e.g., differentiating with respect to the mutant trait value to get the selection gradient. But oftentimes the leading eigenvalue will be complicated, so here we introduce a little trick. The eigenvalues of \\(\\mathbf{J}_m\\) are determined by the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{J}_m - \\lambda(z_m,z) \\mathbf{I})=0\\) . So we can actually just differentiate this whole equation with respect to the mutant trait value, \\(z_m\\) , evaluate at \\(z_m=z\\) , and solve for the selection gradient, \\(\\partial \\lambda/\\partial z_m|_{z_m=z}\\) . For example, imagine \\(\\mathbf{J}_m\\) was a 2x2 matrix, as above, then \\[ \\begin{aligned} \\mathrm{Det}(\\mathbf{J}_m - \\lambda(z_m,z) \\mathbf{I}) &= 0\\\\ \\lambda(z_m,z)^2 + b(z_m,z) \\lambda(z_m,z) + c(z_m,z) &= 0\\\\ \\frac{\\partial}{\\partial z_m} \\left( \\lambda(z_m,z)^2 + b(z_m,z) \\lambda(z_m,z) + c(z_m,z) \\right)_{z_m=z} &= 0\\\\ 2 \\lambda(z,z) \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial b}{\\partial z_m}\\bigg|_{z_m=z} \\lambda(z,z) + b(z,z) \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial c}{\\partial z_m}\\bigg|_{z_m=z} &= 0\\\\ -\\left(\\frac{\\partial b}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial c}{\\partial z_m}\\bigg|_{z_m=z} \\right) / (2 + b(z,z)) &= \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z}\\\\ \\end{aligned} \\] Once we have the selection gradient, \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) , we can find the singular values (where the gradient is zero) and determine their evolutionary and convergence stability. We'll take a look at an example in tutorial where a population is genetically structured -- gametes have either an \\(A_1\\) or \\(A_2\\) allele at one locus and we look at the invasion of a mutant allele at another locus. In particular, we'll be looking at a particularly famous model for the evolution of dominance, where the mutant allele at the second locus modifies the dominance coefficient at the first locus. This is called a modifier model because the mutant does not directly influence fitness but changes the fitness of individuals that carry it by modifying some aspect of the model. Can natural selection explain why deleterious alleles are often recessive?","title":"Lecture 17"},{"location":"lectures/lecture-17/#lecture-17-evolutionary-invasion-analysis-structured-populations","text":"Run notes interactively?","title":"Lecture 17: Evolutionary invasion analysis - structured populations"},{"location":"lectures/lecture-17/#lecture-overview","text":"Background Analyzing evolutionary invasion in structured populations","title":"Lecture overview"},{"location":"lectures/lecture-17/#1-background","text":"In this lecture we're going to add a small complication to the evolutionary invasion analyses we've already seen, which will greatly increase the scope of questions we can address. Specifically, we'll now consider populations that are structured, causing the dynamics of both the resident and mutant to each be determined by multiple equations. For example, we may want to structure a population into juveniles and adults, so that the dynamics of residents and mutants are determined by \\[ \\begin{aligned} J(t+1) &= f(J(t), A(t), J_m(t), A_m(t))\\\\ A(t+1) &= g(J(t), A(t), J_m(t), A_m(t))\\\\ J_m(t+1) &= f_m(J(t), A(t), J_m(t), A_m(t))\\\\ A_m(t+1) &= g_m(J(t), A(t), J_m(t), A_m(t)) \\end{aligned} \\] Fortunately, we can essentially follow the same procedure as we did in the unstructured case...","title":"1. Background"},{"location":"lectures/lecture-17/#2-analyzing-evolutionary-invasion-in-structured-populations","text":"First, we find an equilibrium with no mutants. Using the notation of our example, \\[ \\hat{J}>0, \\hat{A}>0, \\hat{J}_m = 0, \\hat{A}_m=0 \\] We then calculate the Jacobian and evaluate it at this resident equilibrium. This is no longer guaranteed to be an upper triangular matrix, but it can be written as an upper triangular block matrix \\[ \\mathbf{J}|_{\\hat{J}>0, \\hat{A}>0, \\hat{J}_m = 0, \\hat{A}_m=0} = \\begin{pmatrix} \\mathbf{J}_r & \\mathbf{V} \\\\ \\mathbf{0} & \\mathbf{J}_m \\end{pmatrix} \\] This means that the eigenvalues are determined by the eigenvalues of the submatrices along the diagonal, \\(\\mathbf{J}_r\\) and \\(\\mathbf{J}_m\\) . \\(\\mathbf{J}_r\\) is a submatrix describing the stability of the resident equilibrium to perturbations in the number of residents, and it's leading eigenvalue determines the stability of the resident equilibrium in the absence of the mutant. \\(\\mathbf{J}_m\\) is a submatrix describing the stability of the resident equilibrium to perturbations in the number of mutants, and it's leading eigenvalue is the mutant's invasion fitness . OK, so the leading eigenvalue of \\(\\mathbf{J}_m\\) gives the invasion fitness of a mutant, \\(\\lambda(z_m,z)\\) . In some cases it might be possible to write this leading eigenvalue in a simple form, so that we can work with it as we have in previous invasion analyses, e.g., differentiating with respect to the mutant trait value to get the selection gradient. But oftentimes the leading eigenvalue will be complicated, so here we introduce a little trick. The eigenvalues of \\(\\mathbf{J}_m\\) are determined by the characteristic polynomial, \\(\\mathrm{Det}(\\mathbf{J}_m - \\lambda(z_m,z) \\mathbf{I})=0\\) . So we can actually just differentiate this whole equation with respect to the mutant trait value, \\(z_m\\) , evaluate at \\(z_m=z\\) , and solve for the selection gradient, \\(\\partial \\lambda/\\partial z_m|_{z_m=z}\\) . For example, imagine \\(\\mathbf{J}_m\\) was a 2x2 matrix, as above, then \\[ \\begin{aligned} \\mathrm{Det}(\\mathbf{J}_m - \\lambda(z_m,z) \\mathbf{I}) &= 0\\\\ \\lambda(z_m,z)^2 + b(z_m,z) \\lambda(z_m,z) + c(z_m,z) &= 0\\\\ \\frac{\\partial}{\\partial z_m} \\left( \\lambda(z_m,z)^2 + b(z_m,z) \\lambda(z_m,z) + c(z_m,z) \\right)_{z_m=z} &= 0\\\\ 2 \\lambda(z,z) \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial b}{\\partial z_m}\\bigg|_{z_m=z} \\lambda(z,z) + b(z,z) \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial c}{\\partial z_m}\\bigg|_{z_m=z} &= 0\\\\ -\\left(\\frac{\\partial b}{\\partial z_m}\\bigg|_{z_m=z} + \\frac{\\partial c}{\\partial z_m}\\bigg|_{z_m=z} \\right) / (2 + b(z,z)) &= \\frac{\\partial \\lambda}{\\partial z_m}\\bigg|_{z_m=z}\\\\ \\end{aligned} \\] Once we have the selection gradient, \\(\\frac{\\partial \\lambda}{\\partial z_m}\\Big|_{z_m=z}\\) , we can find the singular values (where the gradient is zero) and determine their evolutionary and convergence stability. We'll take a look at an example in tutorial where a population is genetically structured -- gametes have either an \\(A_1\\) or \\(A_2\\) allele at one locus and we look at the invasion of a mutant allele at another locus. In particular, we'll be looking at a particularly famous model for the evolution of dominance, where the mutant allele at the second locus modifies the dominance coefficient at the first locus. This is called a modifier model because the mutant does not directly influence fitness but changes the fitness of individuals that carry it by modifying some aspect of the model. Can natural selection explain why deleterious alleles are often recessive?","title":"2. Analyzing evolutionary invasion in structured populations"},{"location":"lectures/lecture-18/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 18: Introduction to probability Run notes interactively? Lecture overview What is probability? Useful terminology 1. What is probability? In the final lectures, we will outline basic probability theory and emphasize the importance of probabilistic modelling in biology. Up until now, we have studied only deterministic models, in which future states are entirely specified by the current state of the system. In the real world, however, chance plays a major role in the dynamics of a population. Lightning may strike an individual. A fire may decimate a population. Individuals may fail to reproduce or produce a bonanza crop of offspring. New beneficial mutations may, by happenstance, occur in individuals that leave no children. Drought and famine may occur, or rains and excess. Probabilistic models include chance events and outcomes and can lead to results that differ from purely deterministic models. In this lecture, we'll begin with some basic definitions and rules from probability theory. So then, what is probability? Imagine an event, or trial , that can have more than one outcome We can consider the outcome of the trial to be a random variable The chance of some particular outcome is its probability Frequency interpretation: \"Probabilities are understood as mathematically convenient approximations to long run relative frequencies.\" e.g., \\(\\approx50\\%\\) of individuals are male, so the probability a baby is male is \\(0.5\\) (for a deeper dive, see Edwards, 1984 ) Subjective interpretation: \"A probability statement expresses the opinion of some individual regarding how certain an event is to occur.\" e.g., \\(\\approx50\\%\\) of sperm should contain a Y chromosome, so the probability a baby is male is \\(0.5\\) (for a deeper dive, see Jaynes, 2002 ) 2. Useful terminology Complement Rule : The probability that \\(A\\) does not occur is equal to the probability that the complement of event \\(A\\) occurs, \\(P(A^c) = 1 - P(A)\\) . Inclusion-Exclusion Rule : The probability of either \\(A\\) or \\(B\\) (or both) occurring is \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) . Example: If the probability of having green eyes is 0.1, the probability of having brown hair is 0.75, and the probability of being a green-eyed brown haired person is 0.09, what is the probability of Not having green eyes? \\[ P(A^c) = 1 - P(A) = 1 - 0.1 = 0.9 \\] Having green eyes but not brown hair? \\[ P(A \\cap (A \\cap B^c)) = P(A) - P(A \\cap B) = 0.1 - 0.09 = 0.01 \\] Having green eyes and/or brown hair? \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.1 + 0.75 - 0.09 = 0.76 \\] Independence : If \\(A\\) and \\(B\\) are independent \\(P(A \\cap B) = P(A)P(B)\\) . Mutually exclusive : If \\(A\\) and \\(B\\) are mutually exclusive \\(P(A \\cap B) = 0\\) . Conditional Probability : The probability that \\(A\\) occurs given that \\(B\\) has occurred is written \\(P(A|B)\\) . In other words, among those cases where \\(B\\) has occurred, \\(P(A|B)\\) is the proportion of cases in which \\(A\\) occurs. Multiplication Rule : The probability of both \\(A\\) and \\(B\\) occurring is equal to the probability of \\(B\\) times the probability that \\(A\\) occurs given that \\(B\\) has: \\[ P(A\\cap B) = P(B) P(A|B) \\] Consequently, the conditional probability is given by \\[ P(A|B) = P(A\\cap B)/P(B) \\] Similarly, the probability of both \\(A\\) and \\(B\\) occurring occurs is equal to the probability of \\(A\\) times the probability that \\(B\\) occurs given that \\(A\\) has: \\[ \\begin{aligned} P(A\\cap B) &= P(A) P(B|A)\\\\ P(B|A) &= P(A\\cap B)/P(A) \\end{aligned} \\] Example (based on eye color problem above) What is the probability that you have brown hair if you have green eyes? \\[ P(B|A) = P(A\\cap B)/P(A) = 0.09/0.1 = 0.9 \\] What is the probability that you have green eyes if you have brown hair? \\[ P(A|B) = P(A\\cap B)/P(B) = 0.09/0.75 = 0.12 \\] Bayes' Rule : Equating \\(P(A\\cap B) = P(A|B)P(B)\\) and \\(P(A\\cap B) = P(B|A)P(A)\\) gives \\[ P(B|A) = \\frac{P(B) P(A|B)}{P(A)} \\] Example: The ability to taste phenylthiocarbamide (PTC) is thought to be determined by a single dominant gene with incomplete penetrance. Consider a population where there is a 70\\% chance of being able to taste PTC, \\(P(\\text{taster}) = 0.7\\) . If everybody who tastes PTC is a carrier, \\(P(\\text{carrier}|\\text{taster}) = 1\\) , and if 80\\% of the population carries the gene, \\(P(\\text{carrier}) = 0.8\\) , what is the penetrance of the gene? That is, what is the probability of tasting PTC if you are a carrier, \\(P(\\text{taster}|\\text{carrier})\\) ? \\[ \\begin{aligned} P(\\text{taster}|\\text{carrier}) &= \\frac{P(\\text{taster}) P(\\text{carrier}|\\text{taster})}{P(\\text{carrier})}\\\\ &= (0.7)(1) / (0.8)\\\\ &=7/8 \\approx 0.87 \\end{aligned} \\] Law of total probability : Suppose the outcomes, \\(B_i\\) , consist of \\(n\\) mutually exclusive events whose probabilities sum to one. Then the probability of \\(A\\) is the sum of the probabilities of \\(A\\) given each \\(B_i\\) , weighted by the probability of each \\(B_i\\) : \\[ P(A) = P(A|B_1) P(B_1) + P(A|B_2) P(B_2) + ... + P(A|B_n) P(B_n) \\] Example: What is the overall probability of dying of malaria in a region where the probability of dying of malaria is 0.15 in individuals that do not carry the sickle cell allele and 0.01 in carriers? Assume that the frequency of carriers is 0.25. \\[ \\begin{aligned} P(\\text{dying}) &= P(\\text{dying}|\\text{non-carrier}) P(\\text{non-carrier}) + P(\\text{dying}|\\text{carrier}) P(\\text{carrier})\\\\ &= 0.15(0.75) + 0.01(0.25)\\\\ &\\approx 0.11 \\end{aligned} \\] Law of total expectation : Suppose the outcomes, \\(B_i\\) , of a random variable consist of \\(n\\) mutually exclusive events whose probabilities sum to one. Then the \\textit{expectation} of another random variable, \\(X\\) , is the sum of the expectation of \\(X\\) given each \\(B_i\\) , weighted by the probability of each \\(B_i\\) : \\[ \\mathbb{E}[X] = \\mathbb{E}[X|B_1] P(B_1) + \\mathbb{E}[X|B_2] P(B_2) + ... + \\mathbb{E}[X|B_n] P(B_n) \\] Example: If the frequency of \\(AA\\) , \\(Aa\\) , and \\(aa\\) in a population are \\(p^2\\) , \\(2pq\\) , and \\(q^2\\) , and their fitnesses are \\(W_{AA}\\) , \\(W_{Aa}\\) , and \\(W_{aa}\\) , what is the expected fitness in the population? \\[ \\begin{aligned} \\mathbb{E}[W] &= \\mathbb{E}[W|AA] P(AA) + \\mathbb{E}[W|Aa] P(Aa) + \\mathbb{E}[W|aa] P(aa) \\\\ &= W_{AA}p^2 + W_{Aa}2pq + W_{aa} q^2 \\end{aligned} \\]","title":"Lecture 18"},{"location":"lectures/lecture-18/#lecture-18-introduction-to-probability","text":"Run notes interactively?","title":"Lecture 18: Introduction to probability"},{"location":"lectures/lecture-18/#lecture-overview","text":"What is probability? Useful terminology","title":"Lecture overview"},{"location":"lectures/lecture-18/#1-what-is-probability","text":"In the final lectures, we will outline basic probability theory and emphasize the importance of probabilistic modelling in biology. Up until now, we have studied only deterministic models, in which future states are entirely specified by the current state of the system. In the real world, however, chance plays a major role in the dynamics of a population. Lightning may strike an individual. A fire may decimate a population. Individuals may fail to reproduce or produce a bonanza crop of offspring. New beneficial mutations may, by happenstance, occur in individuals that leave no children. Drought and famine may occur, or rains and excess. Probabilistic models include chance events and outcomes and can lead to results that differ from purely deterministic models. In this lecture, we'll begin with some basic definitions and rules from probability theory. So then, what is probability? Imagine an event, or trial , that can have more than one outcome We can consider the outcome of the trial to be a random variable The chance of some particular outcome is its probability Frequency interpretation: \"Probabilities are understood as mathematically convenient approximations to long run relative frequencies.\" e.g., \\(\\approx50\\%\\) of individuals are male, so the probability a baby is male is \\(0.5\\) (for a deeper dive, see Edwards, 1984 ) Subjective interpretation: \"A probability statement expresses the opinion of some individual regarding how certain an event is to occur.\" e.g., \\(\\approx50\\%\\) of sperm should contain a Y chromosome, so the probability a baby is male is \\(0.5\\) (for a deeper dive, see Jaynes, 2002 )","title":"1. What is probability?"},{"location":"lectures/lecture-18/#2-useful-terminology","text":"Complement Rule : The probability that \\(A\\) does not occur is equal to the probability that the complement of event \\(A\\) occurs, \\(P(A^c) = 1 - P(A)\\) . Inclusion-Exclusion Rule : The probability of either \\(A\\) or \\(B\\) (or both) occurring is \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) . Example: If the probability of having green eyes is 0.1, the probability of having brown hair is 0.75, and the probability of being a green-eyed brown haired person is 0.09, what is the probability of Not having green eyes? \\[ P(A^c) = 1 - P(A) = 1 - 0.1 = 0.9 \\] Having green eyes but not brown hair? \\[ P(A \\cap (A \\cap B^c)) = P(A) - P(A \\cap B) = 0.1 - 0.09 = 0.01 \\] Having green eyes and/or brown hair? \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.1 + 0.75 - 0.09 = 0.76 \\] Independence : If \\(A\\) and \\(B\\) are independent \\(P(A \\cap B) = P(A)P(B)\\) . Mutually exclusive : If \\(A\\) and \\(B\\) are mutually exclusive \\(P(A \\cap B) = 0\\) . Conditional Probability : The probability that \\(A\\) occurs given that \\(B\\) has occurred is written \\(P(A|B)\\) . In other words, among those cases where \\(B\\) has occurred, \\(P(A|B)\\) is the proportion of cases in which \\(A\\) occurs. Multiplication Rule : The probability of both \\(A\\) and \\(B\\) occurring is equal to the probability of \\(B\\) times the probability that \\(A\\) occurs given that \\(B\\) has: \\[ P(A\\cap B) = P(B) P(A|B) \\] Consequently, the conditional probability is given by \\[ P(A|B) = P(A\\cap B)/P(B) \\] Similarly, the probability of both \\(A\\) and \\(B\\) occurring occurs is equal to the probability of \\(A\\) times the probability that \\(B\\) occurs given that \\(A\\) has: \\[ \\begin{aligned} P(A\\cap B) &= P(A) P(B|A)\\\\ P(B|A) &= P(A\\cap B)/P(A) \\end{aligned} \\] Example (based on eye color problem above) What is the probability that you have brown hair if you have green eyes? \\[ P(B|A) = P(A\\cap B)/P(A) = 0.09/0.1 = 0.9 \\] What is the probability that you have green eyes if you have brown hair? \\[ P(A|B) = P(A\\cap B)/P(B) = 0.09/0.75 = 0.12 \\] Bayes' Rule : Equating \\(P(A\\cap B) = P(A|B)P(B)\\) and \\(P(A\\cap B) = P(B|A)P(A)\\) gives \\[ P(B|A) = \\frac{P(B) P(A|B)}{P(A)} \\] Example: The ability to taste phenylthiocarbamide (PTC) is thought to be determined by a single dominant gene with incomplete penetrance. Consider a population where there is a 70\\% chance of being able to taste PTC, \\(P(\\text{taster}) = 0.7\\) . If everybody who tastes PTC is a carrier, \\(P(\\text{carrier}|\\text{taster}) = 1\\) , and if 80\\% of the population carries the gene, \\(P(\\text{carrier}) = 0.8\\) , what is the penetrance of the gene? That is, what is the probability of tasting PTC if you are a carrier, \\(P(\\text{taster}|\\text{carrier})\\) ? \\[ \\begin{aligned} P(\\text{taster}|\\text{carrier}) &= \\frac{P(\\text{taster}) P(\\text{carrier}|\\text{taster})}{P(\\text{carrier})}\\\\ &= (0.7)(1) / (0.8)\\\\ &=7/8 \\approx 0.87 \\end{aligned} \\] Law of total probability : Suppose the outcomes, \\(B_i\\) , consist of \\(n\\) mutually exclusive events whose probabilities sum to one. Then the probability of \\(A\\) is the sum of the probabilities of \\(A\\) given each \\(B_i\\) , weighted by the probability of each \\(B_i\\) : \\[ P(A) = P(A|B_1) P(B_1) + P(A|B_2) P(B_2) + ... + P(A|B_n) P(B_n) \\] Example: What is the overall probability of dying of malaria in a region where the probability of dying of malaria is 0.15 in individuals that do not carry the sickle cell allele and 0.01 in carriers? Assume that the frequency of carriers is 0.25. \\[ \\begin{aligned} P(\\text{dying}) &= P(\\text{dying}|\\text{non-carrier}) P(\\text{non-carrier}) + P(\\text{dying}|\\text{carrier}) P(\\text{carrier})\\\\ &= 0.15(0.75) + 0.01(0.25)\\\\ &\\approx 0.11 \\end{aligned} \\] Law of total expectation : Suppose the outcomes, \\(B_i\\) , of a random variable consist of \\(n\\) mutually exclusive events whose probabilities sum to one. Then the \\textit{expectation} of another random variable, \\(X\\) , is the sum of the expectation of \\(X\\) given each \\(B_i\\) , weighted by the probability of each \\(B_i\\) : \\[ \\mathbb{E}[X] = \\mathbb{E}[X|B_1] P(B_1) + \\mathbb{E}[X|B_2] P(B_2) + ... + \\mathbb{E}[X|B_n] P(B_n) \\] Example: If the frequency of \\(AA\\) , \\(Aa\\) , and \\(aa\\) in a population are \\(p^2\\) , \\(2pq\\) , and \\(q^2\\) , and their fitnesses are \\(W_{AA}\\) , \\(W_{Aa}\\) , and \\(W_{aa}\\) , what is the expected fitness in the population? \\[ \\begin{aligned} \\mathbb{E}[W] &= \\mathbb{E}[W|AA] P(AA) + \\mathbb{E}[W|Aa] P(Aa) + \\mathbb{E}[W|aa] P(aa) \\\\ &= W_{AA}p^2 + W_{Aa}2pq + W_{aa} q^2 \\end{aligned} \\]","title":"2. Useful terminology"},{"location":"lectures/lecture-19/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 19: Discrete probability distributions Run notes interactively? Lecture overview Discrete probability distributions Bernoulli distribution Binomial distribution Geometric distribution Negative binomial distribution Poisson distribution Summary 1. Discrete probability distributions In this section, we will introduce the concept of a random variable and examine its probability distribution, expectation, and variance. Random variables Consider an event or series of events with a variety of possible outcomes. We define a random variable that represents a particular outcome of these events. Random variables are generally denoted by capital letters, e.g., \\(X\\) , \\(Y\\) , \\(Z\\) . For instance, toss a coin twice and record the number of heads. The number of heads is a random variable and it may take on three values: \\(X=0\\) (no heads), \\(X=1\\) (one head), and \\(X=2\\) (two heads). First, we will consider random variables which have only a discrete set of possible outcomes, as in this example ( \\(X=0,1,2\\) ). In many cases of interest, one can specify the probability distribution that a random variable will follow. That is, one can give the probability that \\(X\\) takes on a particular value \\(x\\) , written as \\(P(X=x)\\) , for all possible values of \\(x\\) . E.g., for a fair coin tossed twice, \\(P(X=0)=1/4\\) , \\(P(X=1)=1/2\\) , \\(P(X=2)=1/4\\) . Note: Because \\(X\\) must take on some value, the sum of \\(P(X=x)\\) over all possible values of \\(x\\) must equal one: \\[ \\sum_{\\text{all }x} P(X=x) = 1 \\] Expectations The expectation or mean of a random variable \\(X\\) , written as \\(\\mathbb{E}(X)\\) is equal to the sum of the value of an outcome, \\(x\\) , weighted by the probability that the random variable will equal \\(x\\) : \\[ \\mathbb{E}(X) = \\sum_{\\text{all }x} x \\cdot P(X=x) \\] Notice that you can find the expectation of a random variable from its distribution, you don\u2019t actually need to perform the experiment. For example \\[ \\mathbb{E}(\\text{number of heads in two coin tosses}) \\] \\[ = 0*P(X=0) + 1*P(X=1) + 2*P(X=2) \\] \\[ = 0*1/4 + 1*1/2 + 2*1/4 = 1 \\] You expect to see one head on average in two coin tosses. \\(\\mathbb{E}(cX) = c \\mathbb{E}(X)\\) if \\(c\\) is a constant \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\) \"the expectation of a sum equals the sum of the expectations\" \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\mathbb{E}(Y)\\) only if \\(X\\) and \\(Y\\) are independent random variables. \\(\\mathbb{E}[g(X)] = \\sum_{\\text{all }x} g(x) P(X=x)\\) Variance Often, we want to know how dispersed the random variable is around its mean, \\(\\mathbb{E}[X]= \\mu\\) . One measure of dispersion is the variance of \\(X\\) , \\[ \\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu )^2] \\] Another common measure of dispersion is the standard deviation, \\(\\mathrm{SD}(X)\\) , which equals the square root of \\(\\mathrm{Var}(X)\\) . We can calculate the variance by finding the expectation of \\(g(X)=(X-\\mu )^2\\) . Alternatively, notice that \\(\\mathbb{E}[(X-\\mu )^2] = \\mathbb{E}[X^2 - 2X\\mu + \\mu^2]\\) . We simplify this using the above rules. First, because the expectation of a sum equals the sum of expectations: \\(\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[2X\\mu] + \\mathbb{E}[\\mu^2]\\) . Then, because constants may be taken out of an expectation: \\[ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - 2 \\mu \\mathbb{E}[X] + \\mu^2 \\mathbb{E}[1] = \\mathbb{E}[X^2] - 2\\mu^2 + \\mu^2 \\] \\[ = \\mathbb{E}[X^2] - \\mu^2 \\] Finally, notice that \\(\\mathbb{E}[X^2]\\) can be written as \\(\\mathbb{E}[g(X)]\\) where \\(g(X)=X^2\\) . From the final fact about expectations, we can calculate this: \\[ \\mathbb{E}[X^2] = \\sum_{\\text{all }x} x^2 P(X=x) \\] For example, for the coin toss, \\[ \\mathbb{E}[X^2] = (0)^2\\cdot P(X=0) + (1)^2\\cdot P(X=1) + (2)^2 \\cdot P(X=2) \\] \\[ = 0\\cdot 1/4 + 1\\cdot 1/2 + 4\\cdot 1/4 = 3/2 \\] From this we calculate the variance, \\[ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mu^2 = 3/2 - (1)^2 = 1/2 \\] Useful facts about variances: \\(\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)\\) if \\(c\\) is a constant. \\(\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\) only if \\(X\\) and \\(Y\\) are independent. There are several probability distributions that arise again and again. We'll look at a number of these distributions. It is worth knowing what type of distribution you expect under different circumstances, because if you expect a particular distribution you can determine the mean and variance that you expect to observe. 2. Bernoulli distribution If a single trial has two possible outcomes (which we'll call 0 and 1, sometimes called \"failure\" and \"success\"), and the probability of success is \\(p\\) , the probability distribution is given by a Bernoulli distribution \\[ P(X=k) = \\begin{cases} 1-p & k=0 \\\\ p & k=1 \\end{cases} \\] Expectation This has expectation \\[ \\mathbb{E}[X] = 0(1-p) + 1 p = p \\] Variance and variance \\[ \\mathrm{Var}(X) = 0^2(1-p) + 1^2p - \\mathbb{E}[X]^2 = p(1-p) \\] 3. Binomial distribution If we then imagine \\(n\\) \"Bernoulli trials\" the probability of getting \\(k\\) ones/successes in \\(n\\) trials is given by the binomial distribution . If \\(n=1\\) the probability distribution is the Bernoulli distribution. If \\(n=2\\) , the probability of getting two zeros is \\(P(X=0) = (1-p)^2\\) (getting a zero on the first trial and then independently getting a zero on the second trial), the probability of getting a one is \\(P(X=1) = p(1-p) + (1-p)p = 2 p (1-p)\\) , and the probability of getting two ones is \\(P(X=2) = p^2\\) . These are the only possibilities [ \\(P(X=0)+P(X=1)+P(X=2)=1\\) ]. For general \\(n\\) , we use the binomial distribution to determine the probability of \\(k\\) successs in \\(n\\) trials: \\[ P(X=k) = {n \\choose k} p^k (1-p)^{n-k} \\] where $$ {n \\choose k} = \\frac{n!}{k!(n-k)!} $$ is called \"n choose k\". \"n choose k\" is the number of ways that you can arrange \\(k\\) ones and \\(n-k\\) zeros in a row. For instance, if you wrote down, in a row, the results of \\(n\\) coin tosses, the number of different ways that you could write down all the outcomes and have exactly \\(k\\) heads is \"n choose k\". Example: If the probability of a head is \\(p\\) , what is the probability of having three heads in 5 tosses? \\[ P(X=3) = {5 \\choose 3} p^3 (1-p)^{2} \\] We can draw the probability distribution given specific values of \\(p\\) : import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(1,2, figsize=(9,3)) # Plot binomial distributions from k=0 to k=5 k = np.arange(0, 5) for i, pi in enumerate([0.5, 0.45]): ax[i].bar(k, stats.binom.pmf(k, 5, p=pi), color='darkred') ax[i].set_title(f\"p = {pi}, n = {5}\") ax[i].set_xlabel('k') Expectation The expectation of the binomial distribution is \\[ \\mathbb{E}(X)=np \\] Proof: The expectation of the sum of \\(X_i\\) equals the sum of the expected values of \\(X_i\\) . Since \\(\\mathbb{E}[X_i]\\) equals \\(p\\) (Bernoulli) the sum over \\(n\\) such trials equals \\(np\\) . Variance The variance of the binomial distribution is equal to \\[ \\mathrm{Var}(X)=np(1-p) \\] Proof: Because each trial is independent, the variance of the sum of \\(X_i\\) equals the sum of the variances of \\(X_i\\) . Since \\(Var(X_i)\\) equals \\((1-p)p\\) (Bernoulli), the sum of \\(n\\) variances will equal \\(np(1-p)\\) . 4. Geometric distribution The probability of having to observe \\(k\\) Bernoulli trials before the first success is given by the geometric distribution . The probability that the first success occurs on the first trial is \\(p\\) . The probability that the first success occurs on the second trial is \\((1-p)*p\\) , because the first trial had to have been a failure followed by a success. By generalizing this procedure, the probability that there will be \\(k-1\\) failures before the first success is: \\[ P(X=k) = (1-p)^{k-1}p \\] Expectation The geometric distribution has expectation \\[ \\mathbb{E}(X)= 1/p \\] Variance and variance \\[ \\mathrm{Var}(X)= (1-p)/p^2 \\] Example: If the probability of extinction of a population is estimated to be 0.1 every year, what is the expected time until extinction? \\[ \\mathbb{E}[X] = 1/p = 1/0.1 = 10 \\] Notice that the variance in this case is large relative to the mean, \\[ \\mathrm{Var}(X)= (1-p)/p^2 = 0.9/0.1^2=90 \\] This means that the actual year in which the population will go extinct is very hard to predict accurately. We can see this from the distribution: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, p_extinction = np.arange(0, 20), 0.1 ax.bar(k, stats.geom.pmf(k, p=p_extinction), color='darkred') ax.set_title(r\"Probability of extinction in year $k$\") ax.set_xlabel('Year (k)'); 5. Negative binomial distribution As an extension of the geometric distribution, how many Bernoulli trials will it take before \\(r\\) successes? The probability of the \\(r^{th}\\) success occurring on the \\(k^{th}\\) trial is given by the negative binomial distribution : \\[ P(X=k) = \\underbrace{{k-1 \\choose r-1} p^{r-1}(1-p)^{k-r}}_{\\substack{\\text{probability of r-1 successes in}\\\\\\text{the previous $k-1$ trials}}} \\cdot p \\] \\[ = {k-1 \\choose r-1} p^{r}(1-p)^{k-r} \\] Expectation A negative binomial distribution has expectation \\[ \\mathbb{E}[X]= r/p \\] Variance and variance \\[ \\mathrm{Var}(X)= r(1-p)/p^2 \\] Example: If a predator must capture \\(10\\) prey before growing enough to reproduce, what would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is \\(0.1\\) ? \\[ \\mathbb{E}[X]= r/p = 10/0.1 = 100 \\] Notice that again the variance in this case is quite high \\[ \\mathrm{Var}(X)= r(1-p)/p^2=10(0.9)/0.1^2=900 \\] and that the distribution looks quite skewed (=not symmetric). Some predators will reach reproductive age much sooner and some much later than the average: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, p_prey = np.arange(0, 240), 0.1 ax.bar(k, stats.nbinom.pmf(k, n=10, p=p_extinction), color='darkred') ax.set_title(r\"Probability of reaching reproductive capacity in day $k$\") ax.set_xlabel('Day (k)'); 6. Poisson distribution Imagine events occurring randomly and independently of one another (in space or time). If the rate at which events occur is \\(b\\) we expect \\(\\mu=b\\Delta t\\) events in time \\(\\Delta t\\) , and the probability of \\(k\\) events is given by the Poisson distribution \\[ P(X=k) = \\frac{e^{-\\mu} \\mu^k}{k!} \\] Expectation & Variance A Poisson distribution has the unique property that its variance equals its mean \\[ \\mathbb{E}[X]=\\mathrm{Var}(X)=\\mu \\] The Poisson distribution is also an approximation to the binomial distribution, with \\(\\mu=np\\) , when \\(n\\) is large and \\(p\\) is small. Example: If there are \\(3\\times 10^9\\) basepairs in the human genome and the mutation rate per generation per basepair is \\(10^{-9}\\) , what is the mean number of new mutations that a child will have? What is the variance in this number? What will the distribution look like? \\[ \\mathbb{E}[X]=\\mathrm{Var}(X)=\\mu=3\\times 10^9 \\cdot 10^{-9} = 3 \\] import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, rate = np.arange(0, 10), 10**-9 * (3*10**9) ax.bar(k, stats.poisson.pmf(k, rate), color='darkred') ax.set_title(r\"Probability of k new mutations ($\\mu = 3$)\") ax.set_xlabel('Number of mutations (k)'); 7. Summary distribution Discrete probability distributions satisfy \\[ \\sum_{\\text{all }x} P(X=x) = 1 \\] and have mean \\[ \\mathbb{E}(X) = \\sum_{\\text{all }x} x \\cdot P(X=x) \\] and variance \\[ \\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu )^2] \\] We've looked at the following examples: Bernoulli Binomial Geometric Negative Binomial Poisson","title":"Lecture 19"},{"location":"lectures/lecture-19/#lecture-19-discrete-probability-distributions","text":"Run notes interactively?","title":"Lecture 19: Discrete probability distributions"},{"location":"lectures/lecture-19/#lecture-overview","text":"Discrete probability distributions Bernoulli distribution Binomial distribution Geometric distribution Negative binomial distribution Poisson distribution Summary","title":"Lecture overview"},{"location":"lectures/lecture-19/#1-discrete-probability-distributions","text":"In this section, we will introduce the concept of a random variable and examine its probability distribution, expectation, and variance.","title":"1. Discrete probability distributions"},{"location":"lectures/lecture-19/#random-variables","text":"Consider an event or series of events with a variety of possible outcomes. We define a random variable that represents a particular outcome of these events. Random variables are generally denoted by capital letters, e.g., \\(X\\) , \\(Y\\) , \\(Z\\) . For instance, toss a coin twice and record the number of heads. The number of heads is a random variable and it may take on three values: \\(X=0\\) (no heads), \\(X=1\\) (one head), and \\(X=2\\) (two heads). First, we will consider random variables which have only a discrete set of possible outcomes, as in this example ( \\(X=0,1,2\\) ). In many cases of interest, one can specify the probability distribution that a random variable will follow. That is, one can give the probability that \\(X\\) takes on a particular value \\(x\\) , written as \\(P(X=x)\\) , for all possible values of \\(x\\) . E.g., for a fair coin tossed twice, \\(P(X=0)=1/4\\) , \\(P(X=1)=1/2\\) , \\(P(X=2)=1/4\\) . Note: Because \\(X\\) must take on some value, the sum of \\(P(X=x)\\) over all possible values of \\(x\\) must equal one: \\[ \\sum_{\\text{all }x} P(X=x) = 1 \\]","title":"Random variables"},{"location":"lectures/lecture-19/#expectations","text":"The expectation or mean of a random variable \\(X\\) , written as \\(\\mathbb{E}(X)\\) is equal to the sum of the value of an outcome, \\(x\\) , weighted by the probability that the random variable will equal \\(x\\) : \\[ \\mathbb{E}(X) = \\sum_{\\text{all }x} x \\cdot P(X=x) \\] Notice that you can find the expectation of a random variable from its distribution, you don\u2019t actually need to perform the experiment. For example \\[ \\mathbb{E}(\\text{number of heads in two coin tosses}) \\] \\[ = 0*P(X=0) + 1*P(X=1) + 2*P(X=2) \\] \\[ = 0*1/4 + 1*1/2 + 2*1/4 = 1 \\] You expect to see one head on average in two coin tosses. \\(\\mathbb{E}(cX) = c \\mathbb{E}(X)\\) if \\(c\\) is a constant \\(\\mathbb{E}(X+Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\) \"the expectation of a sum equals the sum of the expectations\" \\(\\mathbb{E}(XY) = \\mathbb{E}(X) \\mathbb{E}(Y)\\) only if \\(X\\) and \\(Y\\) are independent random variables. \\(\\mathbb{E}[g(X)] = \\sum_{\\text{all }x} g(x) P(X=x)\\)","title":"Expectations"},{"location":"lectures/lecture-19/#variance","text":"Often, we want to know how dispersed the random variable is around its mean, \\(\\mathbb{E}[X]= \\mu\\) . One measure of dispersion is the variance of \\(X\\) , \\[ \\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu )^2] \\] Another common measure of dispersion is the standard deviation, \\(\\mathrm{SD}(X)\\) , which equals the square root of \\(\\mathrm{Var}(X)\\) . We can calculate the variance by finding the expectation of \\(g(X)=(X-\\mu )^2\\) . Alternatively, notice that \\(\\mathbb{E}[(X-\\mu )^2] = \\mathbb{E}[X^2 - 2X\\mu + \\mu^2]\\) . We simplify this using the above rules. First, because the expectation of a sum equals the sum of expectations: \\(\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[2X\\mu] + \\mathbb{E}[\\mu^2]\\) . Then, because constants may be taken out of an expectation: \\[ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - 2 \\mu \\mathbb{E}[X] + \\mu^2 \\mathbb{E}[1] = \\mathbb{E}[X^2] - 2\\mu^2 + \\mu^2 \\] \\[ = \\mathbb{E}[X^2] - \\mu^2 \\] Finally, notice that \\(\\mathbb{E}[X^2]\\) can be written as \\(\\mathbb{E}[g(X)]\\) where \\(g(X)=X^2\\) . From the final fact about expectations, we can calculate this: \\[ \\mathbb{E}[X^2] = \\sum_{\\text{all }x} x^2 P(X=x) \\] For example, for the coin toss, \\[ \\mathbb{E}[X^2] = (0)^2\\cdot P(X=0) + (1)^2\\cdot P(X=1) + (2)^2 \\cdot P(X=2) \\] \\[ = 0\\cdot 1/4 + 1\\cdot 1/2 + 4\\cdot 1/4 = 3/2 \\] From this we calculate the variance, \\[ \\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mu^2 = 3/2 - (1)^2 = 1/2 \\] Useful facts about variances: \\(\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)\\) if \\(c\\) is a constant. \\(\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)\\) only if \\(X\\) and \\(Y\\) are independent. There are several probability distributions that arise again and again. We'll look at a number of these distributions. It is worth knowing what type of distribution you expect under different circumstances, because if you expect a particular distribution you can determine the mean and variance that you expect to observe.","title":"Variance"},{"location":"lectures/lecture-19/#2-bernoulli-distribution","text":"If a single trial has two possible outcomes (which we'll call 0 and 1, sometimes called \"failure\" and \"success\"), and the probability of success is \\(p\\) , the probability distribution is given by a Bernoulli distribution \\[ P(X=k) = \\begin{cases} 1-p & k=0 \\\\ p & k=1 \\end{cases} \\]","title":"2. Bernoulli distribution"},{"location":"lectures/lecture-19/#expectation","text":"This has expectation \\[ \\mathbb{E}[X] = 0(1-p) + 1 p = p \\]","title":"Expectation"},{"location":"lectures/lecture-19/#variance_1","text":"and variance \\[ \\mathrm{Var}(X) = 0^2(1-p) + 1^2p - \\mathbb{E}[X]^2 = p(1-p) \\]","title":"Variance"},{"location":"lectures/lecture-19/#3-binomial-distribution","text":"If we then imagine \\(n\\) \"Bernoulli trials\" the probability of getting \\(k\\) ones/successes in \\(n\\) trials is given by the binomial distribution . If \\(n=1\\) the probability distribution is the Bernoulli distribution. If \\(n=2\\) , the probability of getting two zeros is \\(P(X=0) = (1-p)^2\\) (getting a zero on the first trial and then independently getting a zero on the second trial), the probability of getting a one is \\(P(X=1) = p(1-p) + (1-p)p = 2 p (1-p)\\) , and the probability of getting two ones is \\(P(X=2) = p^2\\) . These are the only possibilities [ \\(P(X=0)+P(X=1)+P(X=2)=1\\) ]. For general \\(n\\) , we use the binomial distribution to determine the probability of \\(k\\) successs in \\(n\\) trials: \\[ P(X=k) = {n \\choose k} p^k (1-p)^{n-k} \\] where $$ {n \\choose k} = \\frac{n!}{k!(n-k)!} $$ is called \"n choose k\". \"n choose k\" is the number of ways that you can arrange \\(k\\) ones and \\(n-k\\) zeros in a row. For instance, if you wrote down, in a row, the results of \\(n\\) coin tosses, the number of different ways that you could write down all the outcomes and have exactly \\(k\\) heads is \"n choose k\". Example: If the probability of a head is \\(p\\) , what is the probability of having three heads in 5 tosses? \\[ P(X=3) = {5 \\choose 3} p^3 (1-p)^{2} \\] We can draw the probability distribution given specific values of \\(p\\) : import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(1,2, figsize=(9,3)) # Plot binomial distributions from k=0 to k=5 k = np.arange(0, 5) for i, pi in enumerate([0.5, 0.45]): ax[i].bar(k, stats.binom.pmf(k, 5, p=pi), color='darkred') ax[i].set_title(f\"p = {pi}, n = {5}\") ax[i].set_xlabel('k')","title":"3. Binomial distribution"},{"location":"lectures/lecture-19/#expectation_1","text":"The expectation of the binomial distribution is \\[ \\mathbb{E}(X)=np \\] Proof: The expectation of the sum of \\(X_i\\) equals the sum of the expected values of \\(X_i\\) . Since \\(\\mathbb{E}[X_i]\\) equals \\(p\\) (Bernoulli) the sum over \\(n\\) such trials equals \\(np\\) .","title":"Expectation"},{"location":"lectures/lecture-19/#variance_2","text":"The variance of the binomial distribution is equal to \\[ \\mathrm{Var}(X)=np(1-p) \\] Proof: Because each trial is independent, the variance of the sum of \\(X_i\\) equals the sum of the variances of \\(X_i\\) . Since \\(Var(X_i)\\) equals \\((1-p)p\\) (Bernoulli), the sum of \\(n\\) variances will equal \\(np(1-p)\\) .","title":"Variance"},{"location":"lectures/lecture-19/#4-geometric-distribution","text":"The probability of having to observe \\(k\\) Bernoulli trials before the first success is given by the geometric distribution . The probability that the first success occurs on the first trial is \\(p\\) . The probability that the first success occurs on the second trial is \\((1-p)*p\\) , because the first trial had to have been a failure followed by a success. By generalizing this procedure, the probability that there will be \\(k-1\\) failures before the first success is: \\[ P(X=k) = (1-p)^{k-1}p \\]","title":"4. Geometric distribution"},{"location":"lectures/lecture-19/#expectation_2","text":"The geometric distribution has expectation \\[ \\mathbb{E}(X)= 1/p \\]","title":"Expectation"},{"location":"lectures/lecture-19/#variance_3","text":"and variance \\[ \\mathrm{Var}(X)= (1-p)/p^2 \\] Example: If the probability of extinction of a population is estimated to be 0.1 every year, what is the expected time until extinction? \\[ \\mathbb{E}[X] = 1/p = 1/0.1 = 10 \\] Notice that the variance in this case is large relative to the mean, \\[ \\mathrm{Var}(X)= (1-p)/p^2 = 0.9/0.1^2=90 \\] This means that the actual year in which the population will go extinct is very hard to predict accurately. We can see this from the distribution: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, p_extinction = np.arange(0, 20), 0.1 ax.bar(k, stats.geom.pmf(k, p=p_extinction), color='darkred') ax.set_title(r\"Probability of extinction in year $k$\") ax.set_xlabel('Year (k)');","title":"Variance"},{"location":"lectures/lecture-19/#5-negative-binomial-distribution","text":"As an extension of the geometric distribution, how many Bernoulli trials will it take before \\(r\\) successes? The probability of the \\(r^{th}\\) success occurring on the \\(k^{th}\\) trial is given by the negative binomial distribution : \\[ P(X=k) = \\underbrace{{k-1 \\choose r-1} p^{r-1}(1-p)^{k-r}}_{\\substack{\\text{probability of r-1 successes in}\\\\\\text{the previous $k-1$ trials}}} \\cdot p \\] \\[ = {k-1 \\choose r-1} p^{r}(1-p)^{k-r} \\]","title":"5. Negative binomial distribution"},{"location":"lectures/lecture-19/#expectation_3","text":"A negative binomial distribution has expectation \\[ \\mathbb{E}[X]= r/p \\]","title":"Expectation"},{"location":"lectures/lecture-19/#variance_4","text":"and variance \\[ \\mathrm{Var}(X)= r(1-p)/p^2 \\] Example: If a predator must capture \\(10\\) prey before growing enough to reproduce, what would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is \\(0.1\\) ? \\[ \\mathbb{E}[X]= r/p = 10/0.1 = 100 \\] Notice that again the variance in this case is quite high \\[ \\mathrm{Var}(X)= r(1-p)/p^2=10(0.9)/0.1^2=900 \\] and that the distribution looks quite skewed (=not symmetric). Some predators will reach reproductive age much sooner and some much later than the average: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, p_prey = np.arange(0, 240), 0.1 ax.bar(k, stats.nbinom.pmf(k, n=10, p=p_extinction), color='darkred') ax.set_title(r\"Probability of reaching reproductive capacity in day $k$\") ax.set_xlabel('Day (k)');","title":"Variance"},{"location":"lectures/lecture-19/#6-poisson-distribution","text":"Imagine events occurring randomly and independently of one another (in space or time). If the rate at which events occur is \\(b\\) we expect \\(\\mu=b\\Delta t\\) events in time \\(\\Delta t\\) , and the probability of \\(k\\) events is given by the Poisson distribution \\[ P(X=k) = \\frac{e^{-\\mu} \\mu^k}{k!} \\]","title":"6. Poisson distribution"},{"location":"lectures/lecture-19/#expectation-variance","text":"A Poisson distribution has the unique property that its variance equals its mean \\[ \\mathbb{E}[X]=\\mathrm{Var}(X)=\\mu \\] The Poisson distribution is also an approximation to the binomial distribution, with \\(\\mu=np\\) , when \\(n\\) is large and \\(p\\) is small. Example: If there are \\(3\\times 10^9\\) basepairs in the human genome and the mutation rate per generation per basepair is \\(10^{-9}\\) , what is the mean number of new mutations that a child will have? What is the variance in this number? What will the distribution look like? \\[ \\mathbb{E}[X]=\\mathrm{Var}(X)=\\mu=3\\times 10^9 \\cdot 10^{-9} = 3 \\] import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(8,4)) # Plot binomial distributions from k=0 to k=5 k, rate = np.arange(0, 10), 10**-9 * (3*10**9) ax.bar(k, stats.poisson.pmf(k, rate), color='darkred') ax.set_title(r\"Probability of k new mutations ($\\mu = 3$)\") ax.set_xlabel('Number of mutations (k)');","title":"Expectation &amp; Variance"},{"location":"lectures/lecture-19/#7-summary-distribution","text":"Discrete probability distributions satisfy \\[ \\sum_{\\text{all }x} P(X=x) = 1 \\] and have mean \\[ \\mathbb{E}(X) = \\sum_{\\text{all }x} x \\cdot P(X=x) \\] and variance \\[ \\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu )^2] \\] We've looked at the following examples: Bernoulli Binomial Geometric Negative Binomial Poisson","title":"7. Summary distribution"},{"location":"lectures/lecture-20/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 20: Continuous probability distributions Run notes interactively? Lecture overview Continuous probability distributions Uniform distribution Normal distribution Exponential distribution Gamma distribution Summary 1. Continuous probability distributions The distributions discussed so far have only a discrete set of possible outcomes (e.g., \\(0,1,2, ...\\) ). Today, we'll discuss several common continuous distributions, whose outcomes lie along the real line. One interesting point about continuous probability distributions is that, because an infinite number of points lie on the real line, the probability of observing any particular point is effectively zero. Continuous distributions are described by probability density functions , \\(f(x)\\) , which give the probability that an observation falls near a point \\(x\\) : \\[ P(\\text{observation lies within } dx \\text{ of } x) = f(x)dx \\] One can, therefore, find the probability that a random variable \\(X\\) will fall between two values by integrating \\(f(x)\\) over the interval: \\[ P(a \\leq X \\leq b) = \\int_a^b f(x) dx \\] The total integral over the real line must equal one: \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] As in the discrete case, \\(\\mathbb{E}[X]\\) may be found by integrating the product of \\(x\\) and the probability density function over all possible values of \\(x\\) : \\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} xf(x) dx \\] The \\(Var(X)\\) equals \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) , where the expectation of \\(X^2\\) is given by: \\[ \\mathbb{E}[X^2] = \\int_{-\\infty}^{\\infty} x^2f(x) dx \\] 2. Uniform distribution This is the simplest of continuous distributions. The probability density function is \\[ f(x)=1/(b-a) \\] if \\(x\\) lies between \\(a\\) and \\(b\\) and \\(f(x)=0\\) otherwise Expectation The expected value of the uniform distribution equals: \\[ \\mathbb{E}[X] = \\int_a^b x \\cdot \\frac{1}{(b-a)} dx \\hspace{0.5em}=\\hspace{0.5em} \\frac{1}{(b-a)} \\int_a^b x \\cdot dx \\hspace{0.5em}=\\hspace{0.5em} \\frac{a+b}{2} \\] which is the midpoint between \\(a\\) and \\(b\\) Variance The variance of the uniform distribution equals: \\[ \\mathrm{Var}[X] = \\int_a^b x^2 \\cdot \\frac{1}{(b-a)} dx - \\left(\\frac{a+b}{2}\\right)^2 = \\frac{(b-a)^2}{12} \\] Example: Say that a particular chromosome is mapped out to be 2 morgans long. This chromosome is observed in several meiotic cells. Among those chromosomes containing a single cross-over, the mean position of the cross-over is 1 morgan from an end, but the variance is 1/2. Is this variance higher or lower than expected? What might cause such an observation? If we assume the location of the cross over is uniformly distributed along the chromosome, from \\(a=0\\) to \\(b=2\\) , then the mean should be \\[ \\mathbb{E}[X] = \\frac{a+b}{2} = 1 \\] as observed. The variance should be \\[ \\mathrm{Var}[X] = \\frac{(b-a)^2}{12} = \\frac{4}{12} = 1/3 \\] which is less than observed. This implies cross overs may not be uniformly distributed, as they are more dispersed around the mean than expected, perhaps due to recombination hot-spots (sequences that are more likely to get a double-strand break during meiosis). 3. Normal distribution This is the most familiar of continuous distributions. The probability density function of the normal distribution is given by: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance of the distribution. The mean, for example, may be found by integrating \\(x \\cdot f(x)\\) over all values of \\(x\\) . import numpy as np from scipy import stats import matplotlib.pyplot as plt # Normal distribution parameters mu = 0 var = 1 # Range of distribution z = np.linspace(-5, 5, 250) # Initialize plot fig, ax = plt.subplots(figsize=(8,4.5)) # Plot PDF ax.plot(z, stats.norm.pdf(z, mu, var), linewidth=4, color='black') # Add line for mean ax.axvline(mu, linestyle='dotted', color='black') stdev = np.sqrt(var) # Add standard deviations for i in [(1, 'green'), (2, 'red'), (3, 'blue')]: d, c = i ax.axvspan(mu + stdev*(d-1), mu + stdev*d, alpha=0.25, color=c) ax.axvspan(mu - stdev*d, - stdev*(d-1), alpha=0.25, color=c, label=str(d-1) + r'$\\sigma$ - ' + str(d) + r'$\\sigma$') # Add titltes and legend ax.set_xlabel('z') ax.set_title(f\"Normal distribution with $\\mu$ = {mu} and $\\sigma^2$ = {var}\") ax.legend(frameon=False) The normal distribution arises repeatedly in biology. Gauss and Laplace noticed that measurement errors tend to follow a normal distribution. Quetelet and Galton observed that the normal distribution fits data on the heights and weights of human and animal populations. This holds true for many other characters as well. Why does the normal distribution play such a ubiquitous role? The Central Limit Theorem : For independent and identically distributed random variables, their sum (or their average) tends towards a normal distribution as the number of events summed (or averaged) goes to infinity. For example, as the number of trials, \\(n\\) , increases in the binomial distribution, the number of successes approaches a normal distribution: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(1,3, figsize=(9,3)) # Plot binomial distributions k, p = np.arange(0, 100), 0.1 for i, n in enumerate([10, 50, 500]): ax[i].bar(k, stats.binom.pmf(k, n, p=p), color='darkred') ax[i].set_title(f\"p = {p}, n = {n}\") ax[i].set_xlabel('k') fig.tight_layout() In particular, we expect that if several genes contribute to a trait, trait values should be normally distributed. The random variable being summed or averaged is the contribution of each gene to the trait. 4. Exponential distribution If events occur randomly over time at a rate \\(\\lambda\\) , then the time until the first event occurs has an exponential distribution: \\[ f(x) = \\lambda e^{-\\lambda x} \\] for \\(x\\geq0\\) and \\(f(x)=0\\) otherwise. This is the equivalent of the geometric distribution for events that occur continuously over time, rather than at discrete intervals. Expectation Integrating \\(x \\cdot f(x)\\) from \\(0\\) to \\(\\infty\\) , gives \\[ \\mathbb{E}[X] = 1/\\lambda \\] Variance Similarly, the variance of the exponential distribution can be shown to be \\[ \\mathrm{Var}[X]=1/\\lambda^2 \\] For example, let \\(\\lambda\\) equal the instantaneous death rate of an individual. The lifespan of the individual would be described by an exponential distribution import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(6,4)) # Plot binomial distributions from k=0 to k=5 x = np.arange(0, 50) for l in [0.1, 0.05, 0.01]: ax.plot(x, l * np.exp(-l * x), label=f\"$\\lambda$ = {l}/year\") ax.set_xlabel('Year') ax.legend(frameon=False) 5. Gamma distribution As the negative binomial generalizes the geometric distribution, the gamma distribution generalizes the exponential distribution. It describes the waiting time until the \\(r^{th}\\) event for a process that occurs randomly over time at a rate \\(\\lambda\\) : \\[ f(x) = \\underbrace{\\frac{e^{-\\lambda x}(\\lambda x)^{r-1}}{(r-1)!}}_{\\substack{\\text{probability of $r-1$ \"successes\"}\\\\\\text{in $x$ time (=Poisson)}}}\\cdot \\underbrace{\\lambda}_{\\text{+1 more \"success\"}} \\] The gamma distribution is often generalized to include cases where \\(r\\) is not an integer \\[ f(x) = \\frac{e^{-\\lambda x}(\\lambda x)^{r-1}}{\\Gamma(r)}\\lambda \\] where \\(\\Gamma(r)\\) is the \"gamma function\" Expectation The mean of the gamma distribution is \\[ \\mathbb{E}[X] = r/\\lambda \\] Variance and the variance is \\[ \\mathrm{Var}[X] = r/\\lambda^2 \\] Example: If you are studying lion hunts and observe 1 successful hunt every second day on average, how long will it take for you to collect \\(100\\) data points? If your committee asks you to give an upper 95\\% confidence limit on how long it would take for you to complete your data collection, how might you answer them? The number of days until \\(r=100\\) successful hunts, at rate of \\(\\lambda=0.5\\) , is given by a gamma distribution import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(6,4)) # Plot binomial distributions days, r = np.arange(0, 350), 0.5 ax.plot(days, stats.gamma.pdf(days, 100, 1/r), color='black') ax.set_xlabel('k') ax.set_xlabel('Days') This is nearly normal (by the central limit theorem), with mean \\(r/\\lambda=200\\) and variance \\(r/\\lambda^2=400\\) . By using a statistical table, you figure out that the upper 95\\% confidence limit is \\(1.65\\) standard deviations above the mean. In this case, your estimate would be \\(200 + 1.65*\\sqrt{400} = 233\\) days. No problem! 6. Summary Today we looked at continuous probability distributions, which satisfy \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] and have mean \\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} xf(x) dx \\] and variance \\[ \\mathrm{Var}[X] = \\mathbb{E}[X^2] - E[X]^2 = \\int_{-\\infty}^{\\infty} x^2f(x) dx - E[X]^2 \\] We mentioned the \\textbf{central limit theorem} And we've defined a few common distributions uniform normal exponential gamma","title":"Lecture 20"},{"location":"lectures/lecture-20/#lecture-20-continuous-probability-distributions","text":"Run notes interactively?","title":"Lecture 20: Continuous probability distributions"},{"location":"lectures/lecture-20/#lecture-overview","text":"Continuous probability distributions Uniform distribution Normal distribution Exponential distribution Gamma distribution Summary","title":"Lecture overview"},{"location":"lectures/lecture-20/#1-continuous-probability-distributions","text":"The distributions discussed so far have only a discrete set of possible outcomes (e.g., \\(0,1,2, ...\\) ). Today, we'll discuss several common continuous distributions, whose outcomes lie along the real line. One interesting point about continuous probability distributions is that, because an infinite number of points lie on the real line, the probability of observing any particular point is effectively zero. Continuous distributions are described by probability density functions , \\(f(x)\\) , which give the probability that an observation falls near a point \\(x\\) : \\[ P(\\text{observation lies within } dx \\text{ of } x) = f(x)dx \\] One can, therefore, find the probability that a random variable \\(X\\) will fall between two values by integrating \\(f(x)\\) over the interval: \\[ P(a \\leq X \\leq b) = \\int_a^b f(x) dx \\] The total integral over the real line must equal one: \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] As in the discrete case, \\(\\mathbb{E}[X]\\) may be found by integrating the product of \\(x\\) and the probability density function over all possible values of \\(x\\) : \\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} xf(x) dx \\] The \\(Var(X)\\) equals \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) , where the expectation of \\(X^2\\) is given by: \\[ \\mathbb{E}[X^2] = \\int_{-\\infty}^{\\infty} x^2f(x) dx \\]","title":"1. Continuous probability distributions"},{"location":"lectures/lecture-20/#2-uniform-distribution","text":"This is the simplest of continuous distributions. The probability density function is \\[ f(x)=1/(b-a) \\] if \\(x\\) lies between \\(a\\) and \\(b\\) and \\(f(x)=0\\) otherwise","title":"2. Uniform distribution"},{"location":"lectures/lecture-20/#expectation","text":"The expected value of the uniform distribution equals: \\[ \\mathbb{E}[X] = \\int_a^b x \\cdot \\frac{1}{(b-a)} dx \\hspace{0.5em}=\\hspace{0.5em} \\frac{1}{(b-a)} \\int_a^b x \\cdot dx \\hspace{0.5em}=\\hspace{0.5em} \\frac{a+b}{2} \\] which is the midpoint between \\(a\\) and \\(b\\)","title":"Expectation"},{"location":"lectures/lecture-20/#variance","text":"The variance of the uniform distribution equals: \\[ \\mathrm{Var}[X] = \\int_a^b x^2 \\cdot \\frac{1}{(b-a)} dx - \\left(\\frac{a+b}{2}\\right)^2 = \\frac{(b-a)^2}{12} \\] Example: Say that a particular chromosome is mapped out to be 2 morgans long. This chromosome is observed in several meiotic cells. Among those chromosomes containing a single cross-over, the mean position of the cross-over is 1 morgan from an end, but the variance is 1/2. Is this variance higher or lower than expected? What might cause such an observation? If we assume the location of the cross over is uniformly distributed along the chromosome, from \\(a=0\\) to \\(b=2\\) , then the mean should be \\[ \\mathbb{E}[X] = \\frac{a+b}{2} = 1 \\] as observed. The variance should be \\[ \\mathrm{Var}[X] = \\frac{(b-a)^2}{12} = \\frac{4}{12} = 1/3 \\] which is less than observed. This implies cross overs may not be uniformly distributed, as they are more dispersed around the mean than expected, perhaps due to recombination hot-spots (sequences that are more likely to get a double-strand break during meiosis).","title":"Variance"},{"location":"lectures/lecture-20/#3-normal-distribution","text":"This is the most familiar of continuous distributions. The probability density function of the normal distribution is given by: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance of the distribution. The mean, for example, may be found by integrating \\(x \\cdot f(x)\\) over all values of \\(x\\) . import numpy as np from scipy import stats import matplotlib.pyplot as plt # Normal distribution parameters mu = 0 var = 1 # Range of distribution z = np.linspace(-5, 5, 250) # Initialize plot fig, ax = plt.subplots(figsize=(8,4.5)) # Plot PDF ax.plot(z, stats.norm.pdf(z, mu, var), linewidth=4, color='black') # Add line for mean ax.axvline(mu, linestyle='dotted', color='black') stdev = np.sqrt(var) # Add standard deviations for i in [(1, 'green'), (2, 'red'), (3, 'blue')]: d, c = i ax.axvspan(mu + stdev*(d-1), mu + stdev*d, alpha=0.25, color=c) ax.axvspan(mu - stdev*d, - stdev*(d-1), alpha=0.25, color=c, label=str(d-1) + r'$\\sigma$ - ' + str(d) + r'$\\sigma$') # Add titltes and legend ax.set_xlabel('z') ax.set_title(f\"Normal distribution with $\\mu$ = {mu} and $\\sigma^2$ = {var}\") ax.legend(frameon=False) The normal distribution arises repeatedly in biology. Gauss and Laplace noticed that measurement errors tend to follow a normal distribution. Quetelet and Galton observed that the normal distribution fits data on the heights and weights of human and animal populations. This holds true for many other characters as well. Why does the normal distribution play such a ubiquitous role? The Central Limit Theorem : For independent and identically distributed random variables, their sum (or their average) tends towards a normal distribution as the number of events summed (or averaged) goes to infinity. For example, as the number of trials, \\(n\\) , increases in the binomial distribution, the number of successes approaches a normal distribution: import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(1,3, figsize=(9,3)) # Plot binomial distributions k, p = np.arange(0, 100), 0.1 for i, n in enumerate([10, 50, 500]): ax[i].bar(k, stats.binom.pmf(k, n, p=p), color='darkred') ax[i].set_title(f\"p = {p}, n = {n}\") ax[i].set_xlabel('k') fig.tight_layout() In particular, we expect that if several genes contribute to a trait, trait values should be normally distributed. The random variable being summed or averaged is the contribution of each gene to the trait.","title":"3. Normal distribution"},{"location":"lectures/lecture-20/#4-exponential-distribution","text":"If events occur randomly over time at a rate \\(\\lambda\\) , then the time until the first event occurs has an exponential distribution: \\[ f(x) = \\lambda e^{-\\lambda x} \\] for \\(x\\geq0\\) and \\(f(x)=0\\) otherwise. This is the equivalent of the geometric distribution for events that occur continuously over time, rather than at discrete intervals.","title":"4. Exponential distribution"},{"location":"lectures/lecture-20/#expectation_1","text":"Integrating \\(x \\cdot f(x)\\) from \\(0\\) to \\(\\infty\\) , gives \\[ \\mathbb{E}[X] = 1/\\lambda \\]","title":"Expectation"},{"location":"lectures/lecture-20/#variance_1","text":"Similarly, the variance of the exponential distribution can be shown to be \\[ \\mathrm{Var}[X]=1/\\lambda^2 \\] For example, let \\(\\lambda\\) equal the instantaneous death rate of an individual. The lifespan of the individual would be described by an exponential distribution import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(6,4)) # Plot binomial distributions from k=0 to k=5 x = np.arange(0, 50) for l in [0.1, 0.05, 0.01]: ax.plot(x, l * np.exp(-l * x), label=f\"$\\lambda$ = {l}/year\") ax.set_xlabel('Year') ax.legend(frameon=False)","title":"Variance"},{"location":"lectures/lecture-20/#5-gamma-distribution","text":"As the negative binomial generalizes the geometric distribution, the gamma distribution generalizes the exponential distribution. It describes the waiting time until the \\(r^{th}\\) event for a process that occurs randomly over time at a rate \\(\\lambda\\) : \\[ f(x) = \\underbrace{\\frac{e^{-\\lambda x}(\\lambda x)^{r-1}}{(r-1)!}}_{\\substack{\\text{probability of $r-1$ \"successes\"}\\\\\\text{in $x$ time (=Poisson)}}}\\cdot \\underbrace{\\lambda}_{\\text{+1 more \"success\"}} \\] The gamma distribution is often generalized to include cases where \\(r\\) is not an integer \\[ f(x) = \\frac{e^{-\\lambda x}(\\lambda x)^{r-1}}{\\Gamma(r)}\\lambda \\] where \\(\\Gamma(r)\\) is the \"gamma function\"","title":"5. Gamma distribution"},{"location":"lectures/lecture-20/#expectation_2","text":"The mean of the gamma distribution is \\[ \\mathbb{E}[X] = r/\\lambda \\]","title":"Expectation"},{"location":"lectures/lecture-20/#variance_2","text":"and the variance is \\[ \\mathrm{Var}[X] = r/\\lambda^2 \\] Example: If you are studying lion hunts and observe 1 successful hunt every second day on average, how long will it take for you to collect \\(100\\) data points? If your committee asks you to give an upper 95\\% confidence limit on how long it would take for you to complete your data collection, how might you answer them? The number of days until \\(r=100\\) successful hunts, at rate of \\(\\lambda=0.5\\) , is given by a gamma distribution import numpy as np from scipy import stats import matplotlib.pyplot as plt # Initialize plots fig, ax = plt.subplots(figsize=(6,4)) # Plot binomial distributions days, r = np.arange(0, 350), 0.5 ax.plot(days, stats.gamma.pdf(days, 100, 1/r), color='black') ax.set_xlabel('k') ax.set_xlabel('Days') This is nearly normal (by the central limit theorem), with mean \\(r/\\lambda=200\\) and variance \\(r/\\lambda^2=400\\) . By using a statistical table, you figure out that the upper 95\\% confidence limit is \\(1.65\\) standard deviations above the mean. In this case, your estimate would be \\(200 + 1.65*\\sqrt{400} = 233\\) days. No problem!","title":"Variance"},{"location":"lectures/lecture-20/#6-summary","text":"Today we looked at continuous probability distributions, which satisfy \\[ \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\] and have mean \\[ \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} xf(x) dx \\] and variance \\[ \\mathrm{Var}[X] = \\mathbb{E}[X^2] - E[X]^2 = \\int_{-\\infty}^{\\infty} x^2f(x) dx - E[X]^2 \\] We mentioned the \\textbf{central limit theorem} And we've defined a few common distributions uniform normal exponential gamma","title":"6. Summary"},{"location":"lectures/lecture-4/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 4: One-locus selection Run notes interactively? Lecture overview Review of Hardy-Weinberg One-locus haploid selection in discrete time One-locus diploid selection in discrete time One-locus haploid selection in continuous time Comparing the haploid selection models in discrete and continuous time The models we've covered 1. Review of Hardy-Weinberg After one generation of random mating in a single large population with no selection and no mutation, diploid genotype frequencies at a locus with two alleles become: \\(Freq(AA) = x = p^2\\) \\(Freq(Aa) = y = 2pq\\) \\(Freq(aa) = z = q^2\\) \\(\\rightarrow\\) Hardy-Weinberg equilibrium . Furthermore, \\(p' = p\\) . \\(\\rightarrow\\) Allele frequencies do not change over time. Question : What if the genotypes vary in fitness? 2. One-locus haploid selection in discrete time We begin by examining selection in a model with haploid selection. Life-cycle diagram Figure. Life-cycle diagram of haploid selection We will census at the beginning of the haploid phase (immediately after meiosis). Derivation Let there be two haploid genotypes ( \\(A\\) and \\(a\\) ) with \\(N_A =\\) number of \\(A\\) individuals \\(N_a =\\) number of \\(a\\) individuals We assume that these numbers are very large. What is the frequency of \\(A\\) , \\(p(t)\\) , at this point? \\[ p(t) = \\frac{N_a(t)}{N_A(t) + N_a(t)} \\] Now, let\u2019s assume that not all haploid individuals survive to reproduce: \\(v_A =\\) probability of survival (or viability) of an \\(A\\) individual \\(v_a =\\) probability of survival (or viability) of an \\(a\\) individual After these deaths have occurred (but before gametes unite): How many \\(A\\) individuals do we expect to still be alive? \\[ v_AN_A(t) \\] How many a individuals do we expect to still be alive? \\[ v_aN_a(t) \\] What is the frequency of A among these survivors? \\[ \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] Because gametes unite at random and undergo meiosis to produce the next generation of haploids, and neither of these processes change allele frequencies, the frequency of A at the next census is: \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] We initially showed that the frequency of A in the current generation is \\[ p(t) = \\frac{N_A(t)}{N_A(t) + N_a(t)} \\] And we just showed that the frequency of A in the next generation is \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] but... this isn\u2019t quite a recursion equation. Why? (Hint: what's the difference between \\(p\\) and \\(N\\) ?) \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} = \\frac{v_A\\frac{N_A(t)}{N_A(t) + N_a(t)}}{v_A\\frac{N_A(t)}{N_A(t) + N_a(t)} + v_a\\frac{N_a(t)}{N_A(t) + N_a(t)}} = \\frac{v_Ap(t)}{v_Ap(t) + v_a(1-p(t))} \\] This is a recursion equation giving the allele frequency over time in our model of viability selection during the haploid phase. The denominator in this equation is the mean absolute fitness of the population, \\(\\bar v(t)\\) , which will determine the dynamics of the total population size \\[ N(t+1) \\equiv N_A(t+1) + N_a(t+1) = N(t)\\bar v(t) \\] Simplifying our equation of one-locus haploid selection Our current recursion is a function of two parameters, the absolute viabilities, \\(v_A\\) and \\(v_a\\) \\[ p(t + 1) = \\frac{v_Ap(t)}{v_Ap(t) + v_a(1 \u2212 p(t))} \\] Now notice that if we divide both the numerator and denominator by one of these viabilities, say \\(v_a\\) \\[ p(t + 1) = \\frac{(v_A/v_a)p(t)}{(v_A/v_a)p(t) + (v_a/v_a)(1 \u2212 p(t))} \\] We can reduce the recursion to a function of only one parameter, \\(V_A = v_A/v_a\\) , the relative viability of \\(A\\) \\[ p(t + 1) = \\frac{V_Ap(t)}{V_Ap(t) + 1 - p(t)} \\] The denominator in this equation is the mean relative fitness of the population. This simpler equation shows that the dynamics of the total number of individuals does not influence evolution in this model. All we need to predict evolution are the relative fitnesses. 3. One-locus diploid selection in discrete time Since we are all currently in the diploid phase of our life-cycle, it is natural to ask: Does selection in the diploid phase work the same way? Life-cycle diagram Figure. Life-cycle diagram of diploid selection We will census at the beginning of the diploid phase (immediately after gamete union). Derivation Let there be three genotypes \\(AA\\) , \\(Aa\\) , and \\(aa\\) with \\(N_{AA}(t) =\\) number of \\(AA\\) individuals \\(N_{Aa}(t) =\\) number of \\(Aa\\) individuals \\(N_{aa}(t) =\\) number of \\(aa\\) individuals and we assume these to be very large. What is the frequency of \\(A\\) , \\(p(t)\\) , at this point? \\[ p(t) = \\frac{2 * N_{AA}(t) + N_{Aa}(t)}{2 * N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)} = \\frac{N_{AA}(t) + \\frac{1}{2}N_{Aa}(t)}{N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)} \\] Now let's assume that not all diploid individuals survive to reproduce: \\(V_{AA} =\\) probability of survival (or viability) of an \\(AA\\) individual \\(V_{Aa} =\\) probability of survival (or viability) of an \\(Aa\\) individual \\(V_{aa} =\\) probability of survival (or viability) of an \\(aa\\) individual After these deaths (and before meiosis): How many AA individuals will still be alive, on average? \\[ v_{AA}N_{AA}(t) \\] How many Aa individuals will still be alive, on average? \\[ v_{Aa}N_{Aa}(t) \\] How many aa individuals will still be alive, on average? \\[ v_{aa}N_{aa}(t) \\] What is the frequency of the \\(A\\) allele among adults? \\[ \\frac{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t)}{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t) + v_{aa}N_{aa}(t)} \\] And since meiosis and gamete union will not change this frequency, the frequency of the A allele in the next generation is \\[ p(t+1) = \\frac{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t)}{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t) + v_{aa}N_{aa}(t)} \\] With random union of gametes in a gamete pool or random mating of individuals, the diploid offspring are in Hardy-Weinberg proportions: \\(p^2(t)\\) \u2236 \\(2p(t)q(t)\\) \u2236 \\(q^2(t)\\) , where \\(p(t)\\) is the frequency of the A allele among the mating individuals at time t. Specifically, \\(N_{AA}(t) = p^2(t)N(t)\\) \\(N_{Aa}(t) = 2p(t)q(t)N(t)\\) \\(N_{aa}(t) = q^2(t)N(t)\\) where \\(N(t) \\equiv N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)\\) is the total population size. Substituting these in we can rewrite the above equation as \\[ p(t+1) = \\frac{v_{AA}p(t)^2 + v_{Aa}p(t)q(t)}{v_{AA}p(t)^2 + 2v_{Aa}p(t)q(t) + v_{aa}q(t)^2} \\] This is a recursion equation giving the allele frequency over time in the model with viability selection during the diploid phase. Simplifying our equation of one-locus diploid selection As in the haploid selection case, we can rewrite our recursion with absolute viabilities in terms of relative viabilities by dividing the numerator and denominator by some constant, say \\(v_{aa}\\) , \\[ p(t+1) = \\frac{(v_{AA}/v_{aa})p(t)^2 + (v_{Aa}/v_{aa})p(t)q(t)}{(v_{AA}/v_{aa})p(t)^2 + 2(v_{Aa}/v_{aa})p(t)q(t) + (v_{aa}/v_{aa})q(t)^2} \\] \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t)q(t) + q(t)^2} \\] which is a function of only two parameters, WAA = vAA/vaa and WAa = vAa/vaa, and shows that evolution once again does not depend on the dynamics of total population size (just relative fitnesses). Comparing the one-locus haploid and diploid discrete-time models So, returning to our original question, how does evolution under diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t)q(t) + q(t)^2} \\] compare to evolution under haploid selection \\[ p(t+1) = \\frac{v_Ap(t)}{v_ap(t) + v_aq(t)} \\] To compare these models, let\u2019s assume the viability of a diploid genotype is the product of the haploid viabilities, ie, \\(v_{AA} = v_Av_A\\) , \\(v_{Aa} = v_Av_a\\) , and \\(v_{aa} = v_av_a\\) . Then our diploid recursion reduces to \\[ p(t+1) = \\frac{v_{A}v_{A}p(t)^2 + v_{A}v_{a}p(t)q(t)}{v_{A}v_{A}p(t)^2 + 2v_{A}v_{a}p(t)q(t) + v_{a}v_{a}q(t)^2} \\] \\[ p(t+1) = \\frac{v_{A}p(t)(p(t) + v_{a}q(t))}{v_{A}p(t)(v_Ap(t) + v_{a}q(t)) + v_{a}q(t)(v_Ap(t) + v_aq(t))} \\] \\[ p(t+1) = \\frac{v_Ap(t)}{v_Ap(t) + v_aq(t)} \\] Thus we need twice as much selection (eg, \\(v_{AA} = v_Av_A\\) ) under diploid selection for evolution to proceed as quickly as under haploid selection. Why is that? 4. One-locus haploid selection in continuous time We have assumed, so far, that generations are discrete. For populations with overlapping generations, a similar model may be constructed in continuous time. We now define fitness according to the growth rate of each genotype: \\(r_A =\\) growth rate of each \\(A\\) individual \\(r_a =\\) growth rate of each \\(a\\) individual These definitions tell us that: \\(\\frac{dN_A}{dt} = r_AN_A\\) \\(\\frac{dN_a}{dt} = r_aN_a\\) At any particular point in time, \\(p = N_A/(N_A + N_a)\\) . How can we derive \\(dp/dt\\) given that we know how the number of each type changes over time? Mathematical aside: the chain rule A function of one variable, \\(f(x(t))\\) , obeys the one-variable chain rule: \\[ \\frac{df}{dt} = \\frac{df}{dx}\\frac{dx}{dt} \\] A function of two variables, \\(f(x(t), y(t))\\) , obeys the two-variable chain rule: \\[ \\frac{df}{dt} = \\frac{\\delta f}{\\delta x}\\frac{dx}{dt} + \\frac{\\delta f}{\\delta y}\\frac{dy}{dt} \\] Now, because \\(p\\) is a function of both \\(N_A\\) and \\(N_a\\) , we can use the two-variable chain rule to determine that \\[ \\frac{dp}{dt} = \\frac{\\delta p}{\\delta N_A}\\frac{dN_A}{dt} + \\frac{\\delta p}{\\delta N_a}\\frac{dN_a}{dt} \\] \\[ \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] where \\((r_A - r_a)\\) is the selective advantage of allele \\(A\\) in terms of its growth rate. A similar equation can be derived for the diploid-selection model in continuous time, but we will not study this equation in class. 5. Comparing the haploid selection models in discrete and continuous time Are the haploid-selection models in discrete and continuous time as different as they look? \\[ \\text{Discrete model: } p(t+1) = \\frac{v_Ap(t)}{v_Ap(t) + v_aq(t)} \\] \\[ \\text{Continuous model: } \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] Not really. Discrete and continuous time models generally behave in a similar fashion when changes occur slowly over time. For this model of selection, this implies that they will be similar when the fitnesses are nearly equal, ie, \\(v_A \u2212 v_a\\) is small. This is fairly easy to prove with the definition of a derivative !!! note \"Mathematical aside: definition of the derivative $$ \\frac{df}{dt} = \\lim_{\\Delta t \\rightarrow 0}\\frac{f(t + \\Delta t) - f(t)}{\\Delta t} = \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Delta f}{\\Delta t} $$ When selection is weak, the change in allele frequency in the discrete model may be described approximately by the differential equation: \\[ \\frac{dp}{dt} = s p (1 \u2212 p) \\] where \\(s = vA \u2212 va\\) . This is the same equation governing the change in allele frequency over time in the continuous time model, if we define \\(s = rA \u2212 ra\\) . 6. The models we've covered Model Discrete time Continous time Exponential growth \\(N(t+1) = (1+r)N(t)\\) \\(\\frac{dN(t)}{dt} = rN(t)\\) Logistic growth \\(N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t)\\) \\(\\frac{dN}{dt} = r(1 - \\frac{N(t)}{K})N(t)\\) Haploid selection \\(p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a}\\) \\(\\frac{dp}{dt} = (r_A - r_a)p(1-p)\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t)q(t)W_{Aa}}{p(t)^2W_{AA} + p(t)q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived See textbook sections 3.4 and 3.5 for models of interacting species and epidemiology.","title":"Lecture 4"},{"location":"lectures/lecture-4/#lecture-4-one-locus-selection","text":"Run notes interactively?","title":"Lecture 4: One-locus selection"},{"location":"lectures/lecture-4/#lecture-overview","text":"Review of Hardy-Weinberg One-locus haploid selection in discrete time One-locus diploid selection in discrete time One-locus haploid selection in continuous time Comparing the haploid selection models in discrete and continuous time The models we've covered","title":"Lecture overview"},{"location":"lectures/lecture-4/#1-review-of-hardy-weinberg","text":"After one generation of random mating in a single large population with no selection and no mutation, diploid genotype frequencies at a locus with two alleles become: \\(Freq(AA) = x = p^2\\) \\(Freq(Aa) = y = 2pq\\) \\(Freq(aa) = z = q^2\\) \\(\\rightarrow\\) Hardy-Weinberg equilibrium . Furthermore, \\(p' = p\\) . \\(\\rightarrow\\) Allele frequencies do not change over time. Question : What if the genotypes vary in fitness?","title":"1. Review of Hardy-Weinberg"},{"location":"lectures/lecture-4/#2-one-locus-haploid-selection-in-discrete-time","text":"We begin by examining selection in a model with haploid selection.","title":"2. One-locus haploid selection in discrete time"},{"location":"lectures/lecture-4/#life-cycle-diagram","text":"Figure. Life-cycle diagram of haploid selection We will census at the beginning of the haploid phase (immediately after meiosis).","title":"Life-cycle diagram"},{"location":"lectures/lecture-4/#derivation","text":"Let there be two haploid genotypes ( \\(A\\) and \\(a\\) ) with \\(N_A =\\) number of \\(A\\) individuals \\(N_a =\\) number of \\(a\\) individuals We assume that these numbers are very large. What is the frequency of \\(A\\) , \\(p(t)\\) , at this point? \\[ p(t) = \\frac{N_a(t)}{N_A(t) + N_a(t)} \\] Now, let\u2019s assume that not all haploid individuals survive to reproduce: \\(v_A =\\) probability of survival (or viability) of an \\(A\\) individual \\(v_a =\\) probability of survival (or viability) of an \\(a\\) individual After these deaths have occurred (but before gametes unite): How many \\(A\\) individuals do we expect to still be alive? \\[ v_AN_A(t) \\] How many a individuals do we expect to still be alive? \\[ v_aN_a(t) \\] What is the frequency of A among these survivors? \\[ \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] Because gametes unite at random and undergo meiosis to produce the next generation of haploids, and neither of these processes change allele frequencies, the frequency of A at the next census is: \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] We initially showed that the frequency of A in the current generation is \\[ p(t) = \\frac{N_A(t)}{N_A(t) + N_a(t)} \\] And we just showed that the frequency of A in the next generation is \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} \\] but... this isn\u2019t quite a recursion equation. Why? (Hint: what's the difference between \\(p\\) and \\(N\\) ?) \\[ p(t+1) = \\frac{v_AN_A(t)}{v_AN_A(t) + v_aN_a(t)} = \\frac{v_A\\frac{N_A(t)}{N_A(t) + N_a(t)}}{v_A\\frac{N_A(t)}{N_A(t) + N_a(t)} + v_a\\frac{N_a(t)}{N_A(t) + N_a(t)}} = \\frac{v_Ap(t)}{v_Ap(t) + v_a(1-p(t))} \\] This is a recursion equation giving the allele frequency over time in our model of viability selection during the haploid phase. The denominator in this equation is the mean absolute fitness of the population, \\(\\bar v(t)\\) , which will determine the dynamics of the total population size \\[ N(t+1) \\equiv N_A(t+1) + N_a(t+1) = N(t)\\bar v(t) \\]","title":"Derivation"},{"location":"lectures/lecture-4/#simplifying-our-equation-of-one-locus-haploid-selection","text":"Our current recursion is a function of two parameters, the absolute viabilities, \\(v_A\\) and \\(v_a\\) \\[ p(t + 1) = \\frac{v_Ap(t)}{v_Ap(t) + v_a(1 \u2212 p(t))} \\] Now notice that if we divide both the numerator and denominator by one of these viabilities, say \\(v_a\\) \\[ p(t + 1) = \\frac{(v_A/v_a)p(t)}{(v_A/v_a)p(t) + (v_a/v_a)(1 \u2212 p(t))} \\] We can reduce the recursion to a function of only one parameter, \\(V_A = v_A/v_a\\) , the relative viability of \\(A\\) \\[ p(t + 1) = \\frac{V_Ap(t)}{V_Ap(t) + 1 - p(t)} \\] The denominator in this equation is the mean relative fitness of the population. This simpler equation shows that the dynamics of the total number of individuals does not influence evolution in this model. All we need to predict evolution are the relative fitnesses.","title":"Simplifying our equation of one-locus haploid selection"},{"location":"lectures/lecture-4/#3-one-locus-diploid-selection-in-discrete-time","text":"Since we are all currently in the diploid phase of our life-cycle, it is natural to ask: Does selection in the diploid phase work the same way?","title":"3. One-locus diploid selection in discrete time"},{"location":"lectures/lecture-4/#life-cycle-diagram_1","text":"Figure. Life-cycle diagram of diploid selection We will census at the beginning of the diploid phase (immediately after gamete union).","title":"Life-cycle diagram"},{"location":"lectures/lecture-4/#derivation_1","text":"Let there be three genotypes \\(AA\\) , \\(Aa\\) , and \\(aa\\) with \\(N_{AA}(t) =\\) number of \\(AA\\) individuals \\(N_{Aa}(t) =\\) number of \\(Aa\\) individuals \\(N_{aa}(t) =\\) number of \\(aa\\) individuals and we assume these to be very large. What is the frequency of \\(A\\) , \\(p(t)\\) , at this point? \\[ p(t) = \\frac{2 * N_{AA}(t) + N_{Aa}(t)}{2 * N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)} = \\frac{N_{AA}(t) + \\frac{1}{2}N_{Aa}(t)}{N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)} \\] Now let's assume that not all diploid individuals survive to reproduce: \\(V_{AA} =\\) probability of survival (or viability) of an \\(AA\\) individual \\(V_{Aa} =\\) probability of survival (or viability) of an \\(Aa\\) individual \\(V_{aa} =\\) probability of survival (or viability) of an \\(aa\\) individual After these deaths (and before meiosis): How many AA individuals will still be alive, on average? \\[ v_{AA}N_{AA}(t) \\] How many Aa individuals will still be alive, on average? \\[ v_{Aa}N_{Aa}(t) \\] How many aa individuals will still be alive, on average? \\[ v_{aa}N_{aa}(t) \\] What is the frequency of the \\(A\\) allele among adults? \\[ \\frac{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t)}{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t) + v_{aa}N_{aa}(t)} \\] And since meiosis and gamete union will not change this frequency, the frequency of the A allele in the next generation is \\[ p(t+1) = \\frac{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t)}{v_{AA}N_{AA}(t) + \\frac{1}{2}v_{Aa}N_{Aa}(t) + v_{aa}N_{aa}(t)} \\] With random union of gametes in a gamete pool or random mating of individuals, the diploid offspring are in Hardy-Weinberg proportions: \\(p^2(t)\\) \u2236 \\(2p(t)q(t)\\) \u2236 \\(q^2(t)\\) , where \\(p(t)\\) is the frequency of the A allele among the mating individuals at time t. Specifically, \\(N_{AA}(t) = p^2(t)N(t)\\) \\(N_{Aa}(t) = 2p(t)q(t)N(t)\\) \\(N_{aa}(t) = q^2(t)N(t)\\) where \\(N(t) \\equiv N_{AA}(t) + N_{Aa}(t) + N_{aa}(t)\\) is the total population size. Substituting these in we can rewrite the above equation as \\[ p(t+1) = \\frac{v_{AA}p(t)^2 + v_{Aa}p(t)q(t)}{v_{AA}p(t)^2 + 2v_{Aa}p(t)q(t) + v_{aa}q(t)^2} \\] This is a recursion equation giving the allele frequency over time in the model with viability selection during the diploid phase.","title":"Derivation"},{"location":"lectures/lecture-4/#simplifying-our-equation-of-one-locus-diploid-selection","text":"As in the haploid selection case, we can rewrite our recursion with absolute viabilities in terms of relative viabilities by dividing the numerator and denominator by some constant, say \\(v_{aa}\\) , \\[ p(t+1) = \\frac{(v_{AA}/v_{aa})p(t)^2 + (v_{Aa}/v_{aa})p(t)q(t)}{(v_{AA}/v_{aa})p(t)^2 + 2(v_{Aa}/v_{aa})p(t)q(t) + (v_{aa}/v_{aa})q(t)^2} \\] \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t)q(t) + q(t)^2} \\] which is a function of only two parameters, WAA = vAA/vaa and WAa = vAa/vaa, and shows that evolution once again does not depend on the dynamics of total population size (just relative fitnesses).","title":"Simplifying our equation of one-locus diploid selection"},{"location":"lectures/lecture-4/#comparing-the-one-locus-haploid-and-diploid-discrete-time-models","text":"So, returning to our original question, how does evolution under diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + 2W_{Aa}p(t)q(t) + q(t)^2} \\] compare to evolution under haploid selection \\[ p(t+1) = \\frac{v_Ap(t)}{v_ap(t) + v_aq(t)} \\] To compare these models, let\u2019s assume the viability of a diploid genotype is the product of the haploid viabilities, ie, \\(v_{AA} = v_Av_A\\) , \\(v_{Aa} = v_Av_a\\) , and \\(v_{aa} = v_av_a\\) . Then our diploid recursion reduces to \\[ p(t+1) = \\frac{v_{A}v_{A}p(t)^2 + v_{A}v_{a}p(t)q(t)}{v_{A}v_{A}p(t)^2 + 2v_{A}v_{a}p(t)q(t) + v_{a}v_{a}q(t)^2} \\] \\[ p(t+1) = \\frac{v_{A}p(t)(p(t) + v_{a}q(t))}{v_{A}p(t)(v_Ap(t) + v_{a}q(t)) + v_{a}q(t)(v_Ap(t) + v_aq(t))} \\] \\[ p(t+1) = \\frac{v_Ap(t)}{v_Ap(t) + v_aq(t)} \\] Thus we need twice as much selection (eg, \\(v_{AA} = v_Av_A\\) ) under diploid selection for evolution to proceed as quickly as under haploid selection. Why is that?","title":"Comparing the one-locus haploid and diploid discrete-time models"},{"location":"lectures/lecture-4/#4-one-locus-haploid-selection-in-continuous-time","text":"We have assumed, so far, that generations are discrete. For populations with overlapping generations, a similar model may be constructed in continuous time. We now define fitness according to the growth rate of each genotype: \\(r_A =\\) growth rate of each \\(A\\) individual \\(r_a =\\) growth rate of each \\(a\\) individual These definitions tell us that: \\(\\frac{dN_A}{dt} = r_AN_A\\) \\(\\frac{dN_a}{dt} = r_aN_a\\) At any particular point in time, \\(p = N_A/(N_A + N_a)\\) . How can we derive \\(dp/dt\\) given that we know how the number of each type changes over time? Mathematical aside: the chain rule A function of one variable, \\(f(x(t))\\) , obeys the one-variable chain rule: \\[ \\frac{df}{dt} = \\frac{df}{dx}\\frac{dx}{dt} \\] A function of two variables, \\(f(x(t), y(t))\\) , obeys the two-variable chain rule: \\[ \\frac{df}{dt} = \\frac{\\delta f}{\\delta x}\\frac{dx}{dt} + \\frac{\\delta f}{\\delta y}\\frac{dy}{dt} \\] Now, because \\(p\\) is a function of both \\(N_A\\) and \\(N_a\\) , we can use the two-variable chain rule to determine that \\[ \\frac{dp}{dt} = \\frac{\\delta p}{\\delta N_A}\\frac{dN_A}{dt} + \\frac{\\delta p}{\\delta N_a}\\frac{dN_a}{dt} \\] \\[ \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] where \\((r_A - r_a)\\) is the selective advantage of allele \\(A\\) in terms of its growth rate. A similar equation can be derived for the diploid-selection model in continuous time, but we will not study this equation in class.","title":"4. One-locus haploid selection in continuous time"},{"location":"lectures/lecture-4/#5-comparing-the-haploid-selection-models-in-discrete-and-continuous-time","text":"Are the haploid-selection models in discrete and continuous time as different as they look? \\[ \\text{Discrete model: } p(t+1) = \\frac{v_Ap(t)}{v_Ap(t) + v_aq(t)} \\] \\[ \\text{Continuous model: } \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] Not really. Discrete and continuous time models generally behave in a similar fashion when changes occur slowly over time. For this model of selection, this implies that they will be similar when the fitnesses are nearly equal, ie, \\(v_A \u2212 v_a\\) is small. This is fairly easy to prove with the definition of a derivative !!! note \"Mathematical aside: definition of the derivative $$ \\frac{df}{dt} = \\lim_{\\Delta t \\rightarrow 0}\\frac{f(t + \\Delta t) - f(t)}{\\Delta t} = \\lim_{\\Delta t \\rightarrow 0}\\frac{\\Delta f}{\\Delta t} $$ When selection is weak, the change in allele frequency in the discrete model may be described approximately by the differential equation: \\[ \\frac{dp}{dt} = s p (1 \u2212 p) \\] where \\(s = vA \u2212 va\\) . This is the same equation governing the change in allele frequency over time in the continuous time model, if we define \\(s = rA \u2212 ra\\) .","title":"5. Comparing the haploid selection models in discrete and continuous time"},{"location":"lectures/lecture-4/#6-the-models-weve-covered","text":"Model Discrete time Continous time Exponential growth \\(N(t+1) = (1+r)N(t)\\) \\(\\frac{dN(t)}{dt} = rN(t)\\) Logistic growth \\(N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t)\\) \\(\\frac{dN}{dt} = r(1 - \\frac{N(t)}{K})N(t)\\) Haploid selection \\(p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a}\\) \\(\\frac{dp}{dt} = (r_A - r_a)p(1-p)\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t)q(t)W_{Aa}}{p(t)^2W_{AA} + p(t)q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived See textbook sections 3.4 and 3.5 for models of interacting species and epidemiology.","title":"6. The models we've covered"},{"location":"lectures/lecture-5/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 5: Numerical and graphical techniques I (univariate) Run notes interactively? Lecture overview Numerical and graphical techniques Plots of variables over time Plots of variables as a function of the variables themselves Summary 1. Numerical and graphical techniques Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models. To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time. The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally. The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses. 2. Plots of variables over time Exponential growth model In the exponential growth model, there is one parameter, R, the average number of offspring per parent. In last week\u2019s lab we wrote a recursive function to generate values of n(t), the population size, at sequential time points import numpy as np def n(t0, n0, R, max=np.inf): # Set the initial value of t and n(t) t, nt = t0, n0 # Yield new values of n(t) if t hasn't gone past the max value while t < = max: yield t, nt # Then update t (this is equivalent to t = t + 1) and n(t) t, nt = t + 1, nt * R We then chose some parameter values (reproductive factor R = 2) and initial conditions (initial population size n0 = 1) to get the values of \\([t, n(t)]\\) from t = 0 to t = 10 nt = n(t0=0, n0=1, R=2, max=10) #choose some parameter values t_nt = np.array([[t,n] for t,n in nt]) #get all the t, n(t) values t_nt And we then plotted n(t) as a function of t import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(t_nt[:,0], t_nt[:,1]) ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') plt.show() This allowed us to compare what happens for different values of R colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) t_nt = np.array([[t,n] for t,n in nt]) ax.scatter(t_nt[:,0], t_nt[:,1], color=colors[i], label=f\"R = {R}\") ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) plt.show() And while we cannot iterate a differential equation in the same way, in this simple case we can solve the differential equation to compare the discrete and continuous models import sympy # Initialize symbols and functions # (we add _ to each variable to differentiate between discrete time eq. above) r_, t_, n0_ = sympy.symbols('r_, t_, n0_') #define the variables n_ = sympy.Function('n_') #define a general function # Initialize the differential equation diffeq = sympy.Eq(n_(t_).diff(t_), r_ * n_(t_)) # Solve the differential equation fn = sympy.dsolve( diffeq, # The differential equation n_(t_), # The function we want to solve for ics={n_(0): n0_} #t: 0} ) # Convert the solved differential equation into a pythonic function p = sympy.lambdify((n0_, t_, r_), fn.rhs) # Plot the differential equation over the discrete time scatter plot colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) # Discrete time t_nt = np.array([[t,n] for t,n in nt]) ax.scatter(t_nt[:,0], t_nt[:,1], color=colors[i], label=f\"R = {R}\") # Continuous time # (remember that growth rate r is equal to the reproductive factor R - 1) t_diff = np.array([[t, p(100, t, R-1)] for t in np.linspace(0, 10, 10)]) ax.plot(t_diff[:,0], t_diff[:,1], color=colors[i]) ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) plt.show() Logistic growth model In the logistic growth model , there are two parameters, the carrying capacity K and the growth rate r. The behaviour doesn\u2019t change much with different values of K, but it is extremely sensitive to the value of r, as we saw in last week\u2019s lab. def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < = max: yield t, nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Initialize plots fig, ax = plt.subplots(1, 2, sharex=True, sharey=True) fig.set_size_inches(12,4) # Logistic growth with \"normal\" dynamics for r in [0.40, 0.70, 1.80, 2.10]: ax[0].plot( np.arange(0, 26), [nt[1] for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\" ) # Logistic growth with \"chaotic\" or \"degenerate\" dynamics for r in [2.70, 3.0995]: ax[1].plot( np.arange(0, 26), [nt[1] for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\" ) # Add titles and annotations ax[0].set_title('\"Normal\" dynamics') ax[1].set_title('\"Chaotic\" dynamics') for i in range(2): ax[i].set_xlabel('generation, $t$') ax[i].set_ylabel('population size, $n(t)$') ax[i].legend(frameon=False) fig.tight_layout() plt.show() We can also check out how the dynamics oscillate by creating a bifurcation diagram of the growth after carrying capacity has been reached. # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt[1] for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') These are some very complex and potentially strange dynamics! Does the continuous-time model behave the same? Let\u2019s numerically solve the differential equation for some particular parameter values # Initialize symbols and functions # (we add _ to each variable to differentiate between discrete time eq. above) r, t, n0, K = sympy.symbols('r, t, n0, K') #define the variables n_ = sympy.Function('n_') #define a general function # Initialize the differential equation diffeq = sympy.Eq(n_(t).diff(t), r * n_(t) * (1 - n_(t) / K)) # Solve the differential equation fn = sympy.dsolve( diffeq, # The differential equation n_(t), # The function we want to solve for ics={n_(0): n0} #t: 0} ) # Convert the solved differential equation into a pythonic function p = sympy.lambdify((n0, t, r, K), fn.rhs) # Logistic growth in discrete time vs continuous time fig, ax = plt.subplots() ax.plot( np.arange(0, 26), [nt[1] for nt in n(1, 2.70, 1000, max=25)], label = f\"r = 2.70 (discrete time)\" ) ax.plot( np.linspace(0, 26, 100), [p(1, t, 2.7, 1000) for t in np.linspace(0, 26, 100)], label = f\"r = 2.70 (continuous time)\" ) # Add titles and annotations ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) fig.tight_layout() plt.show() 3. Plots of variables as a function of the variables themselves OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with). Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable, eg, n(t + 1) as a function of n(t). Haploid selection Let's start with our model of haploid selection \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") ax.plot(t, t, color='black', linestyle='--') # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) Note that the cobweb plots (staircase looking lines in blue) track the movement of the allele frequencies from \\(t \\rightarrow t + 1\\) . By following the cobweb, you can determine if and where the system will converge to an equilibrium. Diploid selection Now let\u2019s move on to the slightly more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] def cobweb_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_diploid_selection(WAA, WAa, Waa, ax=None, p0=0.5): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WAA * pt**2 + WAa * pt * (1- pt) ) / (WAA * pt**2 + WAa * 2 * pt * (1 - pt) + Waa * (1 - pt)**2) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation x = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(x) # Build plot if ax == None: fig, ax = plt.subplots() # Add cobweb cobweb = np.array([p for p in cobweb_diploid(p0, WAA, WAa, Waa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.plot(x, fy, color='black', label=f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}\") ax.plot(x, x, color='black', linestyle='--') ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.95, ax=ax[0]) # Second cobweb with WA < Wa plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.21, ax=ax[1]) Other models We can do something very similar for difference and differential equations. Now we plot the change in the variable as a function of the current value of the variable, eg, \\(dn/dt\\) as a function of \\(n(t)\\) . For example, in our model of haploid selection we have \\[ \\frac{dp}{dt} = sp(t)(1-p(t)) \\] # Initialize sympy symbols p0, s, t = sympy.symbols('p0, s, t') p = sympy.Function('t') # Specify differential equation diffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t))) # Convert differential equation RHS to pythonic function dp = sympy.lambdify((s, p(t)), diffeq.rhs) # Plot the curve fig, ax = plt.subplots() for s_coeff in [0.01, -0.01]: ax.plot( np.linspace(0, 1, 100), dp(s_coeff, np.linspace(0,1, 100)), label=f\"s = {s_coeff}\" ) ax.set_xlabel('allele frequency at $t, p(t)$') ax.set_ylabel('change in allele frequency, $dp/dt$') ax.legend(frameon=False) We\u2019ll take a look at the even more complex model of logistic growth in this week\u2019s lab \\[ n(t+1) = n(t) = rn(t)(1 - n(t)/K) \\] 4. Summary To get a feel for our model it is helpful to graph some numerical examples: Plot the variable as a function of time Plot the variable as a function of itself (univariate) Next time we\u2019ll look at a technique for models of multiple variables...","title":"Lecture 5"},{"location":"lectures/lecture-5/#lecture-5-numerical-and-graphical-techniques-i-univariate","text":"Run notes interactively?","title":"Lecture 5: Numerical and graphical techniques I (univariate)"},{"location":"lectures/lecture-5/#lecture-overview","text":"Numerical and graphical techniques Plots of variables over time Plots of variables as a function of the variables themselves Summary","title":"Lecture overview"},{"location":"lectures/lecture-5/#1-numerical-and-graphical-techniques","text":"Before we jump into more rigorous mathematical analyses, we\u2019re first going to learn how to get a feel for the dynamics of our models. To do so we\u2019re going to choose some particular numerical values for our parameters and then use our models to predict what happens over time. The downside of this approach is that we often won\u2019t know the parameter values to choose and, regardless, choosing particular values doesn\u2019t tell us about the dynamics of our model more generally. The upside is that this approach can highlight errors or reveal unexpected patterns that guide future mathematical analyses.","title":"1. Numerical and graphical techniques"},{"location":"lectures/lecture-5/#2-plots-of-variables-over-time","text":"","title":"2. Plots of variables over time"},{"location":"lectures/lecture-5/#exponential-growth-model","text":"In the exponential growth model, there is one parameter, R, the average number of offspring per parent. In last week\u2019s lab we wrote a recursive function to generate values of n(t), the population size, at sequential time points import numpy as np def n(t0, n0, R, max=np.inf): # Set the initial value of t and n(t) t, nt = t0, n0 # Yield new values of n(t) if t hasn't gone past the max value while t < = max: yield t, nt # Then update t (this is equivalent to t = t + 1) and n(t) t, nt = t + 1, nt * R We then chose some parameter values (reproductive factor R = 2) and initial conditions (initial population size n0 = 1) to get the values of \\([t, n(t)]\\) from t = 0 to t = 10 nt = n(t0=0, n0=1, R=2, max=10) #choose some parameter values t_nt = np.array([[t,n] for t,n in nt]) #get all the t, n(t) values t_nt And we then plotted n(t) as a function of t import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(t_nt[:,0], t_nt[:,1]) ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') plt.show() This allowed us to compare what happens for different values of R colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) t_nt = np.array([[t,n] for t,n in nt]) ax.scatter(t_nt[:,0], t_nt[:,1], color=colors[i], label=f\"R = {R}\") ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) plt.show() And while we cannot iterate a differential equation in the same way, in this simple case we can solve the differential equation to compare the discrete and continuous models import sympy # Initialize symbols and functions # (we add _ to each variable to differentiate between discrete time eq. above) r_, t_, n0_ = sympy.symbols('r_, t_, n0_') #define the variables n_ = sympy.Function('n_') #define a general function # Initialize the differential equation diffeq = sympy.Eq(n_(t_).diff(t_), r_ * n_(t_)) # Solve the differential equation fn = sympy.dsolve( diffeq, # The differential equation n_(t_), # The function we want to solve for ics={n_(0): n0_} #t: 0} ) # Convert the solved differential equation into a pythonic function p = sympy.lambdify((n0_, t_, r_), fn.rhs) # Plot the differential equation over the discrete time scatter plot colors = ['black','blue','red'] fig, ax = plt.subplots() for i, R in enumerate([1.1,1,0.9]): nt = n(t0=0, n0=100, R=R, max=10) # Discrete time t_nt = np.array([[t,n] for t,n in nt]) ax.scatter(t_nt[:,0], t_nt[:,1], color=colors[i], label=f\"R = {R}\") # Continuous time # (remember that growth rate r is equal to the reproductive factor R - 1) t_diff = np.array([[t, p(100, t, R-1)] for t in np.linspace(0, 10, 10)]) ax.plot(t_diff[:,0], t_diff[:,1], color=colors[i]) ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) plt.show()","title":"Exponential growth model"},{"location":"lectures/lecture-5/#logistic-growth-model","text":"In the logistic growth model , there are two parameters, the carrying capacity K and the growth rate r. The behaviour doesn\u2019t change much with different values of K, but it is extremely sensitive to the value of r, as we saw in last week\u2019s lab. def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < = max: yield t, nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Initialize plots fig, ax = plt.subplots(1, 2, sharex=True, sharey=True) fig.set_size_inches(12,4) # Logistic growth with \"normal\" dynamics for r in [0.40, 0.70, 1.80, 2.10]: ax[0].plot( np.arange(0, 26), [nt[1] for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\" ) # Logistic growth with \"chaotic\" or \"degenerate\" dynamics for r in [2.70, 3.0995]: ax[1].plot( np.arange(0, 26), [nt[1] for nt in n(1, r, 1000, max=25)], label = f\"r = {r}\" ) # Add titles and annotations ax[0].set_title('\"Normal\" dynamics') ax[1].set_title('\"Chaotic\" dynamics') for i in range(2): ax[i].set_xlabel('generation, $t$') ax[i].set_ylabel('population size, $n(t)$') ax[i].legend(frameon=False) fig.tight_layout() plt.show() We can also check out how the dynamics oscillate by creating a bifurcation diagram of the growth after carrying capacity has been reached. # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt[1] for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') These are some very complex and potentially strange dynamics! Does the continuous-time model behave the same? Let\u2019s numerically solve the differential equation for some particular parameter values # Initialize symbols and functions # (we add _ to each variable to differentiate between discrete time eq. above) r, t, n0, K = sympy.symbols('r, t, n0, K') #define the variables n_ = sympy.Function('n_') #define a general function # Initialize the differential equation diffeq = sympy.Eq(n_(t).diff(t), r * n_(t) * (1 - n_(t) / K)) # Solve the differential equation fn = sympy.dsolve( diffeq, # The differential equation n_(t), # The function we want to solve for ics={n_(0): n0} #t: 0} ) # Convert the solved differential equation into a pythonic function p = sympy.lambdify((n0, t, r, K), fn.rhs) # Logistic growth in discrete time vs continuous time fig, ax = plt.subplots() ax.plot( np.arange(0, 26), [nt[1] for nt in n(1, 2.70, 1000, max=25)], label = f\"r = 2.70 (discrete time)\" ) ax.plot( np.linspace(0, 26, 100), [p(1, t, 2.7, 1000) for t in np.linspace(0, 26, 100)], label = f\"r = 2.70 (continuous time)\" ) # Add titles and annotations ax.set_xlabel('generation, $t$') ax.set_ylabel('population size, $n(t)$') ax.legend(frameon=False) fig.tight_layout() plt.show()","title":"Logistic growth model"},{"location":"lectures/lecture-5/#3-plots-of-variables-as-a-function-of-the-variables-themselves","text":"OK, so now we\u2019ll move on to a plot that is easier to generate and is very useful for models with just one variable (which is what we\u2019ve been working with). Instead of plotting the variable as a function of time, we\u2019ll plot the variable as a function of the variable, eg, n(t + 1) as a function of n(t).","title":"3. Plots of variables as a function of the variables themselves"},{"location":"lectures/lecture-5/#haploid-selection","text":"Let's start with our model of haploid selection \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") ax.plot(t, t, color='black', linestyle='--') # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax[0]) # Second cobweb with WA < Wa plot_haploid_selection(WA = 0.5, Wa = 1, ax=ax[1]) Note that the cobweb plots (staircase looking lines in blue) track the movement of the allele frequencies from \\(t \\rightarrow t + 1\\) . By following the cobweb, you can determine if and where the system will converge to an equilibrium.","title":"Haploid selection"},{"location":"lectures/lecture-5/#diploid-selection","text":"Now let\u2019s move on to the slightly more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] def cobweb_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_diploid_selection(WAA, WAa, Waa, ax=None, p0=0.5): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WAA * pt**2 + WAa * pt * (1- pt) ) / (WAA * pt**2 + WAa * 2 * pt * (1 - pt) + Waa * (1 - pt)**2) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation x = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(x) # Build plot if ax == None: fig, ax = plt.subplots() # Add cobweb cobweb = np.array([p for p in cobweb_diploid(p0, WAA, WAa, Waa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.plot(x, fy, color='black', label=f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}\") ax.plot(x, x, color='black', linestyle='--') ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1,2) fig.set_size_inches(12,4) # First cobweb with WA > Wa plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.95, ax=ax[0]) # Second cobweb with WA < Wa plot_diploid_selection(WAA=1, WAa=2, Waa=1, p0=0.21, ax=ax[1])","title":"Diploid selection"},{"location":"lectures/lecture-5/#other-models","text":"We can do something very similar for difference and differential equations. Now we plot the change in the variable as a function of the current value of the variable, eg, \\(dn/dt\\) as a function of \\(n(t)\\) . For example, in our model of haploid selection we have \\[ \\frac{dp}{dt} = sp(t)(1-p(t)) \\] # Initialize sympy symbols p0, s, t = sympy.symbols('p0, s, t') p = sympy.Function('t') # Specify differential equation diffeq = sympy.Eq(p(t).diff(t), s * p(t) * (1 - p(t))) # Convert differential equation RHS to pythonic function dp = sympy.lambdify((s, p(t)), diffeq.rhs) # Plot the curve fig, ax = plt.subplots() for s_coeff in [0.01, -0.01]: ax.plot( np.linspace(0, 1, 100), dp(s_coeff, np.linspace(0,1, 100)), label=f\"s = {s_coeff}\" ) ax.set_xlabel('allele frequency at $t, p(t)$') ax.set_ylabel('change in allele frequency, $dp/dt$') ax.legend(frameon=False) We\u2019ll take a look at the even more complex model of logistic growth in this week\u2019s lab \\[ n(t+1) = n(t) = rn(t)(1 - n(t)/K) \\]","title":"Other models"},{"location":"lectures/lecture-5/#4-summary","text":"To get a feel for our model it is helpful to graph some numerical examples: Plot the variable as a function of time Plot the variable as a function of itself (univariate) Next time we\u2019ll look at a technique for models of multiple variables...","title":"4. Summary"},{"location":"lectures/lecture-6/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 6: Numerical and graphical techniques II (multivariate) Run notes interactively? Lecture overview Numerical and graphical techniques Phase-line diagrams Phase-plane diagrams 1. Numerical and graphical techniques Last time we talked about two numerical/graphical approaches to get a better understanding of our models: Plotting our variable as a function of time (eg, p(t) as a function of t) Plotting our variable as a function of itself (eg, p(t + 1) as a function of p(t)) The latter works well for models with one variable. Today we\u2019re going to talk about a third numerical technique, a phase-plane diagram , which is especially useful for models that have two variables. 2. Phase-line diagrams Before looking at models with two variables, let\u2019s first consider some with only one. Consider again haploid selection where \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] Last time we plotted p(t + 1) as a function of p(t). We called this plot a cob-web plot. import sympy import numpy as np import matplotlib.pyplot as plt # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") ax.plot(t, t, color='black', linestyle='--') # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1) fig.set_size_inches(6,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax) Now let's simplify the cob-web and just indicate the direction of change in \\(p(t)\\) . This is known as a phase-line diagram. def phase_line_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_haploid(WA, Wa, p0, max=10): fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pl = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] ax.plot( pl, np.zeros(max+1), alpha=1 ) # Plot phase-line markers marker = '>' if pl[2] > pl[1] else ' < ' ax.scatter( pl, np.zeros(max+1), marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_haploid(WA=1, Wa=0.5, p0=0.1) plot_phase_line_haploid(WA=0.5, Wa=1, p0=0.3) Similarly with the more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20): fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pl = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] ax.plot( pl, np.zeros(max+1), alpha=1 ) # Plot phase-line markers marker = '>' if pl[2] > pl[1] else ' < ' ax.scatter( pl, np.zeros(max+1), marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_diploid(WAA=0.8, WAa=1.0, Waa=0.9, p0=0.6, max=100) 3. Phase-plane diagrams So now let\u2019s move on from one to two variables. Lotka-Volterra model And let\u2019s introduce a new model for this purpose, the Lotka-Volterra model of competition (see section 3.4.1 in the text). This is an extension of the logistic growth model to include competition between multiple species (in our case two). Let the population size of each species be \\(n_1(t)\\) and \\(n_2(t)\\) . These are our two variables. And let them have different intrinsic growth rates, \\(r_1\\) and \\(r_2\\) , and carrying capacities, \\(K_1\\) and \\(K_2\\) . To model competition, we\u2019ll assume that, for an individual of species \\(i\\) , an individual of species \\(j\\) is the competitive equivalent of \\(\\alpha ij\\) individuals of species \\(i\\) . We then have \\[ n_1(t+1) = n_1(t) + r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ n_2(t+1) = n_2(t) + r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] Often individuals of the same species will require more similar resources and therefore \\(0 < \\alpha ij < 1\\) , but not always. And, in fact, we could model other types of interactions (eg, mutualism) by making some of the interactions beneficial, \\(\\alpha ij < 0\\) . Phase-planes and vector fields So why did we introduce the Lotka-Volterra model and what's the relevance to phase-plane diagrams? Well, phase-plane diagrams plot the change in \\(n_1\\) and the change in \\(n_2\\) as a vector originating from many different starting conditions on a 2D plane. We can graphically investigate the dynamics of the Lotka-Volterra model by first defining the rates of change \\(\\Delta\\) in both \\(n_1\\) and \\(n_2\\) and then choosing some parameter values to explore. \\[ \\Delta n_1 \\equiv n_1(t+1) - n_1(t) = r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ \\Delta n_2 \\equiv n_2(t+1) - n_2(t) = r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] Let's explore the Lotka-Volterra with the following fixed parameters: \\(r_1 = 0.5, r_2 = 0.5, K_1 = 1000, K_2 = 1000, \\alpha_{12} = 0.0, \\alpha_{21} = 0.5\\) . # Define a function to plot the vector field for the change in n1 and n2 def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None]): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) # Plot figure fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) if show == True: plt.show() else: return ax # Initialize the sympy variables n1, n2 = sympy.symbols('n1, n2') # Choose the parameter values r1, r2 = 0.5, 0.5 k1, k2 = 1000, 1000 a12, a21 = 0, 0.5 # Specify the difference equations dn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1) dn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2) # Plot the vector field plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) # Add the stable equilibrium plt.scatter(1000, 500, s=150, color='blue') This plot is a phase-plane (plot of \\(n_1\\) vs \\(n_2\\) ) with a vector field . Here the length of the vectors indicates the magnitude of change in population sizes of \\(n_1\\) and \\(n_2\\) . If you inspect this vector field, you can see that the dynamics approach a stable equilibrium near \\(n_1 = 1000, n_2 = 500\\) . Null clines To better understand the dynamics, we can ask for what values of our variables ( \\(n_1, n_2\\) ) are our variables constant ( \\(\\Delta n_1 = 0\\) , \\(\\Delta n_2 = 0\\) ) \\(\\Longrightarrow\\) the coordinates/axes where the variables are constant are known as Null clines . Concretely, going back to our previous formula for the change in both the \\(n_1\\) and \\(n_2\\) species population sizes in the Lotka-Volterra model \\[ \\Delta n_1 = r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ \\Delta n_2 = r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] We want to know when \\(\\Delta n_1\\) and \\(\\Delta n_2\\) are 0. Solving for these inequalities shows that \\[ \\Delta n_1 = 0 \\Longrightarrow n_1(t) = 0, 1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1} = 0 \\] \\[ \\Delta n_2 = 0 \\Longrightarrow n_2(t) = 0, 1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2} = 0 \\] Plotting these null clines on the phase-plane diagram, we get # Initialize plot and ranges ax = plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) xrange, yrange = np.linspace(0, 1200, 100), np.linspace(0, 1200, 100) #plot the null clines for species 1 (blue) def plot_nullclines(ax): nullcline_1 = list(sympy.solve(sympy.Eq(dn1, 0))) ax.plot([nullcline_1[0] for i in range(len(xrange))], yrange, color='b') ax.plot([nullcline_1[1] for i in range(len(xrange))], yrange, color='b') # #plot the null clines for species 2 (red) nullcline_2 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn2, 0))] ax.plot(sympy.lambdify(n2, nullcline_2[0])(yrange), yrange, color='r') # A this null cline is a function of n2 (i.e. y) ax.plot(xrange, [nullcline_2[1] for i in range(len(yrange))], color='r') ax.set_ylim(-10, 1210) ax.set_xlim(-10, 1210) return ax plot_nullclines(ax) We can also make phase diagrams for continuous-time models, just using the differential equations in place of the difference equations. We\u2019ll see an example of that for another model, of predator and prey, in this week\u2019s lab.","title":"Lecture 6"},{"location":"lectures/lecture-6/#lecture-6-numerical-and-graphical-techniques-ii-multivariate","text":"Run notes interactively?","title":"Lecture 6: Numerical and graphical techniques II (multivariate)"},{"location":"lectures/lecture-6/#lecture-overview","text":"Numerical and graphical techniques Phase-line diagrams Phase-plane diagrams","title":"Lecture overview"},{"location":"lectures/lecture-6/#1-numerical-and-graphical-techniques","text":"Last time we talked about two numerical/graphical approaches to get a better understanding of our models: Plotting our variable as a function of time (eg, p(t) as a function of t) Plotting our variable as a function of itself (eg, p(t + 1) as a function of p(t)) The latter works well for models with one variable. Today we\u2019re going to talk about a third numerical technique, a phase-plane diagram , which is especially useful for models that have two variables.","title":"1. Numerical and graphical techniques"},{"location":"lectures/lecture-6/#2-phase-line-diagrams","text":"Before looking at models with two variables, let\u2019s first consider some with only one. Consider again haploid selection where \\[ p(t+1) = \\frac{W_Ap(t)}{W_Ap(t) + W_a(1-p(t))} \\] Last time we plotted p(t + 1) as a function of p(t). We called this plot a cob-web plot. import sympy import numpy as np import matplotlib.pyplot as plt # Build cobweb plotting function def cobweb_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow, pnext #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) yield pnow, pnext #current value of p(t) and p(t+1) pnow = pnext #update p(t) t += 1 #update t # Build function for generating figure def plot_haploid_selection(WA, Wa, p0=0.5, ax=None): pt = sympy.symbols('pt') #define our variable p(t) # Write out sympy equation f = (WA * pt) / (WA * pt + Wa * (1 - pt)) #the recursion equation # Compute function over a set of points in [0,1] by 'lambdifying' sympy equation t = np.linspace(0,1,100) fy = sympy.lambdify(pt, f)(t) # Build plot if ax == None: fig, ax = plt.subplots() ax.plot(t, fy, color='black', label=f\"$W_A$ = {WA}, $W_a$ = {Wa}\") ax.plot(t, t, color='black', linestyle='--') # Add cobweb cobweb = np.array([p for p in cobweb_haploid(p0, WA, Wa, max=100)]) ax.plot(cobweb[:,0], cobweb[:,1]) # Annotate and label plot ax.set_xlim(0,1) ax.set_ylim(0,1) ax.set_xlabel(\"allele frequency at $t$, $p(t)$\") ax.set_ylabel(\"allele frequency at $t+1$, $p(t+1)$\") ax.legend(frameon=False) return ax # Plot figure fig, ax = plt.subplots(1) fig.set_size_inches(6,4) # First cobweb with WA > Wa plot_haploid_selection(WA = 1, Wa = 0.5, ax=ax) Now let's simplify the cob-web and just indicate the direction of change in \\(p(t)\\) . This is known as a phase-line diagram. def phase_line_haploid(p0, WA, Wa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow #current value of p(t) and p(t+1) pnext = (WA * pnow) / (WA * pnow + Wa * (1 - pnow)) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_haploid(WA, Wa, p0, max=10): fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pl = [pt for pt in phase_line_haploid(p0, WA, Wa, max=max)] ax.plot( pl, np.zeros(max+1), alpha=1 ) # Plot phase-line markers marker = '>' if pl[2] > pl[1] else ' < ' ax.scatter( pl, np.zeros(max+1), marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$ = {WA}, $W_a$ = {Wa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_haploid(WA=1, Wa=0.5, p0=0.1) plot_phase_line_haploid(WA=0.5, Wa=1, p0=0.3) Similarly with the more complex model of diploid selection \\[ p(t+1) = \\frac{W_{AA}p(t)^2 + W_{Aa}p(t)q(t)}{W_{AA}p(t)^2 + W_{Aa}p(t)q(t) + W_{aa}q(t)^2} \\] def phase_line_diploid(p0, WAA, WAa, Waa, max=np.inf): t, pnow, pnext = 0, p0, 0 #initial conditions while t < = max: yield pnow #current value of p(t) and p(t+1) pnext = (WAA * pnow**2 + WAa * pnow * (1 - pnow)) / (WAA * pnow**2 + WAa * 2 * pnow * (1 - pnow) + Waa * (1 - pnow)**2) #update p(t+1) pnow = pnext #update p(t) t += 1 #update t def plot_phase_line_diploid(WAA, WAa, Waa, p0, max=20): fig, ax = plt.subplots() fig.set_size_inches(8,0.25) ax.axhline(0, color='black', linewidth=0.5) # Plot phase-line pl = [pt for pt in phase_line_diploid(p0, WAA, WAa, Waa, max=max)] ax.plot( pl, np.zeros(max+1), alpha=1 ) # Plot phase-line markers marker = '>' if pl[2] > pl[1] else ' < ' ax.scatter( pl, np.zeros(max+1), marker=marker, s=150 ) # Remove background axes ax.set_ylabel('$p$', rotation=0) ax.set_xlabel(f\"$W_A$$_A$ = {WAA}, $W_A$$_a$ = {WAa}, $W_a$$_a$ = {Waa}, $p_0$ = {p0}\") ax.get_yaxis().set_ticks([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False) ax.set_xlim(0,1) plt.show() plot_phase_line_diploid(WAA=0.8, WAa=1.0, Waa=0.9, p0=0.6, max=100)","title":"2. Phase-line diagrams"},{"location":"lectures/lecture-6/#3-phase-plane-diagrams","text":"So now let\u2019s move on from one to two variables.","title":"3. Phase-plane diagrams"},{"location":"lectures/lecture-6/#lotka-volterra-model","text":"And let\u2019s introduce a new model for this purpose, the Lotka-Volterra model of competition (see section 3.4.1 in the text). This is an extension of the logistic growth model to include competition between multiple species (in our case two). Let the population size of each species be \\(n_1(t)\\) and \\(n_2(t)\\) . These are our two variables. And let them have different intrinsic growth rates, \\(r_1\\) and \\(r_2\\) , and carrying capacities, \\(K_1\\) and \\(K_2\\) . To model competition, we\u2019ll assume that, for an individual of species \\(i\\) , an individual of species \\(j\\) is the competitive equivalent of \\(\\alpha ij\\) individuals of species \\(i\\) . We then have \\[ n_1(t+1) = n_1(t) + r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ n_2(t+1) = n_2(t) + r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] Often individuals of the same species will require more similar resources and therefore \\(0 < \\alpha ij < 1\\) , but not always. And, in fact, we could model other types of interactions (eg, mutualism) by making some of the interactions beneficial, \\(\\alpha ij < 0\\) .","title":"Lotka-Volterra model"},{"location":"lectures/lecture-6/#phase-planes-and-vector-fields","text":"So why did we introduce the Lotka-Volterra model and what's the relevance to phase-plane diagrams? Well, phase-plane diagrams plot the change in \\(n_1\\) and the change in \\(n_2\\) as a vector originating from many different starting conditions on a 2D plane. We can graphically investigate the dynamics of the Lotka-Volterra model by first defining the rates of change \\(\\Delta\\) in both \\(n_1\\) and \\(n_2\\) and then choosing some parameter values to explore. \\[ \\Delta n_1 \\equiv n_1(t+1) - n_1(t) = r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ \\Delta n_2 \\equiv n_2(t+1) - n_2(t) = r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] Let's explore the Lotka-Volterra with the following fixed parameters: \\(r_1 = 0.5, r_2 = 0.5, K_1 = 1000, K_2 = 1000, \\alpha_{12} = 0.0, \\alpha_{21} = 0.5\\) . # Define a function to plot the vector field for the change in n1 and n2 def plot_vector_field(dn1, dn2, xlim=(0,1200), ylim=(0,1200), n_steps=25, width=8, height=6, show=False, axes_labels=[None, None]): # Set x and y ranges xrange, yrange = np.linspace(xlim[0], xlim[1], n_steps), np.linspace(ylim[0], ylim[1], n_steps) # Initialize 2D grid with x,y values and additional grids to track derivatives X, Y = np.meshgrid(xrange, yrange) U, V = np.zeros(X.shape), np.zeros(Y.shape) # Compute the gradient at each x,y position for i in range(len(xrange)): for j in range(len(xrange)): U[i,j] = sympy.lambdify((n1, n2), dn1)(X[i,j], Y[i,j]) V[i,j] = sympy.lambdify((n1, n2), dn2)(X[i,j], Y[i,j]) # Plot figure fig, ax = plt.subplots() fig.set_size_inches(width, height) ax.set_xlabel(axes_labels[0]) ax.set_ylabel(axes_labels[1]) ax.quiver(X,Y,U,V, linewidth=1) if show == True: plt.show() else: return ax # Initialize the sympy variables n1, n2 = sympy.symbols('n1, n2') # Choose the parameter values r1, r2 = 0.5, 0.5 k1, k2 = 1000, 1000 a12, a21 = 0, 0.5 # Specify the difference equations dn1 = r1 * n1 * (1 - (n1 + a12 * n2) / k1) dn2 = r2 * n2 * (1 - (n2 + a21 * n1) / k2) # Plot the vector field plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) # Add the stable equilibrium plt.scatter(1000, 500, s=150, color='blue') This plot is a phase-plane (plot of \\(n_1\\) vs \\(n_2\\) ) with a vector field . Here the length of the vectors indicates the magnitude of change in population sizes of \\(n_1\\) and \\(n_2\\) . If you inspect this vector field, you can see that the dynamics approach a stable equilibrium near \\(n_1 = 1000, n_2 = 500\\) .","title":"Phase-planes and vector fields"},{"location":"lectures/lecture-6/#null-clines","text":"To better understand the dynamics, we can ask for what values of our variables ( \\(n_1, n_2\\) ) are our variables constant ( \\(\\Delta n_1 = 0\\) , \\(\\Delta n_2 = 0\\) ) \\(\\Longrightarrow\\) the coordinates/axes where the variables are constant are known as Null clines . Concretely, going back to our previous formula for the change in both the \\(n_1\\) and \\(n_2\\) species population sizes in the Lotka-Volterra model \\[ \\Delta n_1 = r_1n_1(t)(1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1}) \\] \\[ \\Delta n_2 = r_2n_2(t)(1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2}) \\] We want to know when \\(\\Delta n_1\\) and \\(\\Delta n_2\\) are 0. Solving for these inequalities shows that \\[ \\Delta n_1 = 0 \\Longrightarrow n_1(t) = 0, 1 - \\frac{n_1(t) + \\alpha_{12}n_2(t)}{K_1} = 0 \\] \\[ \\Delta n_2 = 0 \\Longrightarrow n_2(t) = 0, 1 - \\frac{n_2(t) + \\alpha_{21}n_1(t)}{K_2} = 0 \\] Plotting these null clines on the phase-plane diagram, we get # Initialize plot and ranges ax = plot_vector_field(dn1, dn2, axes_labels=[\"number of species 1, $n_1$\", \"number of species 2, $n_2$\"]) xrange, yrange = np.linspace(0, 1200, 100), np.linspace(0, 1200, 100) #plot the null clines for species 1 (blue) def plot_nullclines(ax): nullcline_1 = list(sympy.solve(sympy.Eq(dn1, 0))) ax.plot([nullcline_1[0] for i in range(len(xrange))], yrange, color='b') ax.plot([nullcline_1[1] for i in range(len(xrange))], yrange, color='b') # #plot the null clines for species 2 (red) nullcline_2 = [list(i.values())[0] for i in sympy.solve(sympy.Eq(dn2, 0))] ax.plot(sympy.lambdify(n2, nullcline_2[0])(yrange), yrange, color='r') # A this null cline is a function of n2 (i.e. y) ax.plot(xrange, [nullcline_2[1] for i in range(len(yrange))], color='r') ax.set_ylim(-10, 1210) ax.set_xlim(-10, 1210) return ax plot_nullclines(ax) We can also make phase diagrams for continuous-time models, just using the differential equations in place of the difference equations. We\u2019ll see an example of that for another model, of predator and prey, in this week\u2019s lab.","title":"Null clines"},{"location":"lectures/lecture-7/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 7: Equilibria Run notes interactively? Lecture overview Equilibria Exponential growth Logistic growth One-locus haploid selection One-locus diploid selection Summary 1. Equilibrium An equilibrium is any state of a system which tends to persist unchanged over time. For discrete time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. For example, those values of allele frequency p where \\[ \\Delta p = p(t+1) - p(t) = 0 \\newline \\Longrightarrow p(t+1) = p(t) \\] Similarly for continuous time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. For example, those values of allele frequency p where \\[ \\frac{dp}{dt} = 0 \\] What are the equilibria for the following models? Model Discrete time Continous time Exponential growth \\(N(t+1) = (1+r)N(t)\\) \\(\\frac{dN(t)}{dt} = rN(t)\\) Logistic growth \\(N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t)\\) \\(\\frac{dN}{dt} = r(1 - \\frac{N(t)}{K})N(t)\\) Haploid selection \\(p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a}\\) \\(\\frac{dp}{dt} = (r_A - r_a)p(1-p)\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t)q(t)W_{Aa}}{p(t)^2W_{AA} + p(t)q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived 2. Exponential growth Here, we will solve the equilibria for both discrete and continuous time exponential growth models. Discrete time Set \\(N(t+1) = N(t) = \\hat N\\) such that \\[ N(t+1) = (1+r)N(t) \\Longrightarrow \\hat N = (1 + r)\\hat N \\] now solve for \\(\\hat N\\) . \\[ \\hat N = (1 + r)\\hat N \\Longrightarrow \\hat N = 0 \\] Continuous time Set \\(N(t) = \\hat N\\) such that \\[ \\frac{dN(t)}{dt} = rN(t) \\] then set \\(dN(t)/dt = 0\\) and solve for \\(\\hat N\\) . \\[ 0 = r\\hat N \\Longrightarrow \\hat N = 0 \\] So the only equilibrium in both discrete time and continuous time exponential growth is extinction. Note that in both of these models \\(r = 0\\) also satisfies the conditions for an equilibrium. This is called a special case of parameters . Here this refers to the case where individuals perfectly replace themselves so that the population remains constant from any starting value. 3. Logistic growth Here, we will solve the equilibria for both discrete and continuous time logistic growth models. Discrete time \\[ N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t) \\] As above, we substitute \\(N(t+1) = N(t) = \\hat N\\) such that \\[ \\hat N = (1 + r(1 - \\frac{\\hat N}{K}))\\hat N \\] Notice that one equilibrium is \\(\\bf\\hat N = 0\\) . However, this isn't complete - because dividing both sides by \\(\\hat N\\) results in \\[ 1 = 1 + r(1 - \\frac{\\hat N}{K}) \\] \\[ 0 = r(1 - \\frac{\\hat N}{K}) \\] \\[ r = 0, 0 = 1 - \\frac{\\hat N}{K} \\] \\[ \\bf\\hat N = K \\] Continuous time \\[ \\frac{dN(t)}{dt} = r(1 - \\frac{N(t)}{K})N(t) \\] \\[ 0 = r(1 - \\frac{\\hat N}{K})\\hat N \\] \\[ \\Longrightarrow \\hat N = 0, \\hat N = K, \\text{ r = 0} \\] 4. One-locus haploid selection Here, we will solve the equilibria for both discrete and continuous time one-locus haploid selection models. Discrete time In the discrete model, we must find the value of \\(p(t)\\) for which \\(p(t + 1) = p(t)\\) . \\[ p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a} = p(t) \\] Replace \\(p(t)\\) and \\(q(t)\\) with \\(\\hat p\\) and \\(\\hat q\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\frac{\\hat pW_A}{\\hat pW_A + \\hat qW_a} = \\hat p \\] \\[ \\hat pW_A = \\hat p(\\hat pW_A + \\hat qW_a) \\] \\[ 0 = (\\hat pW_A + \\hat qW_a - W_A)\\hat p \\] \\[ 0 = (\\hat qW_a (1 - \\hat p)W_A)\\hat p \\] \\[ 0 = (\\hat qW_a - \\hat qW_A)\\hat p \\] \\[ 0 = (W_a - W_A)\\hat q \\hat p \\] Inspecting the last equation we note that we need either \\(\\hat p = 0 \\Longrightarrow\\) the population is \"fixed\" for the \\(a\\) allele \\(\\hat q = 0 \\Longrightarrow 1 - \\hat p = 0 \\Longrightarrow \\hat p = 1 \\Longrightarrow\\) the population is fixed for the \\(A\\) allele \\(W_A = W_a \\Longrightarrow\\) equal fitness, no selection, no evolution Continuous time In the continuous time model, we must set \\[ \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] equal to zero to solve for equilibria. Doing this shows that \\(dp/dt = 0\\) when \\(p\\) or \\(1 \u2212 p\\) equal zero (or for the special case of the parameters where \\(r_A = r_a\\) ), as in the discrete time model. 5. One-locus diploid selection Here, we will solve the equilibria for the discrete one-locus diploid selection models. Discrete time To determine the equilibria for the diploid-selection model in discrete time: \\[ p(t+1) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] \\[ p(t) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] \\[ \\hat{p} = \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\] \\[ {\\bf{\\hat{p} = 0}}, 1 = \\frac{\\hat{p} W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\] \\[ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} = \\hat{p} W_{AA} + \\hat{q} W_{Aa} \\] \\[ (\\hat{p}^2 - \\hat{p}) W_{AA} + (2 \\hat{p} \\hat{q} - \\hat{q}) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ \\hat{p}(\\hat{p} - 1) W_{AA} + \\hat{q}(2 \\hat{p} - 1) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ \\hat{p}\\hat{q} W_{AA} + \\hat{q}(\\hat{p}-\\hat{q}) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ {\\bf{\\hat{q}=0}}, -\\hat{p} W_{AA} + (\\hat{p}-\\hat{q}) W_{Aa} + \\hat{q} W_{aa} = 0 \\] So up until this point, we have two equilibria \\(\\hat p = 0, \\hat q = 0\\) and one remaining inequality - let's rearrange and simplify this final inequality. \\[ \\hat{p} W_{AA} + (2\\hat{p}-1) W_{Aa} + (1-\\hat{p}) W_{aa} = 0 \\] \\[ \\hat{p}(-W_{AA} + 2 W_{Aa} - W_{aa}) + - W_{Aa} + W_{aa} = 0 \\] \\[ \\hat{p}(-W_{AA} + 2 W_{Aa} - W_{aa}) = W_{Aa} - W_{aa} \\] \\[ \\bf{\\hat{p} = \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}}} \\] Since \\(0 \\leq p \\leq 1\\) , we call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) \"boundary\" equilibria. This also implies the third equilibrium is only biologically valid when \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1 \\] When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an \"internal\" equilibrium, representing a population with both A and a alleles, when \\[ 0 < \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} < 1 \\] The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). So if \\(W_{Aa} > W_{aa}\\) we also need \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) (case A) and if \\(W_{Aa} < W_{aa}\\) we need \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) (case B). Now we can rearrange the equilibrium to show that it is less than 1 when \\[ 0 < \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} \\] Again we need the numerator and denominator to have the same sign. In case A this mean we need \\(W_{Aa} > W_{AA}\\) and in case B we need \\(W_{Aa} < W_{AA}\\) . Putting this altogether, there is a biologically relevant internal equilibrium when either \\(W_{Aa} > W_{aa}\\) and \\(W_{Aa} > W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) ) \\(W_{Aa} < W_{aa}\\) and \\(W_{Aa} < W_{AA}\\) (which ensure \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) ) 6. Summary In summary, the equilibria for the models we have looked at are: Model Discrete time equilibria Continuous time equilibria Exponential growth \\(\\hat n = 0\\) \\(\\hat n = 0\\) Logistic growth \\(\\hat n = 0, \\hat n = K\\) \\(\\hat n = 0, \\hat n = K\\) Haploid selection \\(\\hat p = 0, \\hat p = 1\\) \\(\\hat p = 0, \\hat p = 1\\) Diploid selection \\(\\hat p = 0, \\hat p = 1, \\hat p = \\frac{W_{Aa} - W{aa}}{2W_{Aa} - W_{AA} - W_{AA}}\\) Not derived Make sure that you understand how to determine equilibria and can derive each of the above equilibria.","title":"Lecture 7"},{"location":"lectures/lecture-7/#lecture-7-equilibria","text":"Run notes interactively?","title":"Lecture 7: Equilibria"},{"location":"lectures/lecture-7/#lecture-overview","text":"Equilibria Exponential growth Logistic growth One-locus haploid selection One-locus diploid selection Summary","title":"Lecture overview"},{"location":"lectures/lecture-7/#1-equilibrium","text":"An equilibrium is any state of a system which tends to persist unchanged over time. For discrete time models, the equilibria are defined as those values of the variables where no changes occur from one time step to the next. For example, those values of allele frequency p where \\[ \\Delta p = p(t+1) - p(t) = 0 \\newline \\Longrightarrow p(t+1) = p(t) \\] Similarly for continuous time models, the equilibria are defined as those values of the variables for which the rate of change in the variables equals zero. For example, those values of allele frequency p where \\[ \\frac{dp}{dt} = 0 \\] What are the equilibria for the following models? Model Discrete time Continous time Exponential growth \\(N(t+1) = (1+r)N(t)\\) \\(\\frac{dN(t)}{dt} = rN(t)\\) Logistic growth \\(N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t)\\) \\(\\frac{dN}{dt} = r(1 - \\frac{N(t)}{K})N(t)\\) Haploid selection \\(p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a}\\) \\(\\frac{dp}{dt} = (r_A - r_a)p(1-p)\\) Diploid selection \\(p(t+1) = \\frac{p(t)^2W_{AA} + p(t)q(t)W_{Aa}}{p(t)^2W_{AA} + p(t)q(t)W_{Aa} + q(t)^2W_{aa}}\\) Not derived","title":"1. Equilibrium"},{"location":"lectures/lecture-7/#2-exponential-growth","text":"Here, we will solve the equilibria for both discrete and continuous time exponential growth models.","title":"2. Exponential growth"},{"location":"lectures/lecture-7/#discrete-time","text":"Set \\(N(t+1) = N(t) = \\hat N\\) such that \\[ N(t+1) = (1+r)N(t) \\Longrightarrow \\hat N = (1 + r)\\hat N \\] now solve for \\(\\hat N\\) . \\[ \\hat N = (1 + r)\\hat N \\Longrightarrow \\hat N = 0 \\]","title":"Discrete time"},{"location":"lectures/lecture-7/#continuous-time","text":"Set \\(N(t) = \\hat N\\) such that \\[ \\frac{dN(t)}{dt} = rN(t) \\] then set \\(dN(t)/dt = 0\\) and solve for \\(\\hat N\\) . \\[ 0 = r\\hat N \\Longrightarrow \\hat N = 0 \\] So the only equilibrium in both discrete time and continuous time exponential growth is extinction. Note that in both of these models \\(r = 0\\) also satisfies the conditions for an equilibrium. This is called a special case of parameters . Here this refers to the case where individuals perfectly replace themselves so that the population remains constant from any starting value.","title":"Continuous time"},{"location":"lectures/lecture-7/#3-logistic-growth","text":"Here, we will solve the equilibria for both discrete and continuous time logistic growth models.","title":"3. Logistic growth"},{"location":"lectures/lecture-7/#discrete-time_1","text":"\\[ N(t+1) = (1 + r(1 - \\frac{N(t)}{K}))N(t) \\] As above, we substitute \\(N(t+1) = N(t) = \\hat N\\) such that \\[ \\hat N = (1 + r(1 - \\frac{\\hat N}{K}))\\hat N \\] Notice that one equilibrium is \\(\\bf\\hat N = 0\\) . However, this isn't complete - because dividing both sides by \\(\\hat N\\) results in \\[ 1 = 1 + r(1 - \\frac{\\hat N}{K}) \\] \\[ 0 = r(1 - \\frac{\\hat N}{K}) \\] \\[ r = 0, 0 = 1 - \\frac{\\hat N}{K} \\] \\[ \\bf\\hat N = K \\]","title":"Discrete time"},{"location":"lectures/lecture-7/#continuous-time_1","text":"\\[ \\frac{dN(t)}{dt} = r(1 - \\frac{N(t)}{K})N(t) \\] \\[ 0 = r(1 - \\frac{\\hat N}{K})\\hat N \\] \\[ \\Longrightarrow \\hat N = 0, \\hat N = K, \\text{ r = 0} \\]","title":"Continuous time"},{"location":"lectures/lecture-7/#4-one-locus-haploid-selection","text":"Here, we will solve the equilibria for both discrete and continuous time one-locus haploid selection models.","title":"4. One-locus haploid selection"},{"location":"lectures/lecture-7/#discrete-time_2","text":"In the discrete model, we must find the value of \\(p(t)\\) for which \\(p(t + 1) = p(t)\\) . \\[ p(t+1) = \\frac{p(t)W_A}{p(t)W_A + q(t)W_a} = p(t) \\] Replace \\(p(t)\\) and \\(q(t)\\) with \\(\\hat p\\) and \\(\\hat q\\) and solve for \\(\\hat p\\) and \\(\\hat q\\) \\[ \\frac{\\hat pW_A}{\\hat pW_A + \\hat qW_a} = \\hat p \\] \\[ \\hat pW_A = \\hat p(\\hat pW_A + \\hat qW_a) \\] \\[ 0 = (\\hat pW_A + \\hat qW_a - W_A)\\hat p \\] \\[ 0 = (\\hat qW_a (1 - \\hat p)W_A)\\hat p \\] \\[ 0 = (\\hat qW_a - \\hat qW_A)\\hat p \\] \\[ 0 = (W_a - W_A)\\hat q \\hat p \\] Inspecting the last equation we note that we need either \\(\\hat p = 0 \\Longrightarrow\\) the population is \"fixed\" for the \\(a\\) allele \\(\\hat q = 0 \\Longrightarrow 1 - \\hat p = 0 \\Longrightarrow \\hat p = 1 \\Longrightarrow\\) the population is fixed for the \\(A\\) allele \\(W_A = W_a \\Longrightarrow\\) equal fitness, no selection, no evolution","title":"Discrete time"},{"location":"lectures/lecture-7/#continuous-time_2","text":"In the continuous time model, we must set \\[ \\frac{dp}{dt} = (r_A - r_a)p(1-p) \\] equal to zero to solve for equilibria. Doing this shows that \\(dp/dt = 0\\) when \\(p\\) or \\(1 \u2212 p\\) equal zero (or for the special case of the parameters where \\(r_A = r_a\\) ), as in the discrete time model.","title":"Continuous time"},{"location":"lectures/lecture-7/#5-one-locus-diploid-selection","text":"Here, we will solve the equilibria for the discrete one-locus diploid selection models.","title":"5. One-locus diploid selection"},{"location":"lectures/lecture-7/#discrete-time_3","text":"To determine the equilibria for the diploid-selection model in discrete time: \\[ p(t+1) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] \\[ p(t) = \\frac{p(t)^2 W_{AA} + p(t) q(t) W_{Aa}}{p(t)^2 W_{AA} + 2 p(t) q(t) W_{Aa} + q(t)^2 W_{aa}} \\] \\[ \\hat{p} = \\frac{\\hat{p}^2 W_{AA} + \\hat{p} \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\] \\[ {\\bf{\\hat{p} = 0}}, 1 = \\frac{\\hat{p} W_{AA} + \\hat{q} W_{Aa}}{\\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa}} \\] \\[ \\hat{p}^2 W_{AA} + 2 \\hat{p} \\hat{q} W_{Aa} + \\hat{q}^2 W_{aa} = \\hat{p} W_{AA} + \\hat{q} W_{Aa} \\] \\[ (\\hat{p}^2 - \\hat{p}) W_{AA} + (2 \\hat{p} \\hat{q} - \\hat{q}) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ \\hat{p}(\\hat{p} - 1) W_{AA} + \\hat{q}(2 \\hat{p} - 1) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ \\hat{p}\\hat{q} W_{AA} + \\hat{q}(\\hat{p}-\\hat{q}) W_{Aa} + \\hat{q}^2 W_{aa} = 0 \\] \\[ {\\bf{\\hat{q}=0}}, -\\hat{p} W_{AA} + (\\hat{p}-\\hat{q}) W_{Aa} + \\hat{q} W_{aa} = 0 \\] So up until this point, we have two equilibria \\(\\hat p = 0, \\hat q = 0\\) and one remaining inequality - let's rearrange and simplify this final inequality. \\[ \\hat{p} W_{AA} + (2\\hat{p}-1) W_{Aa} + (1-\\hat{p}) W_{aa} = 0 \\] \\[ \\hat{p}(-W_{AA} + 2 W_{Aa} - W_{aa}) + - W_{Aa} + W_{aa} = 0 \\] \\[ \\hat{p}(-W_{AA} + 2 W_{Aa} - W_{aa}) = W_{Aa} - W_{aa} \\] \\[ \\bf{\\hat{p} = \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}}} \\] Since \\(0 \\leq p \\leq 1\\) , we call \\(\\hat{p}=0\\) and \\(\\hat{p}=1\\) \"boundary\" equilibria. This also implies the third equilibrium is only biologically valid when \\[ 0 \\leq \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} \\leq 1 \\] When \\(W_{Aa} = W_{aa}\\) this equilibrium reduces to \\(\\hat{p}=0\\) and when \\(W_{Aa} = W_{AA}\\) this reduces to \\(\\hat{p}=1\\) (check this for yourself). The third equilibrium will be an \"internal\" equilibrium, representing a population with both A and a alleles, when \\[ 0 < \\frac{W_{Aa} - W_{aa}}{2 W_{Aa} -W_{AA} - W_{aa}} < 1 \\] The equilibrium is positive when the numerator and denominator have the same sign (i.e., are both positive or both negative). So if \\(W_{Aa} > W_{aa}\\) we also need \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) (case A) and if \\(W_{Aa} < W_{aa}\\) we need \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) (case B). Now we can rearrange the equilibrium to show that it is less than 1 when \\[ 0 < \\frac{W_{Aa} - W_{AA}}{2 W_{Aa} -W_{AA} - W_{aa}} \\] Again we need the numerator and denominator to have the same sign. In case A this mean we need \\(W_{Aa} > W_{AA}\\) and in case B we need \\(W_{Aa} < W_{AA}\\) . Putting this altogether, there is a biologically relevant internal equilibrium when either \\(W_{Aa} > W_{aa}\\) and \\(W_{Aa} > W_{AA}\\) (which ensures \\(2 W_{Aa} - W_{AA} - W_{aa} > 0\\) ) \\(W_{Aa} < W_{aa}\\) and \\(W_{Aa} < W_{AA}\\) (which ensure \\(2 W_{Aa} - W_{AA} - W_{aa} < 0\\) )","title":"Discrete time"},{"location":"lectures/lecture-7/#6-summary","text":"In summary, the equilibria for the models we have looked at are: Model Discrete time equilibria Continuous time equilibria Exponential growth \\(\\hat n = 0\\) \\(\\hat n = 0\\) Logistic growth \\(\\hat n = 0, \\hat n = K\\) \\(\\hat n = 0, \\hat n = K\\) Haploid selection \\(\\hat p = 0, \\hat p = 1\\) \\(\\hat p = 0, \\hat p = 1\\) Diploid selection \\(\\hat p = 0, \\hat p = 1, \\hat p = \\frac{W_{Aa} - W{aa}}{2W_{Aa} - W_{AA} - W_{AA}}\\) Not derived Make sure that you understand how to determine equilibria and can derive each of the above equilibria.","title":"6. Summary"},{"location":"lectures/lecture-8/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 8: Stability Run notes interactively? Lecture overview Stability Stability analysis in discrete-time one-variable models Logistic growth (discrete-time) Stability analysis in continuous-time one-variable models Logistic growth (continuous time) Summary 1. Stability What is stability and stability analysis? What happens very near an equilibrium? Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable . In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable . An equilibrium point is said to be globally stable if all initial conditions lead to it. Our goal : To determine whether a small perturbation away from an equilibrium point will grow or shrink in magnitude over time \\(\\Longrightarrow\\) local stability analysis Example Consider logistic growth in discrete time, with \\(K = 1000\\) and \\(r = 0.5\\) . For populations started near carrying capacity (e.g., \\(n(t) = 1100\\) ), the slope of the recursion equation predicts whether the system will move closer to or further away from the equilibrium. import numpy as np import matplotlib.pyplot as plt # Reproductive factor for logistic growth def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < = max: yield t, nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Build cobweb plotting function def cobweb_logistic(n0, r, K, max=np.inf): t, nnow, nnext = 0, n0, 0 #initial conditions while t < = max: yield nnow, nnext #current value of p(t) and p(t+1) nnext = nnow + nnow * r * (1 - nnow/K) yield nnow, nnext #current value of p(t) and p(t+1) nnow = nnext #update p(t) t += 1 #update t def plot_logistic_with_cobweb(r, k, n_cob, ax=None, max=(50,50,50)): # Generate curves of N(t) and N(t+1) if r >= 0: Nb = np.array([i[1] for i in n(1, r, k, max=max[0])]) Na = np.array([i[1] for i in n(k + k * 0.4, r, k, max=max[1])]) if r < 0: Nb = np.array([i[1] for i in n(999, r, k, max=max[0])]) Na = np.array([i[1] for i in n(k + 2, r, k, max=max[1])]) # Plot the curves (add an additional curve past equilibrium to show stability) if ax == None: fig, ax = plt.subplots() ax.plot(Nb[0:-1], Nb[1:], color='black', label=f\"r = {r}, K = {k}\") ax.plot(Na[0:-1], Na[1:], color='black') ax.plot(np.linspace(0, np.max(Na), 1000), np.linspace(0, np.max(Na), 1000), color='black', linestyle='dotted') ax.scatter([0, k], [0, k], s=150, color='black', alpha=1) # Add cobweb cobweb = np.array([i for i in cobweb_logistic(n_cob, r, k, max=max[2])]) plt.plot(cobweb[:,0], cobweb[:,1], color='blue') ax.legend(frameon=False) ax.set_xlim(0, k + k *0.4) ax.set_ylim(0, k + k *0.4) return ax plot_logistic_with_cobweb(r=0.5, k=1000, n_cob=1100) The slope of the recursion at \\(n = K\\) is 0.5, which is between 0 and 1. Now what about \\(r = -0.5\\) ? plot_logistic_with_cobweb(r=-0.5, k=1000, n_cob=1100, max=(100,20,4)) The slope of the recursion at \\(n = K\\) is 1.5, which is greater than 1. Mathematical aside: Taylor series \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] where \\(f^{(k)}(a)\\) is the \\(k^{th}\\) derivative of the function with respect to \\(x\\) , evaluated at point \\(a\\) . (See section P1.3 in the text for more information). So how does knowing the slope at equilibrium help? (e.g. at \\(n = K\\) ). If we have an equilibrium point, call it \\(a\\) , and we have a starting condition, \\(x\\) , which is near a, then we can use the Taylor Series to rewrite the equations that describe how the system changes over time. If we start near enough to the equilibrium, \\((x - a)^k\\) will be tiny for \\(k > 1\\) and the function will be dominated by the first two terms in the sum with \\(k = 0\\) and \\(k = 1\\) : \\[ f (x) \\simeq f (a) + f'(a)(x - a) \\] using \\(f^{(0)}(a) = f(a)\\) and the short-hand notation \\(f'(a) = f^{(1)}(a)\\) . This is great! No matter how complicated and non-linear an equation we have, we can get an approximate equation that describes the dynamics near an equilibrium point. This equation is linear in \\(x\\) and can be easily analysed. 2. Stability analysis in discrete-time one-variable models Given a recursion equation in one variable, \\(x(t+1) = f(x(t))\\) , with an equilibrium \\(\\hat{x}\\) , when will a small perturbation ( \\(\\epsilon\\) ) away from the equilibrium grow in magnitude over time? At time \\(t\\) , say that the population is a small distance from the equilibrium: \\(\\hat{x} + \\epsilon(t)\\) (Note that \\(\\epsilon(t)\\) might be negative). At time \\(t+1\\) , the population will be at \\(\\hat{x} + \\epsilon(t+1)\\) , which equals \\(f(\\hat{x} + \\epsilon(t))\\) . Using the Taylor Series of this function: \\[ \\hat{x} + \\epsilon(t+1) = f(\\hat{x} + \\epsilon(t)) \\newline \\] \\[ \\hat{x} + \\epsilon(t+1) \\approx f(\\hat{x}) + f'(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x}) {\\text{ Taylor Series around } \\hat{x}} \\newline \\] \\[ \\hat{x} + \\epsilon(t+1) = f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\] We know that \\(f(\\hat{x}) = \\hat{x}\\) because \\(\\hat{x}\\) is an equilibrium. \\[ \\implies \\epsilon(t+1) \\simeq f'(\\hat{x})\\epsilon(t) \\] This is the recursion for exponential growth, with reproductive factor \\(\\lambda = f'(\\hat{x})\\) . So knowing that \\[ \\epsilon (t+1) \\simeq \\lambda \\epsilon (t) \\] The perturbation will: move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative grow if \\(\\lambda<-1\\) ( \\(\\hat{x}\\) unstable) or shrink if \\(-1<\\lambda<0\\) ( \\(\\hat{x}\\) locally stable). stay on the same side of the equilibrium if \\(\\lambda\\) is positive and shrink if \\(0<\\lambda<1\\) ( \\(\\hat{x}\\) locally stable). or grow if \\(1<\\lambda\\) ( \\(\\hat{x}\\) unstable) 3. Logistic growth (discrete-time) Let's now return to the logistic growth model in discrete time \\[ f(n) = n + rn(1 - \\frac{n}{K}) \\] When \\(r = 0.5\\) we saw that \\(\\hat n = K\\) was locally stable and the slope of the recusion at \\(n = K\\) , \\(\\lambda = f'(K)\\) , was between 0 and 1. Namely, plotting the same data again below plot_logistic_with_cobweb(r=0.5, k=1000, n_cob=1100) Whereas, when \\(r = -0.5\\) we saw that \\(\\hat n = K\\) was unstable and the slope of the recursion at \\(n = K\\) , \\(\\lambda = f'(K)\\) , was greater than 1, as shown in the plot below. plot_logistic_with_cobweb(r=-0.5, k=1000, n_cob=1100, max=(100,20,4)) Now let's perform a local stability analysis to see, more generally, when \\(\\hat{n}=K\\) is locally stable. \\[ f(n) = n + r n \\left(1 - \\frac{n}{K}\\right) \\] We first take the derivative of \\(f\\) with respect to \\(n\\) \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] Then we plug in \\(n=K\\) \\[ f'(K) = 1 + r - 2 r = 1 - r \\] This will be negative when \\(r > 1\\) , creating oscillations. The equilibrium will be stable when \\(-1 < 1 - r < 1 \\implies 0 < r < 2\\) . This is consistent with the cob-webs we just observed, as well as the bifurcation diagram we saw in the lab (run the code below to see the bifurcation diagram again). # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt[1] for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white') 4. Stability analysis in continuous-time one-variable models Now consider logistic growth in continuous-time with \\(r = 0.5\\) and \\(K = 1000\\) . Last week we saw that the slope of the differential equation at the equilibrium determined stability Below, in the plot on the left, the slope of the differential equation at \\(n = K\\) is -0.5 which is less than 0. In the plot on the right, the slope of the differential equation at n = K is 0.5, which is greater than 0. Again we focus on a small perturbation ( \\(\\epsilon\\) ) away from an equilibrium ( \\(\\hat{x}\\) ) and determine whether this perturbation will grow or shrink. If, at time \\(t\\) , the population is a small distance from equilibrium: \\(\\hat{x} + \\epsilon (t)\\) , the rate of change in \\(x\\) will be \\(\\mathrm{d}x/\\mathrm{d}t = \\mathrm{d}(\\hat{x} + \\epsilon (t))/\\mathrm{d}t\\) . In this case, \\(\\mathrm{d}x/\\mathrm{d}t\\) is the function that we wish to approximate using the Taylor Series: \\[ \\frac{\\mathrm{d}(\\hat{x} + \\epsilon (t))}{\\mathrm{d}t} = f(\\hat{x} + \\epsilon (t)) \\] \\[ \\simeq f(\\hat{x}) + f'(\\hat{x})(\\hat{x} + \\epsilon (t) - \\hat{x}) {\\text{ Taylor Series around } \\hat{x}} \\] \\[ = f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\] Now, \\(f(\\hat{x})\\) equals zero, since \\(\\hat{x}\\) is an equilibrium. Furthermore, by the linearity property of derivatives: \\[ \\frac{\\mathrm{d}(\\hat{x}+\\epsilon (t))}{\\mathrm{d}t} = \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t} + \\frac{\\mathrm{d}\\epsilon (t)}{\\mathrm{d}t} = \\frac{\\mathrm{d}\\epsilon (t)} {\\mathrm{d}t} \\] Combining the above, the perturbation will change over time at a rate \\[ \\frac{\\mathrm{d}(\\hat{x} + \\epsilon (t))}{\\mathrm{d}t} \\simeq f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\hspace{1em} \\rightarrow \\hspace{1em} \\frac{\\mathrm{d} \\epsilon(t)}{\\mathrm{d}t} \\simeq f'(\\hat{x}) \\epsilon(t) \\] This is the same as exponential growth with growth rate \\(r=f'(\\hat{x})\\) . So what will the perturbation do then? Well, note that (as shown above) that the change in the perturbation is equal to \\[ \\frac{\\mathrm{d} \\epsilon(t)}{\\mathrm{d}t} \\simeq r \\epsilon(t) \\] The perturbation will: grow if \\(r>0\\) ( \\(\\hat{x}\\) unstable) shrink if \\(r<0\\) ( \\(\\hat{x}\\) locally stable) Unlike in discrete time, there is no possibility for oscillations here. 5. Logistic growth (continuous time) Let's again look at the model of logistic growth \\[ f(n)=\\frac{\\mathrm{d}n}{\\mathrm{d}t} = rn \\left(1 - \\frac{n}{K}\\right) \\] The derivative of \\(f\\) with respect to \\(n\\) is \\[ f'(n) = r - 2 r\\frac{n}{K} \\] Plugging in \\(n=K\\) gives \\[ f'(K) = r - 2 r = -r \\] This implies that an \\(r>0\\) causes local stability of \\(\\hat{n}=K\\) , consistent with our graphical analysis. 6. Summary Local stability analysis for discrete- and continuous-time models. take the derivative of the recursion/differential equation with respect to the variable, \\(f'(x)\\) plug in the equilibrium value of the variable, \\(f'(\\hat x)\\) determine the sign and magnitude (sign only in continuous-time)","title":"Lecture 8"},{"location":"lectures/lecture-8/#lecture-8-stability","text":"Run notes interactively?","title":"Lecture 8: Stability"},{"location":"lectures/lecture-8/#lecture-overview","text":"Stability Stability analysis in discrete-time one-variable models Logistic growth (discrete-time) Stability analysis in continuous-time one-variable models Logistic growth (continuous time) Summary","title":"Lecture overview"},{"location":"lectures/lecture-8/#1-stability","text":"","title":"1. Stability"},{"location":"lectures/lecture-8/#what-is-stability-and-stability-analysis","text":"What happens very near an equilibrium? Starting near an equilibrium, if the system moves towards the equilibrium over time the equilibrium is said to be locally stable . In contrast, if the system moves away from the equilibrium over time the equilibrium is said to be unstable . An equilibrium point is said to be globally stable if all initial conditions lead to it. Our goal : To determine whether a small perturbation away from an equilibrium point will grow or shrink in magnitude over time \\(\\Longrightarrow\\) local stability analysis","title":"What is stability and stability analysis?"},{"location":"lectures/lecture-8/#example","text":"Consider logistic growth in discrete time, with \\(K = 1000\\) and \\(r = 0.5\\) . For populations started near carrying capacity (e.g., \\(n(t) = 1100\\) ), the slope of the recursion equation predicts whether the system will move closer to or further away from the equilibrium. import numpy as np import matplotlib.pyplot as plt # Reproductive factor for logistic growth def n(n0, r, k, max=np.inf): t, nt = 0, n0 while t < = max: yield t, nt t, nt = t + 1, nt + r * nt * (1 - nt / k) # Build cobweb plotting function def cobweb_logistic(n0, r, K, max=np.inf): t, nnow, nnext = 0, n0, 0 #initial conditions while t < = max: yield nnow, nnext #current value of p(t) and p(t+1) nnext = nnow + nnow * r * (1 - nnow/K) yield nnow, nnext #current value of p(t) and p(t+1) nnow = nnext #update p(t) t += 1 #update t def plot_logistic_with_cobweb(r, k, n_cob, ax=None, max=(50,50,50)): # Generate curves of N(t) and N(t+1) if r >= 0: Nb = np.array([i[1] for i in n(1, r, k, max=max[0])]) Na = np.array([i[1] for i in n(k + k * 0.4, r, k, max=max[1])]) if r < 0: Nb = np.array([i[1] for i in n(999, r, k, max=max[0])]) Na = np.array([i[1] for i in n(k + 2, r, k, max=max[1])]) # Plot the curves (add an additional curve past equilibrium to show stability) if ax == None: fig, ax = plt.subplots() ax.plot(Nb[0:-1], Nb[1:], color='black', label=f\"r = {r}, K = {k}\") ax.plot(Na[0:-1], Na[1:], color='black') ax.plot(np.linspace(0, np.max(Na), 1000), np.linspace(0, np.max(Na), 1000), color='black', linestyle='dotted') ax.scatter([0, k], [0, k], s=150, color='black', alpha=1) # Add cobweb cobweb = np.array([i for i in cobweb_logistic(n_cob, r, k, max=max[2])]) plt.plot(cobweb[:,0], cobweb[:,1], color='blue') ax.legend(frameon=False) ax.set_xlim(0, k + k *0.4) ax.set_ylim(0, k + k *0.4) return ax plot_logistic_with_cobweb(r=0.5, k=1000, n_cob=1100) The slope of the recursion at \\(n = K\\) is 0.5, which is between 0 and 1. Now what about \\(r = -0.5\\) ? plot_logistic_with_cobweb(r=-0.5, k=1000, n_cob=1100, max=(100,20,4)) The slope of the recursion at \\(n = K\\) is 1.5, which is greater than 1. Mathematical aside: Taylor series \\[ f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k \\] where \\(f^{(k)}(a)\\) is the \\(k^{th}\\) derivative of the function with respect to \\(x\\) , evaluated at point \\(a\\) . (See section P1.3 in the text for more information). So how does knowing the slope at equilibrium help? (e.g. at \\(n = K\\) ). If we have an equilibrium point, call it \\(a\\) , and we have a starting condition, \\(x\\) , which is near a, then we can use the Taylor Series to rewrite the equations that describe how the system changes over time. If we start near enough to the equilibrium, \\((x - a)^k\\) will be tiny for \\(k > 1\\) and the function will be dominated by the first two terms in the sum with \\(k = 0\\) and \\(k = 1\\) : \\[ f (x) \\simeq f (a) + f'(a)(x - a) \\] using \\(f^{(0)}(a) = f(a)\\) and the short-hand notation \\(f'(a) = f^{(1)}(a)\\) . This is great! No matter how complicated and non-linear an equation we have, we can get an approximate equation that describes the dynamics near an equilibrium point. This equation is linear in \\(x\\) and can be easily analysed.","title":"Example"},{"location":"lectures/lecture-8/#2-stability-analysis-in-discrete-time-one-variable-models","text":"Given a recursion equation in one variable, \\(x(t+1) = f(x(t))\\) , with an equilibrium \\(\\hat{x}\\) , when will a small perturbation ( \\(\\epsilon\\) ) away from the equilibrium grow in magnitude over time? At time \\(t\\) , say that the population is a small distance from the equilibrium: \\(\\hat{x} + \\epsilon(t)\\) (Note that \\(\\epsilon(t)\\) might be negative). At time \\(t+1\\) , the population will be at \\(\\hat{x} + \\epsilon(t+1)\\) , which equals \\(f(\\hat{x} + \\epsilon(t))\\) . Using the Taylor Series of this function: \\[ \\hat{x} + \\epsilon(t+1) = f(\\hat{x} + \\epsilon(t)) \\newline \\] \\[ \\hat{x} + \\epsilon(t+1) \\approx f(\\hat{x}) + f'(\\hat{x})(\\hat{x} + \\epsilon(t) - \\hat{x}) {\\text{ Taylor Series around } \\hat{x}} \\newline \\] \\[ \\hat{x} + \\epsilon(t+1) = f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\] We know that \\(f(\\hat{x}) = \\hat{x}\\) because \\(\\hat{x}\\) is an equilibrium. \\[ \\implies \\epsilon(t+1) \\simeq f'(\\hat{x})\\epsilon(t) \\] This is the recursion for exponential growth, with reproductive factor \\(\\lambda = f'(\\hat{x})\\) . So knowing that \\[ \\epsilon (t+1) \\simeq \\lambda \\epsilon (t) \\] The perturbation will: move from one side of the equilibrium to the other (i.e., oscillate) if \\(\\lambda\\) is negative grow if \\(\\lambda<-1\\) ( \\(\\hat{x}\\) unstable) or shrink if \\(-1<\\lambda<0\\) ( \\(\\hat{x}\\) locally stable). stay on the same side of the equilibrium if \\(\\lambda\\) is positive and shrink if \\(0<\\lambda<1\\) ( \\(\\hat{x}\\) locally stable). or grow if \\(1<\\lambda\\) ( \\(\\hat{x}\\) unstable)","title":"2. Stability analysis in discrete-time one-variable models"},{"location":"lectures/lecture-8/#3-logistic-growth-discrete-time","text":"Let's now return to the logistic growth model in discrete time \\[ f(n) = n + rn(1 - \\frac{n}{K}) \\] When \\(r = 0.5\\) we saw that \\(\\hat n = K\\) was locally stable and the slope of the recusion at \\(n = K\\) , \\(\\lambda = f'(K)\\) , was between 0 and 1. Namely, plotting the same data again below plot_logistic_with_cobweb(r=0.5, k=1000, n_cob=1100) Whereas, when \\(r = -0.5\\) we saw that \\(\\hat n = K\\) was unstable and the slope of the recursion at \\(n = K\\) , \\(\\lambda = f'(K)\\) , was greater than 1, as shown in the plot below. plot_logistic_with_cobweb(r=-0.5, k=1000, n_cob=1100, max=(100,20,4)) Now let's perform a local stability analysis to see, more generally, when \\(\\hat{n}=K\\) is locally stable. \\[ f(n) = n + r n \\left(1 - \\frac{n}{K}\\right) \\] We first take the derivative of \\(f\\) with respect to \\(n\\) \\[ f'(n) = 1 + r - 2 r \\frac{n}{K} \\] Then we plug in \\(n=K\\) \\[ f'(K) = 1 + r - 2 r = 1 - r \\] This will be negative when \\(r > 1\\) , creating oscillations. The equilibrium will be stable when \\(-1 < 1 - r < 1 \\implies 0 < r < 2\\) . This is consistent with the cob-webs we just observed, as well as the bifurcation diagram we saw in the lab (run the code below to see the bifurcation diagram again). # Sample the periodicity of the oscillations # by taking unique values after reaching carrying capacity def log_map(r, n0=900, k=1000): return np.unique([nt[1] for t, nt in enumerate(n(n0, r, k, max=75)) if t > 30]) # Compute the logistic map for different growth rates in discrete time r, Nr = np.array([]), np.array([]) for i in np.linspace(1.5, 3, 1000): nl = log_map(i) r = np.hstack((r, [i for _ in range(len(nl))])) Nr = np.hstack((Nr, nl)) # Plot the logistic map on a black background fig, ax = plt.subplots() ax.patch.set_facecolor('black') ax.scatter(r, Nr, s=0.075, color='white')","title":"3. Logistic growth (discrete-time)"},{"location":"lectures/lecture-8/#4-stability-analysis-in-continuous-time-one-variable-models","text":"Now consider logistic growth in continuous-time with \\(r = 0.5\\) and \\(K = 1000\\) . Last week we saw that the slope of the differential equation at the equilibrium determined stability Below, in the plot on the left, the slope of the differential equation at \\(n = K\\) is -0.5 which is less than 0. In the plot on the right, the slope of the differential equation at n = K is 0.5, which is greater than 0. Again we focus on a small perturbation ( \\(\\epsilon\\) ) away from an equilibrium ( \\(\\hat{x}\\) ) and determine whether this perturbation will grow or shrink. If, at time \\(t\\) , the population is a small distance from equilibrium: \\(\\hat{x} + \\epsilon (t)\\) , the rate of change in \\(x\\) will be \\(\\mathrm{d}x/\\mathrm{d}t = \\mathrm{d}(\\hat{x} + \\epsilon (t))/\\mathrm{d}t\\) . In this case, \\(\\mathrm{d}x/\\mathrm{d}t\\) is the function that we wish to approximate using the Taylor Series: \\[ \\frac{\\mathrm{d}(\\hat{x} + \\epsilon (t))}{\\mathrm{d}t} = f(\\hat{x} + \\epsilon (t)) \\] \\[ \\simeq f(\\hat{x}) + f'(\\hat{x})(\\hat{x} + \\epsilon (t) - \\hat{x}) {\\text{ Taylor Series around } \\hat{x}} \\] \\[ = f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\] Now, \\(f(\\hat{x})\\) equals zero, since \\(\\hat{x}\\) is an equilibrium. Furthermore, by the linearity property of derivatives: \\[ \\frac{\\mathrm{d}(\\hat{x}+\\epsilon (t))}{\\mathrm{d}t} = \\frac{\\mathrm{d}\\hat{x}}{\\mathrm{d}t} + \\frac{\\mathrm{d}\\epsilon (t)}{\\mathrm{d}t} = \\frac{\\mathrm{d}\\epsilon (t)} {\\mathrm{d}t} \\] Combining the above, the perturbation will change over time at a rate \\[ \\frac{\\mathrm{d}(\\hat{x} + \\epsilon (t))}{\\mathrm{d}t} \\simeq f(\\hat{x}) + f'(\\hat{x})\\epsilon(t) \\hspace{1em} \\rightarrow \\hspace{1em} \\frac{\\mathrm{d} \\epsilon(t)}{\\mathrm{d}t} \\simeq f'(\\hat{x}) \\epsilon(t) \\] This is the same as exponential growth with growth rate \\(r=f'(\\hat{x})\\) . So what will the perturbation do then? Well, note that (as shown above) that the change in the perturbation is equal to \\[ \\frac{\\mathrm{d} \\epsilon(t)}{\\mathrm{d}t} \\simeq r \\epsilon(t) \\] The perturbation will: grow if \\(r>0\\) ( \\(\\hat{x}\\) unstable) shrink if \\(r<0\\) ( \\(\\hat{x}\\) locally stable) Unlike in discrete time, there is no possibility for oscillations here.","title":"4. Stability analysis in continuous-time one-variable models"},{"location":"lectures/lecture-8/#5-logistic-growth-continuous-time","text":"Let's again look at the model of logistic growth \\[ f(n)=\\frac{\\mathrm{d}n}{\\mathrm{d}t} = rn \\left(1 - \\frac{n}{K}\\right) \\] The derivative of \\(f\\) with respect to \\(n\\) is \\[ f'(n) = r - 2 r\\frac{n}{K} \\] Plugging in \\(n=K\\) gives \\[ f'(K) = r - 2 r = -r \\] This implies that an \\(r>0\\) causes local stability of \\(\\hat{n}=K\\) , consistent with our graphical analysis.","title":"5. Logistic growth (continuous time)"},{"location":"lectures/lecture-8/#6-summary","text":"Local stability analysis for discrete- and continuous-time models. take the derivative of the recursion/differential equation with respect to the variable, \\(f'(x)\\) plug in the equilibrium value of the variable, \\(f'(\\hat x)\\) determine the sign and magnitude (sign only in continuous-time)","title":"6. Summary"},{"location":"lectures/lecture-9/","text":"{ requestKernel: true, mountActivateWidget: true, mountStatusWidget: true, binderOptions: { repo: \"tomouellette/executable-cells\", ref: \"main\", binderUrl: \"https://gke.mybinder.org\", }, } Lecture 9: General solutions (univariate) Run notes interactively? Lecture overview General solutions Linear models in discrete time Nonlinear models in discrete time Linear models in continuous time Nonlinear models in continuous time Summary 1. General solutions Last week we learned how to find equilibria and determine their stability in models with one variable (univariate). Those analyses describe the long-term dynamics of our models. This week we\u2019ll look at some simple cases where we can describe the entire dynamics, by solving for the variable at any point in time, eg \\(n(t) = ...\\) This is called a general solution . 2. Linear models in discrete time With a single variable in discrete time all linear models can be written \\[ n(t + 1) = Rn(t) + m \\] There are two cases: 1) \\(m = 0\\) and 2) \\(m \\neq 0\\) 1) When m = 0 we can use brute force iteration \\[ n(t) = Rn(t \u2212 1) \\newline = RRn(t \u2212 2) \\newline = RRRn(t \u2212 3) \\newline = R\u22efRn(0) \\newline = Rtn(0) \\] 2) When \\(m \\neq 0\\) (called an affine model ) we need to use a transformation Solving affine models 2) When \\(m \\neq 0\\) (called an affine model ) we need to use a transformation Step 1 : Solve for the equilibrium \\[ \\hat{n} = \\frac{m}{1 - R} \\] Step 2 : Define \\(\\delta(t) = n(t) - \\hat{n}\\) , the deviation from the equilibrium (this is our transformation) Step 3 : Write the recursion equation for the transformed variable \\[ \\begin{aligned} \\delta(t+1) &= n(t+1) - \\hat{n} \\\\ &= R n(t) + m - \\hat{n} \\\\ &= R(\\delta(t) + \\hat{n}) + m - \\hat{n}\\\\ &= R(\\delta(t) + \\frac{m}{1 - R}) + m - \\frac{m}{1 - R}\\\\ &= R \\delta(t) \\end{aligned} \\] Step 4 : The general solution for the transformed variable is then \\(\\delta(t) = R^t \\delta(0)\\) Step 5 : Reverse transform back to \\(n(t)\\) \\[ \\begin{aligned} n(t) &= \\delta(t) - \\hat{n}\\\\ &= R^t \\delta(0) - \\hat{n}\\\\ &= R^t (n(0) - \\hat{n}) - \\hat{n}\\\\ &= R^t n(0) + (1 - R^t)\\hat{n} \\end{aligned} \\] \\(n(t) = R^t n(0) + (1 - R^t)\\hat{n}\\) This says that our variable moves from \\(n(0)\\) towards/away from \\(\\hat{n}\\) by a factor \\(R\\) per time step. import sympy import matplotlib.pyplot as plt # Plot populations that converge to an equilibrium R, m = 0.99, 1 t, n0 = sympy.symbols('t, n0') n = R**t * n0 + (1 - R**t) * m/(1-R) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {R}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show() # Plot populations that diverge to exponential growth R, m = 1.01, 1 t, n0 = sympy.symbols('t, n0') n = R**t * n0 + (1 - R**t) * m/(1-R) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 14000), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {R}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 14000), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show() 3. Nonlinear models in discrete time Unfortunately there is no recipe to solve nonlinear models in discrete time, even with one variable. In fact, most of the time there is no general solution. Remember logistic growth?? \\[ n(t+1) = n(t) + n(t) r \\left(1 - \\frac{n(t)}{K}\\right) \\] Solving with transformations Sometimes, however, you can find a transformation that works. For example, with haploid selection we have \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] Brute force iteration will just create a giant mess. But what about if we let \\(f(t) = p(t)/q(t)\\) ? Noting that \\(q(t+1) = 1 - p(t+1) = (W_a p(t))/(W_A p(t) + W_a q(t))\\) we have \\[ \\begin{aligned} f(t+1) &= \\frac{p(t+1)}{q(t+1)}\\\\ &= \\frac{W_A p(t)}{W_a q(t)}\\\\ &= \\frac{W_A}{W_a} f(t) \\end{aligned} \\] This implies that \\(f(t) = (W_A/W_a)^t f(0)\\) ! Converting back to \\(p(t)\\) we see \\[ p(t) = \\frac{f(t)}{1-f(t)} = \\frac{W_A^t p(0)}{W_A^t p(0) + W_a^t q(0)} \\] Solving with conceptualization An alternative way to derive this general solution is to think about the \\(A\\) and \\(a\\) alleles as two competing populations that each grow exponentially according to their fitness. \\[ \\begin{aligned} n_A(t) &= W_A^t n_A(0)\\\\ n_a(t) &= W_a^t n_a(0) \\end{aligned} \\] Then the frequency of allele \\(A\\) at time \\(t\\) is \\[ p(t) = \\frac{n_A(t)}{n_A(t) + n_a(t)} = \\frac{W_A^t n_A(0)}{W_A^t n_A(0) + W_a^t n_a(0)} \\] Dividing numerator and denominator by the total initial population size \\(n_A(0) + n_a(0)\\) \\[ p(t) = \\frac{W_A^t p(0)}{W_A^t p(0) + W_a^t q(0)} \\] import numpy as np # Initialize symbols and function WA, Wa = 1.1, 1 t, p0 = sympy.symbols('t, p0') pt = (WA**t * p0)/(WA**t * p0 + Wa**t * (1-p0)) ft = sympy.lambdify(t, pt.subs({'p0': 0.01})) # Plot data fig, ax = plt.subplots() ax.plot(np.linspace(0, 100), ft(np.linspace(0, 100))) ax.set_xlabel('time step, t') ax.set_ylabel('allele frequency, p') plt.show() 4. Linear models in continuous time In continuous time, a linear differential equation of one variable can be written \\[ \\frac{dn}{dt} = r n + m \\] Let's first look at the case where \\(m=0\\) . Separation of variables Here we can use a method called seperation of variables . That is, our differential equation can be written \\(dn/dt = f(n) g(t)\\) , i.e., we can separate the variables \\(n\\) and \\(t\\) . We can then re-write the equation as \\(dn/f(n) = g(t)dt\\) and take the indefinite integral of both sides. In our case we have \\(f(n)=r n\\) and \\(g(t)=1\\) so \\[ \\begin{aligned} \\int \\frac{\\mathrm{d}n}{r n} &= \\int \\mathrm{d}t \\\\ \\frac{ln(n)}{r} + c_1 &= t + c_2\\\\ ln(n) &= r t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ n(t) &= e^{r t} e^{r c} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(n(0) = e^{rc}\\) and so our general solution is \\[ n(t) = n(0) e^{rt} \\] Using transformations Now let's consider the case where \\(m\\neq0\\) . This can also be solved by the method of separation of variables but let's do it with a transformation, like we did in discrete time. Step 1 : Solve for the equilibrium, \\(\\hat{n} = -m/r\\) . Step 2 : Define \\(\\delta = n - \\hat{n}\\) . Step 3 : Derive the differential equation for \\(\\delta\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\delta}{\\mathrm{d}t} &= \\frac{\\mathrm{d}(n - \\hat{n})}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}n}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{n}}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}n}{\\mathrm{d}t}\\\\ &= r n + m\\\\ &= r (\\delta + \\hat{n}) + m\\\\ &= r(\\delta + -m/r) + m\\\\ &= r \\delta \\end{aligned} \\] Step 4 : The general solution for \\(\\delta\\) is \\(\\delta(t) = \\delta(0) e^{r t}\\) . Step 5 : Replace \\(\\delta\\) with \\(n - \\hat{n}\\) to back transform to our original variable \\[ n(t) = e^{r t} n(0) + (1 - e^{r t})\\hat{n} \\] Similar to the discrete case, we see an exponential approach to/departure from \\(\\hat{n}\\) import sympy import matplotlib.pyplot as plt # Plot populations that converge to an equilibrium r, m = -0.01, 1 t, n0 = sympy.symbols('t, n0') n = sympy.exp(r * t) * n0 + (1 - sympy.exp(r*t)) * (-m/r) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {r}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show() 5. Nonlinear models in continuous time Some nonlinear differential equations can also be solved. Separation of variables Sometimes separation of variables works, as in the case of logistic growth and haploid selection (which are equivalent in form in continuous-time). \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= s p (1-p)\\\\ \\frac{\\mathrm{d}p}{s p (1-p)} &= \\mathrm{d}t\\\\ \\left(\\frac{1}{s p} + \\frac{1/s}{1-p}\\right) \\mathrm{d}p &= \\mathrm{d}t \\;\\text{(method of partial fractions, rule A.19)}\\\\ \\int \\frac{1}{s p} \\mathrm{d}p + \\int \\frac{1}{s(1-p)} \\mathrm{d}p &= \\int \\mathrm{d}t\\\\ \\ln(p)/s - \\ln(1 - p)/s + c_1 &= t + c_2 \\\\ \\ln\\left(\\frac{p}{1-p}\\right) &= s t + s c \\\\ \\frac{p}{1-p} &= e^{st} e^{sc} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(p(0)/(1-p(0)) = e^{sc}\\) . Then solving this linear equation for \\(p\\) \\[ p(t) = \\frac{e^{st} p(0)}{1 - p(0) + e^{st} p(0)} \\] This shows essentially the same dynamics as haploid selection in discrete time # Initialize symbols and function s = 0.01 pt = (sympy.exp(s * t) * p0) / (1-p0+sympy.exp(s*t)*p0) ft = sympy.lambdify(t, pt.subs({'p0': 0.01})) # Plot data fig, ax = plt.subplots() ax.plot(np.linspace(0, 1000), ft(np.linspace(0, 1000))) ax.set_xlabel('time, t') ax.set_ylabel('allele frequency, p') plt.show() Alternative methods Separation of variables does not always work -- it may not be possible to solve the integrals. However, separation of variables is not the only method that works. Box 6.2 in the text describes how to solve three forms of differential equations that are not amenable to separation of variables (ie, that cannot be written like \\(dn/dt = f(n) g(t)\\) ). 6. Summary Today we've covered how to find the general solution for some univariate models. We now have three methods to analyze univariate models: numerical and graphical analyses (eg, cobwebbing) finding equilibria and determining their stability (long-term dynamics) finding the general solution (short- and long-term dynamics) Next week we'll be moving on to multivariate models, ie, those with more than one variable.","title":"Lecture 9"},{"location":"lectures/lecture-9/#lecture-9-general-solutions-univariate","text":"Run notes interactively?","title":"Lecture 9: General solutions (univariate)"},{"location":"lectures/lecture-9/#lecture-overview","text":"General solutions Linear models in discrete time Nonlinear models in discrete time Linear models in continuous time Nonlinear models in continuous time Summary","title":"Lecture overview"},{"location":"lectures/lecture-9/#1-general-solutions","text":"Last week we learned how to find equilibria and determine their stability in models with one variable (univariate). Those analyses describe the long-term dynamics of our models. This week we\u2019ll look at some simple cases where we can describe the entire dynamics, by solving for the variable at any point in time, eg \\(n(t) = ...\\) This is called a general solution .","title":"1. General solutions"},{"location":"lectures/lecture-9/#2-linear-models-in-discrete-time","text":"With a single variable in discrete time all linear models can be written \\[ n(t + 1) = Rn(t) + m \\] There are two cases: 1) \\(m = 0\\) and 2) \\(m \\neq 0\\) 1) When m = 0 we can use brute force iteration \\[ n(t) = Rn(t \u2212 1) \\newline = RRn(t \u2212 2) \\newline = RRRn(t \u2212 3) \\newline = R\u22efRn(0) \\newline = Rtn(0) \\] 2) When \\(m \\neq 0\\) (called an affine model ) we need to use a transformation","title":"2. Linear models in discrete time"},{"location":"lectures/lecture-9/#solving-affine-models","text":"2) When \\(m \\neq 0\\) (called an affine model ) we need to use a transformation Step 1 : Solve for the equilibrium \\[ \\hat{n} = \\frac{m}{1 - R} \\] Step 2 : Define \\(\\delta(t) = n(t) - \\hat{n}\\) , the deviation from the equilibrium (this is our transformation) Step 3 : Write the recursion equation for the transformed variable \\[ \\begin{aligned} \\delta(t+1) &= n(t+1) - \\hat{n} \\\\ &= R n(t) + m - \\hat{n} \\\\ &= R(\\delta(t) + \\hat{n}) + m - \\hat{n}\\\\ &= R(\\delta(t) + \\frac{m}{1 - R}) + m - \\frac{m}{1 - R}\\\\ &= R \\delta(t) \\end{aligned} \\] Step 4 : The general solution for the transformed variable is then \\(\\delta(t) = R^t \\delta(0)\\) Step 5 : Reverse transform back to \\(n(t)\\) \\[ \\begin{aligned} n(t) &= \\delta(t) - \\hat{n}\\\\ &= R^t \\delta(0) - \\hat{n}\\\\ &= R^t (n(0) - \\hat{n}) - \\hat{n}\\\\ &= R^t n(0) + (1 - R^t)\\hat{n} \\end{aligned} \\] \\(n(t) = R^t n(0) + (1 - R^t)\\hat{n}\\) This says that our variable moves from \\(n(0)\\) towards/away from \\(\\hat{n}\\) by a factor \\(R\\) per time step. import sympy import matplotlib.pyplot as plt # Plot populations that converge to an equilibrium R, m = 0.99, 1 t, n0 = sympy.symbols('t, n0') n = R**t * n0 + (1 - R**t) * m/(1-R) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {R}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show() # Plot populations that diverge to exponential growth R, m = 1.01, 1 t, n0 = sympy.symbols('t, n0') n = R**t * n0 + (1 - R**t) * m/(1-R) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 14000), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {R}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 14000), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show()","title":"Solving affine models"},{"location":"lectures/lecture-9/#3-nonlinear-models-in-discrete-time","text":"Unfortunately there is no recipe to solve nonlinear models in discrete time, even with one variable. In fact, most of the time there is no general solution. Remember logistic growth?? \\[ n(t+1) = n(t) + n(t) r \\left(1 - \\frac{n(t)}{K}\\right) \\]","title":"3. Nonlinear models in discrete time"},{"location":"lectures/lecture-9/#solving-with-transformations","text":"Sometimes, however, you can find a transformation that works. For example, with haploid selection we have \\[ p(t+1) = \\frac{W_A p(t)}{W_A p(t) + W_a q(t)} \\] Brute force iteration will just create a giant mess. But what about if we let \\(f(t) = p(t)/q(t)\\) ? Noting that \\(q(t+1) = 1 - p(t+1) = (W_a p(t))/(W_A p(t) + W_a q(t))\\) we have \\[ \\begin{aligned} f(t+1) &= \\frac{p(t+1)}{q(t+1)}\\\\ &= \\frac{W_A p(t)}{W_a q(t)}\\\\ &= \\frac{W_A}{W_a} f(t) \\end{aligned} \\] This implies that \\(f(t) = (W_A/W_a)^t f(0)\\) ! Converting back to \\(p(t)\\) we see \\[ p(t) = \\frac{f(t)}{1-f(t)} = \\frac{W_A^t p(0)}{W_A^t p(0) + W_a^t q(0)} \\]","title":"Solving with transformations"},{"location":"lectures/lecture-9/#solving-with-conceptualization","text":"An alternative way to derive this general solution is to think about the \\(A\\) and \\(a\\) alleles as two competing populations that each grow exponentially according to their fitness. \\[ \\begin{aligned} n_A(t) &= W_A^t n_A(0)\\\\ n_a(t) &= W_a^t n_a(0) \\end{aligned} \\] Then the frequency of allele \\(A\\) at time \\(t\\) is \\[ p(t) = \\frac{n_A(t)}{n_A(t) + n_a(t)} = \\frac{W_A^t n_A(0)}{W_A^t n_A(0) + W_a^t n_a(0)} \\] Dividing numerator and denominator by the total initial population size \\(n_A(0) + n_a(0)\\) \\[ p(t) = \\frac{W_A^t p(0)}{W_A^t p(0) + W_a^t q(0)} \\] import numpy as np # Initialize symbols and function WA, Wa = 1.1, 1 t, p0 = sympy.symbols('t, p0') pt = (WA**t * p0)/(WA**t * p0 + Wa**t * (1-p0)) ft = sympy.lambdify(t, pt.subs({'p0': 0.01})) # Plot data fig, ax = plt.subplots() ax.plot(np.linspace(0, 100), ft(np.linspace(0, 100))) ax.set_xlabel('time step, t') ax.set_ylabel('allele frequency, p') plt.show()","title":"Solving with conceptualization"},{"location":"lectures/lecture-9/#4-linear-models-in-continuous-time","text":"In continuous time, a linear differential equation of one variable can be written \\[ \\frac{dn}{dt} = r n + m \\] Let's first look at the case where \\(m=0\\) .","title":"4. Linear models in continuous time"},{"location":"lectures/lecture-9/#separation-of-variables","text":"Here we can use a method called seperation of variables . That is, our differential equation can be written \\(dn/dt = f(n) g(t)\\) , i.e., we can separate the variables \\(n\\) and \\(t\\) . We can then re-write the equation as \\(dn/f(n) = g(t)dt\\) and take the indefinite integral of both sides. In our case we have \\(f(n)=r n\\) and \\(g(t)=1\\) so \\[ \\begin{aligned} \\int \\frac{\\mathrm{d}n}{r n} &= \\int \\mathrm{d}t \\\\ \\frac{ln(n)}{r} + c_1 &= t + c_2\\\\ ln(n) &= r t + c \\; \\text{(where } c = c_2 - c_1\\text{)}\\\\ n(t) &= e^{r t} e^{r c} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(n(0) = e^{rc}\\) and so our general solution is \\[ n(t) = n(0) e^{rt} \\]","title":"Separation of variables"},{"location":"lectures/lecture-9/#using-transformations","text":"Now let's consider the case where \\(m\\neq0\\) . This can also be solved by the method of separation of variables but let's do it with a transformation, like we did in discrete time. Step 1 : Solve for the equilibrium, \\(\\hat{n} = -m/r\\) . Step 2 : Define \\(\\delta = n - \\hat{n}\\) . Step 3 : Derive the differential equation for \\(\\delta\\) \\[ \\begin{aligned} \\frac{\\mathrm{d}\\delta}{\\mathrm{d}t} &= \\frac{\\mathrm{d}(n - \\hat{n})}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}n}{\\mathrm{d}t} - \\frac{\\mathrm{d}\\hat{n}}{\\mathrm{d}t}\\\\ &= \\frac{\\mathrm{d}n}{\\mathrm{d}t}\\\\ &= r n + m\\\\ &= r (\\delta + \\hat{n}) + m\\\\ &= r(\\delta + -m/r) + m\\\\ &= r \\delta \\end{aligned} \\] Step 4 : The general solution for \\(\\delta\\) is \\(\\delta(t) = \\delta(0) e^{r t}\\) . Step 5 : Replace \\(\\delta\\) with \\(n - \\hat{n}\\) to back transform to our original variable \\[ n(t) = e^{r t} n(0) + (1 - e^{r t})\\hat{n} \\] Similar to the discrete case, we see an exponential approach to/departure from \\(\\hat{n}\\) import sympy import matplotlib.pyplot as plt # Plot populations that converge to an equilibrium r, m = -0.01, 1 t, n0 = sympy.symbols('t, n0') n = sympy.exp(r * t) * n0 + (1 - sympy.exp(r*t)) * (-m/r) for i, n0 in enumerate([50, 75, 125, 150, 175]): if i == 0: plots = sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\", xlabel = 'time step, t', ylabel = 'population size, $n$', legend=True, title=f\"R = {r}, m = {m}\" ) else: plots.append( sympy.plot( n.subs({'n0': n0}), (t,0,400), ylim=(0, 180), axis_center=(0,0), show=False, label=f\"n0 = {n0}\")[0] ) plots.show()","title":"Using transformations"},{"location":"lectures/lecture-9/#5-nonlinear-models-in-continuous-time","text":"Some nonlinear differential equations can also be solved.","title":"5. Nonlinear models in continuous time"},{"location":"lectures/lecture-9/#separation-of-variables_1","text":"Sometimes separation of variables works, as in the case of logistic growth and haploid selection (which are equivalent in form in continuous-time). \\[ \\begin{aligned} \\frac{\\mathrm{d}p}{\\mathrm{d}t} &= s p (1-p)\\\\ \\frac{\\mathrm{d}p}{s p (1-p)} &= \\mathrm{d}t\\\\ \\left(\\frac{1}{s p} + \\frac{1/s}{1-p}\\right) \\mathrm{d}p &= \\mathrm{d}t \\;\\text{(method of partial fractions, rule A.19)}\\\\ \\int \\frac{1}{s p} \\mathrm{d}p + \\int \\frac{1}{s(1-p)} \\mathrm{d}p &= \\int \\mathrm{d}t\\\\ \\ln(p)/s - \\ln(1 - p)/s + c_1 &= t + c_2 \\\\ \\ln\\left(\\frac{p}{1-p}\\right) &= s t + s c \\\\ \\frac{p}{1-p} &= e^{st} e^{sc} \\end{aligned} \\] Plugging in \\(t=0\\) we have \\(p(0)/(1-p(0)) = e^{sc}\\) . Then solving this linear equation for \\(p\\) \\[ p(t) = \\frac{e^{st} p(0)}{1 - p(0) + e^{st} p(0)} \\] This shows essentially the same dynamics as haploid selection in discrete time # Initialize symbols and function s = 0.01 pt = (sympy.exp(s * t) * p0) / (1-p0+sympy.exp(s*t)*p0) ft = sympy.lambdify(t, pt.subs({'p0': 0.01})) # Plot data fig, ax = plt.subplots() ax.plot(np.linspace(0, 1000), ft(np.linspace(0, 1000))) ax.set_xlabel('time, t') ax.set_ylabel('allele frequency, p') plt.show()","title":"Separation of variables"},{"location":"lectures/lecture-9/#alternative-methods","text":"Separation of variables does not always work -- it may not be possible to solve the integrals. However, separation of variables is not the only method that works. Box 6.2 in the text describes how to solve three forms of differential equations that are not amenable to separation of variables (ie, that cannot be written like \\(dn/dt = f(n) g(t)\\) ).","title":"Alternative methods"},{"location":"lectures/lecture-9/#6-summary","text":"Today we've covered how to find the general solution for some univariate models. We now have three methods to analyze univariate models: numerical and graphical analyses (eg, cobwebbing) finding equilibria and determining their stability (long-term dynamics) finding the general solution (short- and long-term dynamics) Next week we'll be moving on to multivariate models, ie, those with more than one variable.","title":"6. Summary"},{"location":"lectures/schedule/","text":"Schedule Lecture Topic Reading 1 Introduction Preface, 1.1, 1.4 (5p.) 2 Model construction Chapter 2 (34p.) 3 Exponential and logistic growth 3.1, 3.2 (8p.) 4 One-locus selection 3.3 (10p.) 5 Numerical and graphical techniques I (univariate) 6 Numerical and graphical techniques II (multivariate) 7 Equilibria (univariate) 8 Stability (univariate) 9 General solutions I (univariate) 10 Midterm review 11 General solutions II (multivariate) 12 Equilibria and stability (linear multivariate) 13 Demography 14 Equilibria and stability (nonlinear multivariate) 15 Evolutionary invasion analysis 16 Evolutionary invasion analysis - structured populations 17 Introduction to probability 18 More probability 19 More probability 20 TBD 21 TBD 22 TBD 23 Grad student presentations 24 Final review","title":"Schedule"},{"location":"lectures/schedule/#schedule","text":"Lecture Topic Reading 1 Introduction Preface, 1.1, 1.4 (5p.) 2 Model construction Chapter 2 (34p.) 3 Exponential and logistic growth 3.1, 3.2 (8p.) 4 One-locus selection 3.3 (10p.) 5 Numerical and graphical techniques I (univariate) 6 Numerical and graphical techniques II (multivariate) 7 Equilibria (univariate) 8 Stability (univariate) 9 General solutions I (univariate) 10 Midterm review 11 General solutions II (multivariate) 12 Equilibria and stability (linear multivariate) 13 Demography 14 Equilibria and stability (nonlinear multivariate) 15 Evolutionary invasion analysis 16 Evolutionary invasion analysis - structured populations 17 Introduction to probability 18 More probability 19 More probability 20 TBD 21 TBD 22 TBD 23 Grad student presentations 24 Final review","title":"Schedule"},{"location":"syllabus/assignments/","text":"Assignments Homeworks A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday. Homeworks can be submitted online (Quercus) or on paper (in class). Labs Each computer lab contains a series of problems and the solutions are due at the end of the lab (Quercus).","title":"Assignments"},{"location":"syllabus/assignments/#assignments","text":"","title":"Assignments"},{"location":"syllabus/assignments/#homeworks","text":"A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday. Homeworks can be submitted online (Quercus) or on paper (in class).","title":"Homeworks"},{"location":"syllabus/assignments/#labs","text":"Each computer lab contains a series of problems and the solutions are due at the end of the lab (Quercus).","title":"Labs"},{"location":"syllabus/course_structure/","text":"Course structure Learning objectives Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, tutorials, assignments, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning Weekly tasks attend two lectures [time = 2h] attend one computer lab [time = 2h] read assigned sections of text [approx. time = 2h] do homework assignment [approx. time = 2h] Grading scheme 20% - homework assignments 20% - lab assignments 20% - midterm exam 15% - final project 25% - final exam Late fees: -20% per day for late homeworks and labs, -10% per day for final projects.","title":"Course structure"},{"location":"syllabus/course_structure/#course-structure","text":"","title":"Course structure"},{"location":"syllabus/course_structure/#learning-objectives","text":"Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, tutorials, assignments, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning","title":"Learning objectives"},{"location":"syllabus/course_structure/#weekly-tasks","text":"attend two lectures [time = 2h] attend one computer lab [time = 2h] read assigned sections of text [approx. time = 2h] do homework assignment [approx. time = 2h]","title":"Weekly tasks"},{"location":"syllabus/course_structure/#grading-scheme","text":"20% - homework assignments 20% - lab assignments 20% - midterm exam 15% - final project 25% - final exam Late fees: -20% per day for late homeworks and labs, -10% per day for final projects.","title":"Grading scheme"},{"location":"syllabus/final_project/","text":"Final project Construct your own model. In this project I want you to use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. First come up with a question that you think can be addressed with a model and describe this model in words. Next, write down what parameters and variables you think would be necessary in the model. Then write down equations that are consistent with your verbal description of the model. Get as far as you can in analyzing your model. If you don't have time to finish the analysis (or if your model requires a more complicated analysis), then outline the next steps that you would take in the analysis. Finally, say whether or not the analysis addresses your original question and whether or not you have gained any insights from the model. Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained. We'll do the final project in two parts. Part 1: Biological question and either an illustration (e.g., flow diagram) or preliminary set of equations. Max 1 page. Due November 16. Part 2: Final project. Max 4 pages. Due December 7. And for grad students taking EEB1430, you'll also give a short presentation of your final project on December 7.","title":"Final Project"},{"location":"syllabus/final_project/#final-project","text":"","title":"Final project"},{"location":"syllabus/final_project/#construct-your-own-model","text":"In this project I want you to use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. First come up with a question that you think can be addressed with a model and describe this model in words. Next, write down what parameters and variables you think would be necessary in the model. Then write down equations that are consistent with your verbal description of the model. Get as far as you can in analyzing your model. If you don't have time to finish the analysis (or if your model requires a more complicated analysis), then outline the next steps that you would take in the analysis. Finally, say whether or not the analysis addresses your original question and whether or not you have gained any insights from the model. Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained. We'll do the final project in two parts. Part 1: Biological question and either an illustration (e.g., flow diagram) or preliminary set of equations. Max 1 page. Due November 16. Part 2: Final project. Max 4 pages. Due December 7. And for grad students taking EEB1430, you'll also give a short presentation of your final project on December 7.","title":"Construct your own model."},{"location":"syllabus/general_info/","text":"General info Land acknowledgement I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement . Group norms The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct . Accessibility The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office . Religious observances The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work. Family care responsibilities The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website .","title":"General info"},{"location":"syllabus/general_info/#general-info","text":"","title":"General info"},{"location":"syllabus/general_info/#land-acknowledgement","text":"I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement .","title":"Land acknowledgement"},{"location":"syllabus/general_info/#group-norms","text":"The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct .","title":"Group norms"},{"location":"syllabus/general_info/#accessibility","text":"The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office .","title":"Accessibility"},{"location":"syllabus/general_info/#religious-observances","text":"The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work.","title":"Religious observances"},{"location":"syllabus/general_info/#family-care-responsibilities","text":"The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website .","title":"Family care responsibilities"},{"location":"syllabus/instructors/","text":"Instructors Professor Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Wednesday 11-12, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io Teaching assistant Puneeth Deraje (he/him) email: puneeth.deraje@mail.utoronto.ca office hours: Thursday 10-11, ESC 3046","title":"Instructors"},{"location":"syllabus/instructors/#instructors","text":"","title":"Instructors"},{"location":"syllabus/instructors/#professor","text":"Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Wednesday 11-12, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io","title":"Professor"},{"location":"syllabus/instructors/#teaching-assistant","text":"Puneeth Deraje (he/him) email: puneeth.deraje@mail.utoronto.ca office hours: Thursday 10-11, ESC 3046","title":"Teaching assistant"},{"location":"syllabus/other_resources/","text":"Other resources There are many resources available at the University to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"Other resources"},{"location":"syllabus/other_resources/#other-resources","text":"There are many resources available at the University to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"Other resources"},{"location":"syllabus/syllabus/","text":"General info Land acknowledgement I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement . Group norms The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct . Accessibility The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office . Religious observances The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work. Family care responsibilities The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website . Course structure Learning objectives Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, tutorials, assignments, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning Weekly tasks attend two lectures [time = 2h] attend one computer lab [time = 2h] read assigned sections of text [approx. time = 2h] do homework assignment [approx. time = 2h] Grading scheme 20% - homework assignments 20% - lab assignments 20% - midterm exam 15% - final project 25% - final exam Late fees: -20% per day for late homeworks and labs, -10% per day for final projects. Textbook Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy When and where Lectures Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142 Labs Wednesday, 3:10 - 5:00 PM, RW 109/107 Assignments Homeworks A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday. Homeworks can be submitted online (Quercus) or on paper (in class). Labs Each computer lab contains a series of problems and the solutions are due at the end of the lab (Quercus). Final project Construct your own model. In this project I want you to use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. First come up with a question that you think can be addressed with a model and describe this model in words. Next, write down what parameters and variables you think would be necessary in the model. Then write down equations that are consistent with your verbal description of the model. Get as far as you can in analyzing your model. If you don't have time to finish the analysis (or if your model requires a more complicated analysis), then outline the next steps that you would take in the analysis. Finally, say whether or not the analysis addresses your original question and whether or not you have gained any insights from the model. Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained. We'll do the final project in two parts. Part 1: Biological question and either an illustration (e.g., flow diagram) or preliminary set of equations. Max 1 page. Due November 16. Part 2: Final project. Max 4 pages. Due December 7. And for grad students taking EEB1430, you'll also give a short presentation of your final project on December 7. Instructors Professor Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Wednesday 11-12, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io Teaching assistant Puneeth Deraje (he/him) email: puneeth.deraje@mail.utoronto.ca office hours: Thursday 10-11, ESC 3046 Other resources There are many resources available at the University to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"General info"},{"location":"syllabus/syllabus/#general-info","text":"","title":"General info"},{"location":"syllabus/syllabus/#land-acknowledgement","text":"I wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and I am grateful to have the opportunity to work on this land. For more information see University of Toronto's land acknowledgement .","title":"Land acknowledgement"},{"location":"syllabus/syllabus/#group-norms","text":"The University of Toronto is committed to equity, human rights, and respect for diversity. All members of the learning environment in this course should strive to create an atmosphere of mutual respect where all members of our community can express themselves, engage with each other, and respect one another\u2019s differences. U of T does not condone discrimination or harassment against any persons or communities. Please contact me if you have any concerns. For more information see the Code of Student Conduct .","title":"Group norms"},{"location":"syllabus/syllabus/#accessibility","text":"The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code . This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the University\u2019s courses and programs. Students with diverse learning styles and needs are welcome in this course. If you have a disability that may require accommodations, please feel free to get in touch with me and/or the Accessibility Services office .","title":"Accessibility"},{"location":"syllabus/syllabus/#religious-observances","text":"The University provides reasonable accommodation of the needs of students who observe religious holy days other than those already accommodated by ordinary scheduling and statutory holidays. Students have a responsibility to alert members of the teaching staff in a timely fashion to upcoming religious observances and anticipated absences and I will make every reasonable effort to avoid scheduling tests, examinations or other compulsory activities at these times. Please reach out to me as early as possible to communicate any anticipated absences related to religious observances, and to discuss any possible related implications for course work.","title":"Religious observances"},{"location":"syllabus/syllabus/#family-care-responsibilities","text":"The University of Toronto strives to provide a family-friendly environment. You may wish to inform me if you are a student with family responsibilities. If you are a student parent or have family responsibilities, you also may wish to visit the Family Care Office website .","title":"Family care responsibilities"},{"location":"syllabus/syllabus/#course-structure","text":"","title":"Course structure"},{"location":"syllabus/syllabus/#learning-objectives","text":"Mathematics is central to science because it provides a rigorous way to go from a set of assumptions (what we take to be true) to their logical consequences (what we want to know). In ecology & evolution this might be how we think SARS-CoV-2 may spread and evolve given a set of vaccination rates and travel restrictions, how caribou population sizes are predicted to respond to forecasted rates of climate change, or something much more abstract like the expected amount of genetic diversity in a randomly mating population. In this course we'll learn how to build, analyze, and interpret mathematical models of increasing complexity through readings, lectures, tutorials, assignments, computer labs, and a final project. By the end of the course you will be able to: build a model: go from a verbal description of a biological system to a set of equations analyze a model: manipulate a set of equations into a mathematical expression of interest interpret a model: translate mathematical expressions back into biological meaning","title":"Learning objectives"},{"location":"syllabus/syllabus/#weekly-tasks","text":"attend two lectures [time = 2h] attend one computer lab [time = 2h] read assigned sections of text [approx. time = 2h] do homework assignment [approx. time = 2h]","title":"Weekly tasks"},{"location":"syllabus/syllabus/#grading-scheme","text":"20% - homework assignments 20% - lab assignments 20% - midterm exam 15% - final project 25% - final exam Late fees: -20% per day for late homeworks and labs, -10% per day for final projects.","title":"Grading scheme"},{"location":"syllabus/syllabus/#textbook","text":"Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy","title":"Textbook"},{"location":"syllabus/syllabus/#when-and-where","text":"","title":"When and where"},{"location":"syllabus/syllabus/#lectures","text":"Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142","title":"Lectures"},{"location":"syllabus/syllabus/#labs","text":"Wednesday, 3:10 - 5:00 PM, RW 109/107","title":"Labs"},{"location":"syllabus/syllabus/#assignments","text":"","title":"Assignments"},{"location":"syllabus/syllabus/#homeworks","text":"A list of homework problems will be released after class on Wednesday, with the solutions due by the start of class on Monday. Homeworks can be submitted online (Quercus) or on paper (in class).","title":"Homeworks"},{"location":"syllabus/syllabus/#labs_1","text":"Each computer lab contains a series of problems and the solutions are due at the end of the lab (Quercus).","title":"Labs"},{"location":"syllabus/syllabus/#final-project","text":"","title":"Final project"},{"location":"syllabus/syllabus/#construct-your-own-model","text":"In this project I want you to use the tools you've learned in class and apply them to a model that you develop. The model can be about any phenomenon in ecology and evolution, as long as you make up the model. Be as creative as you want. First come up with a question that you think can be addressed with a model and describe this model in words. Next, write down what parameters and variables you think would be necessary in the model. Then write down equations that are consistent with your verbal description of the model. Get as far as you can in analyzing your model. If you don't have time to finish the analysis (or if your model requires a more complicated analysis), then outline the next steps that you would take in the analysis. Finally, say whether or not the analysis addresses your original question and whether or not you have gained any insights from the model. Tip If you are having trouble coming up with a new model, take one of the models that we've analysed in the course and change one or more of its underlying assumptions to get a new set of equations. Then analyse these equations. Discuss the differences between the assumptions used and also between the results obtained. We'll do the final project in two parts. Part 1: Biological question and either an illustration (e.g., flow diagram) or preliminary set of equations. Max 1 page. Due November 16. Part 2: Final project. Max 4 pages. Due December 7. And for grad students taking EEB1430, you'll also give a short presentation of your final project on December 7.","title":"Construct your own model."},{"location":"syllabus/syllabus/#instructors","text":"","title":"Instructors"},{"location":"syllabus/syllabus/#professor","text":"Matthew Osmond (he/him) email: mm.osmond@utoronto.ca office hours: Wednesday 11-12, Earth Sciences Centre (ESC) 3041 website: osmond-lab.github.io","title":"Professor"},{"location":"syllabus/syllabus/#teaching-assistant","text":"Puneeth Deraje (he/him) email: puneeth.deraje@mail.utoronto.ca office hours: Thursday 10-11, ESC 3046","title":"Teaching assistant"},{"location":"syllabus/syllabus/#other-resources","text":"There are many resources available at the University to help you succeed in this course. Below are a few: Writing Center Academic integrity More on academic integrity CTSI list of supports Academic success module Get help with Quercus","title":"Other resources"},{"location":"syllabus/textbook/","text":"Textbook Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy","title":"Textbook"},{"location":"syllabus/textbook/#textbook","text":"Otto & Day 2007. A biologist's guide to mathematical modeling in ecology and evolution . UofT library e-copies UofT library physical-copies buy your own copy","title":"Textbook"},{"location":"syllabus/when_and_where/","text":"When and where Lectures Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142 Labs Wednesday, 3:10 - 5:00 PM, RW 109/107","title":"When and where"},{"location":"syllabus/when_and_where/#when-and-where","text":"","title":"When and where"},{"location":"syllabus/when_and_where/#lectures","text":"Monday & Wednesday, 10:10 - 11:00 AM, Ramsay Wright (RW) 142","title":"Lectures"},{"location":"syllabus/when_and_where/#labs","text":"Wednesday, 3:10 - 5:00 PM, RW 109/107","title":"Labs"}]}